[{"content":"Khi chúng ta làm việc với Machine Learning và Deep Learning thì chúng ta thường xuyên bắt gặp các thuật ngữ Epoch - Batch size và Iterations. Và thường sẽ không hiểu rõ chúng là gì, và dùng để làm gì. Vậy trong bài viết này chúng ta sẽ đi tìm hiểu rõ hơn về 3 khái niệm này nhé.\nTrước khi đi vào chi tiết 3 khái niệm này thì mình sẽ giới thiệu cho các bạn một khái niệm không thể thiếu trong các dự án machine learning đó là Gradient Descent và các thuật toán phổ biến của nó.\nVậy Gradient Descent là gì?\nGradient Descent là một thuật toán tối ưu lặp (iterative optimization algorithm) được sử dụng trong các bài toán Machine Learning và Deep Learning (thường là các bài toán tối ưu lồi — Convex Optimization) với mục tiêu là tìm một tập các biến nội tại (internal parameters) cho việc tối ưu models. Trong đó:\n Gradient: là tỷ lệ độ nghiêng của đường dốc (rate of inclination or declination of a slope). Về mặt toán học, Gradient của một hàm số là đạo hàm của hàm số đó tương ứng với mỗi biến của hàm. Đối với hàm số đơn biến, chúng ta sử dụng khái niệm Derivative thay cho Gradient. Descent: là từ viết tắt của descending, nghĩa là giảm dần.  Gradient Descent có nhiều dạng khác nhau như: Batch Gradient Descent (hoặc là Gradient Descent), Stochastic Gradient Descent, Mini-batch Gradient Descent. Về cơ bản thì chúng thực thi như nhau\n Khởi tạo biến nội tại. Đánh giá model dựa vào biến nội tại và hàm mất mát (Loss function). Cập nhật các biến nội tại theo hướng tối ưu hàm mất mát (finding optimal points). Lặp lại bước 2, 3 cho tới khi thỏa điều kiện dừng.  Công thức cập nhật cho GD có thể được viết là:\ntrong đó θ là tập các biến cần cập nhật, η là tốc độ học (learning rate), ▽Өf(θ) là Gradient của hàm mất mát f theo tập θ.\nGradient Descent có thể được minh họa trong hình dưới đây:\nNhưng nó khác nhau khi cập nhật thông số sau các lần train và phụ thuộc vào lượng dữ liệu cho mỗi lần cập nhật θ.\n  Batch Gradient Descent(BGD): Batch ở đây có nghĩa là tất cả tức là khi cập nhật θ = w, chúng ta sử dụng tất cả các điểm dữ liệu có sẵn (xi). Có nghĩa là mỗi epoch ứng với một lần cập nhật θ.\nHạn chế: Việc phải tính toán đạo hàm với tất cả các điểm sau mỗi lần lặp trở nên khó khăn và không hiệu quả với online learning.\nOnline learning: khi cơ sở dữ liệu cập nhật liên tục, mỗi lần thêm vài điểm dữ liệu mới thì bắt buộc mô hình chúng ta phải cập nhật, thay đổi để phù hợp với lượng dữ liệu mới này.\n  Stochastic Gradient Descent(SGD): tại một thời điểm, ta chỉ tính đạo hàm của hàm mất mát dựa trên chỉ 1 điểm dữ liệu xi rồi cập nhật θ dựa trên đạo hàm này. Có nghĩa là mỗi epoch ứng với N lần cập nhật θ ( N là số điểm dữ liệu)\nHạn chế: việc cập nhật từng điểm một như thế này thì có thể làm giảm đi tốc độ thực hiện 1 epoch.\nƯu điểm: với phương pháp này chúng ta chỉ cần một lượng nhỏ số epoch sau đó nếu có dữ liệu mới thì chỉ cần chạy 1 epoch. Phù hợp với các bài toán có dữ liệu lớn.\n  Mini-batch Gradient Descent(M-BGD): khác với SGD, mini-batch sử dụng một lượng n lớn hơn 1(nhưng nhỏ hơn tổng số dữ liệu ban đầu rất nhiều). Mini-batch bắt đầu với mỗi epoch bằng việc xáo trộn ngẫu nhiên rồi chia dữ liệu thành mini-batch, mỗi mini-batch có n điểm dữ liệu(trừ mini-batch cuối cùng có thể ít hơn vì N có thể không chia hết cho n). Thuậ toán này lấy ra một mini-batch để cập nhật θ, nó được sử dụng trong hầu hết các thuật toán machine learning và đặc biệt là deep learning.\n  ok, bây giờ chúng ta sẽ tìm hiểu chi tiết về nội dung chính của bài viết nhé.\n  Sample: sample là một dòng dữ liệu riêng biệt. Bao gồm các inputs để đưa vào thuật toán, một output (ground-truth) để so sánh với giá trị dự đoán và tính giá trị của hàm mất mát. Dữ liệu huấn luyện thường bao gồm nhiều samples. Sample còn được gọi là instance, an observation, an input vector, hay a feature vector.\n  Batch\nNhư đã nói ở trên, một tập training set có thể được chia nhỏ thành các batchs. Mỗi batch sẽ chứa các training samples, và số lượng các samples này được gọi là batch size. Chúng ta cần lưu ý rằng batch size và number of batches ( số lượng các batches) là hoàn toàn khác nhau. Tùy thuộc và lượng batch size mà GD sẽ có nhiều biến thể khác nhau:\n  Batch Gradient Descent: Batch Size = Size of Training Dataset\n  Stochastic Gradient Descent: Batch Size = 1\n  Mini-Batch Gradient Descent: 1 \u0026lt; Batch Size \u0026laquo; Size of Training Set\nĐối với Mini-Batch Gradient Descent thì batch size thường được chọn là lũy thừa của 2 (32, 64, 128, 256…) vì tốc độ tính toán sẽ tối ưu cho các arithmetic algorithms của CPU và GPU. Cách chọn batch size cũng tùy theo yêu cầu của bài toán.\nví dụ: Bạn có một tập dữ liệu huấn luyện gồm 64.000 images, bạn muốn có số lượng batches là 1000 thì khi đó batch size sẽ là 64 (64 images)\n    Epoch\nEpoch là một hyperparameter trong ANN, được dùng để định nghĩa số lần learning algorithm hoạt động trên model, một epoch hoàn thành là khi tất cả dữ liệu training được đưa vào mạng neural network một lần (đã bao gồm cả 2 bước forward và backward cho việc cập nhật internal model parameters).\nTuy nhiên khi dữ liệu training là quá lớn (ví dụ training images từ ImageNet, Google Open Images), việc đưa tất cả training data vào trong 1 epoch là không khả thi và không hiệu quả. Trường hợp số epoch nhỏ thì dễ dẫn đến underfitting vì model không “học” được nhiều từ GD để cập nhật các biến nội tại. Đối với các trường hợp này thì giải pháp là chia nhỏ training dataset ra thành các batches cho mỗi epoch thì cơ hội model học được từ GD sẽ nhiều hơn và tốc độ tính toán sẽ tối ưu hơn.\nChọn số epoch như thế nào? Thường chúng ta cần một số lượng lớn epoch để training cho ANN (10, 100, 500, 1000…) tuy nhiên cũng còn tùy thuộc vào bài toán và tài nguyên máy tính. Một cách khác là sử dụng Learning Curve để tìm số epoch.\n  Iterations\nIteration là số lượng batches (number of batches) cần thiết để hoàn thành một epoch. Công thức tính là iterations = training samples/batch size. Ví dụ: Giả sử bạn có tập huấn luyện gồm 64.000 images, lựa chọn batch size có giá trị là 64 images. Đồng nghĩa: mỗi lần cập nhật trọng số, bạn sử dụng 64 images. Khi đó, bạn mất 64.000/64 = 1000 iterations để có thể duyệt qua hết được tập huấn luyện (để hoàn thành 1 epoch). Khi đó model sẽ có cơ hội cập nhật các biến nội tại 1000 lần, nhân với số lượng epoch mà chúng ta muốn thì sẽ được tổng số lần cập nhật cho việc hoàn thành một mô hình huấn luyện.\n  Tổng kết\nNhư vậy chúng ta đã tìm hiểu rõ về 3 thuật ngữ sẽ thường xuyên gặp khi làm việc với machine learning và deep learning. Đó mới chỉ là việc hiểu về khái niệm, còn việc áp dụng thì còn phải tùy thuộc vào bài toán thực tế mà chúng ta làm, phụ thuộc vào tài nguyên máy tính, lượng dữ liệu chúng ta có, và kết quả chúng ta mong muốn\u0026hellip; Để làm được thì chúng ta phải thường xuyên practice.\nHy vọng qua bài viết này chúng ta đã hiểu rõ về các thuật ngữ trên và áp dụng chúng vào các dự án thực tế một cách hiệu quả nhất.\nTài liệu tham khảo\n  https://medium.com/@ewuramaminka/epoch-iterations-batch-size-11fbbd4f0771\n  https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n  https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n  ","description":"Tìm hiểu về khái niệm batch và epoch và sự khác nhau giữa chúng","id":0,"section":"posts","tags":["batch size","epoch"],"title":"Phân biệt Batch và Epoch trong Neural Network","uri":"https://tranvanly107.github.io/posts/batch-and-epoch/"},{"content":"Như chúng ta đã biết để huấn luyện và đánh giá một mô hình machine learning hoàn chỉnh chúng ta cần chia dữ liệu thành 3 phần: training, validation và test. Vậy trước khi tìm hiểu về K-Fold Cross Validation (K-Fold CV) thì chúng ta sẽ tìm hiểu khái niệm của 3 thành phần ở trên.\n Training set: training set là tập dữ liệu dùng để huấn luyện mô hình, bao gồm dữ liệu đầu vào và nhãn. Với training set, mô hình có thể thấy tất cả dữ liệu và nhãn. Nó sử dụng dữ liệu này để tối ưu loss function thông qua việc điều chỉnh parameter. Validation set: dữ liệu tập này giống với training set, nhưng mô hình không hề được nhìn thấy nhãn. Mô hình đơn thuần dùng dữ liệu đầu vào của validation set để tính toán ra output, sau đó nó so sánh với nhãn để tính ra loss function. Parameter hoàn toàn không được điều chỉnh ở bước này. Validation set là bộ dữ liệu để chúng ta giám sát mô hình. Chúng ta sử dụng kết quả mô hình ở training set để đưa ra quyết định như điều chỉnh hyperparameter, bổ sung thêm dữ liệu để tạo ra một mô hình tốt nhất và tránh overfiting. Test set: test set chỉ có dữ liệu đầu vào mà không có nhãn. Mục đích chính của test set là đánh giá kết quả mô hình cuối cùng, nó đánh giá xem mô hình có thực sự tốt trong thực tế hay không. Nếu mô hình làm tốt ở training set và validation set mà không làm tốt ở test set thì không có ý nghĩa gì trong thực tế.  Tới đây sẽ có nhiều bạn thắc mắc rằng tại sao không huấn luyện trên tập training set và evaluate trên tập test set?\n Lý do là như sau: việc phát triển một mô hình dựa trên việc điều chỉnh các siêu tham số ( vd: chọn số lượng layers, kích thước của từng layer\u0026hellip;), thực hiện điều chỉnh các siêu tham số này dựa trên tín hiệu phản hồi về performance của validation set. Sau khi có một mô hình đủ tốt dựa trên validation set thì bạn quan tâm đến performance của mô hình trên một bộ dữ liệu hoàn toàn mới mà mô hình chưa nhìn thấy bao giờ, và bộ dữ liệu đó là tập test set.\n=\u0026gt; Chính vì thế chúng ta không nên dùng test set để đánh giá mô hình trong lúc train để ngăn chặn việc rò rỉ thông tin. Ngoài ra sử dụng test set để điều chỉnh mô hình và sau khi hoàn thành mô hình bạn đánh giá mô hình cuối cùng bằng test set thì sẽ bị thiếu sót, nó không đúng với mục đích của việc xây dựng mô hình tốt nhất.  OK, bây giờ chúng ta sẽ đi vào nội dung chính của tiêu đề bài viết.\nk-Fold Cross-Validation là gì?\nCross validation là một kỹ thuật lấy mẫu để đánh giá mô hình học máy trong trường hợp dữ liệu khá hạn chế.\nTham số quan trọng trong kỹ thuật này được gọi là k, đại diện cho số nhóm mà dữ liệu sẽ được chia ra. Vì lý do đó mà nó được gọi là k-fold cross-validation.\nKỹ thuật này được thực hiện bởi các bước như sau:\n Xáo trộn data set một cách ngẫu nhiên Chia data set thành k nhóm Với mỗi nhóm  Sử dụng một nhóm gọi là test set để đánh giá hiệu quả mô hình Các nhóm còn lại được gọi là training set sử dụng để huấn luyện mô hình Huấn luyện mô hình trên training set và đánh giá nó trên test set Nhận kết quả đánh giá và loại bỏ mô hình   Tổng hợp lại kết quả của mô hình dựa trên các số liệu đánh giá  Một lưu ý quan trọng: Mỗi mẫu chỉ được gán cho duy nhất 1 nhóm và phải ở nguyên trong nhóm đó cho đến hết quá trình.\nViệc hủy mô hình sau mỗi lần đánh giá là bắt buộc, tránh trường hợp mô hình ghi nhớ nhãn của tập test trong lần đánh giá trước. Các lỗi thiết lập này dễ xảy ra và đều dẫn đến kết quả đánh giá không chính xác (thường là tích cực hơn so với thực tế).\nKết quả tổng hợp thường là trung bình của các lần đánh giá. Ngoài ra việc bổ sung thông tin về phương sai và độ lệch chuẩn vào kết quả tổng hợp cũng được sử dụng trong thực tế.\nĐể cho các bạn dễ hiểu mình sẽ lấy một hình ảnh trên google.\nNhìn hình trên chúng ta thấy, dữ liệu được chia thành 10 nhóm có cùng kích cỡ hay còn gọi là 10-fold cross-validation.\n Chúng ta lây 9 nhóm: dùng để huấn luyện ( training set) được thể hiện bởi các ô màu xám trắng ở trên. Nhóm còn lại là để đánh giá mô hình( giống validation set) được thể hiện bởi ô màu xanh đậm.\nChúng ta sẽ huấn luyện 10 lần và tính toán sự khác nhau giữ số fold trên từng lần training. Kết quả của mô hình là trung bình 10 kết quả của 10 lần trained.  Vậy chọn giá trị k như thế nào?\nChọn giá trị k là một việc cực kỳ quan trọng, nếu chọn không phù hợp sẽ ảnh ưởng đến hiệu quả quả mô hình thậm chí sẽ dẫn đến overfitting hoặc underfitting.\nCó 3 chiến thuật phổ biến để lựa chọn giá trị k:\n Đại diện: Giá trị của k được chọn để mỗi tập train/test đủ lớn, có thể đại diện về mặt thống kê cho dataset chứa nó. k = 10: Thông qua thực nghiệm cho thấy giá trị k thường được lựa chọn là 10 và đã được chứng minh là nó cho ra một kết quả chính xác hơn, ít sai số( overfitting và underfitting). k=n: Giá trị của k được gán cố định bằng n , với n là kích thước của dataset, như vậy mỗi mẫu sẽ được sử dụng để đánh giá mô hình một lần. Cách tiếp cận này còn có tên gọi là leave-one-out cross-validation.  Lựa chọn giá trị k còn phụ thuộc vào nhiều yếu tố quan trọng khác như lượng dữ liệu mà bạn đang sử dụng\u0026hellip; Nhưng k =10 là một lựa chọn rất phổ biến. Bạn có thể sử dụng giá trị này nếu như gặp khó khăn để lựa chọn một giá trị thích hợp cho bài toán của bạn. Thư viện scikit-learnin cung cấp các cài đặt đầy đủ của cross-validation. Bạn có thể tham khảo tại đây\nKhi nào thì nên dùng K-Fold CV?\nNhư mình đã nói ở trên, K-Fold CV là một kỹ thuật để so sánh các mô hình mà không cần chia validation set, điều này tiết kiệm được dữ liệu training. Vậy K-Fold CV thường xuyên sử dụng khi chúng ta không có 1 tập dữ liệu đủ lớn để huấn luyện. Đây cũng được xem là một cách để hạn chế overfitting.\nok, vậy chúng ta đã tìm hiểu khá kỹ về lý thuyết, bây giờ chúng ta sẽ làm một ví dụ nhỏ để hiểu hơn về K-Fold CV nhé, ở đây mình sẽ dùng thư viện sklearn để thực hiện.\n Giả sử chúng ta đang có tập dữ liệu gồm 6 samples: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nĐầu tiên chúng ta sẽ lựa chọn giá trị k để xác định số nhóm mà chúng ta cần chia ra. Vì ở đây chỉ có 6 samples nên chúng ta sẽ chọn giá trị k = 3(3 nhóm) và mỗi nhóm có 2 samples.\nví dụ:   Fold1: [0.5, 0.2]\nFold2: [0.1, 0.3]\nFold3: [0.4, 0.6]\n Và chúng ta sẽ train 3 lần như sau:\n model1: Trained on Fold1 + Fold2, Tested on Fold3 model2: Trained on Fold2 + Fold3, Tested on Fold1 model3: Trained on Fold1 + Fold3, Tested on Fold2\nMỗi model sẽ bị loại bỏ khi nó hoàn thành lượt train và đánh giá.\nCode sẽ như sau:  1 2 3 4 5 6 7 8 9 10  # scikit-learn k-fold cross-validation\r from numpy import array\rfrom sklearn.model_selection import KFold\r# data sample\r data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\r# prepare cross validation\r kfold = KFold(3, True, 1)\r# enumerate splits\r for train, test in kfold.split(data):\rprint(\u0026#39;train: %s, test: %s\u0026#39; % (data[train], data[test]))\r  Và kết quả là:\n train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\ntrain: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\ntrain: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n Tổng kết\n Việc phân chia dữ liệu là một trong những bước quan trọng nhất của một dự án machine learning. Để lựa chọn phương pháp, cách thức phù hợp cho từng bài toán thì cần có một số kinh nghiệm nhất định. Bài biết này nhằm giải thích cho các bạn hiểu hơn về một trong những cách để chia dữ liệu trong việc huấn luyện một mô hình machine learning. Để hiểu rõ hơn về cách làm thế nào để chọn tập validation set thì các bạn nên đọc bài viết này. Hy vọng qua bài viết này các bạn đã hiểu rõ hơn về K-Fold CV để áp dụng nó một cách hiệu quả nhất trong các dự án machine learning của mình. Và mình sẽ liên tục cập nhật các bài viết trong series này mong các bạn đón đọc.!!!  Tài liệu tham khảo:\n https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833 https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation  ","description":"Tìm hiểu về k-fold Cross-Validation và cách thực hiện","id":1,"section":"posts","tags":["k-fold introduction"],"title":"Tìm hiểu về K-Fold Cross-Validation","uri":"https://tranvanly107.github.io/posts/kfold-introduction/"},{"content":"Ở bài trước chúng ta đã tìm hiểu một khái niệm đó là Data clearning (làm sạch dữ liệu) , đây là một giai đoạn trong của Data preparation ( chuẩn bị dữ liệu). Vậy Data preparation là gì thì hôm nay chúng ta sẽ tìm hiểu rõ hơn về nó nhé.\n1. Khái niệm Chuẩn bị dữ liệu là làm cho dữ liệu có giá trị, hiệu chỉnh dữ liệu, cấu trúc và mã hoá dữ liệu phù hợp với mô hình machine learning, đầu ra là một bộ dữ liệu hoàn chỉnh đưa vào mô hình machine learning để đưa ra một kết quả dự đoán tốt nhất.\nĐể chuẩn bị một bộ dữ liệu sẵn sàng cho thuật toán machine learning thì cần có 3 bước chính:\n Bước 1: Select Data Bước 2: Preprocess Data Bước 3: Transform Data  Bước 1: Select Data\nỞ bước này chúng ta đi lựa chọn một bộ dữ liệu trên rất nhiều bộ dữ liệu mà hiện nay được public trên internet để làm việc với vấn đề mà chúng ta cần giải quyết. Chúng ta cần xem xét vấn đề cần giải quyết, bái toán của chúng ta là gì, đầu ra dự đoán như thế nào\u0026hellip; để từ đó chúng ta đưa ra lựa chọn bộ dữ liệu phù hợp cho yêu cầu bài toán.\nBước 2: Preproces Data\nSau khi đã lựa chọn được bộ dữ liệu, chúng ta cần xem xét vấn đề là sử dụng bộ dữ liệu đó như thế nào. Vậy ở bước này là công việc của chúng ta để trả lời câu hỏi đó, đó là tiền xử lý dữ liệu (preprocess Data).\nTiền xử lý dữ liệu là cách mà chúng ta xử lý dữ liệu đã được chọn để phù hợp với thuật toán cần dự đoán.\nBa bước phổ biến trong tiền xử lý dữ liệu đó là formatting, cleaning và sampling.\n Formatting: bộ dữ liệu mà chúng ta chọn có thể chưa phù hợp với format mà thuật toán cần dự đoán. Dữ liệu có thể là dataframe, text file\u0026hellip; Chính vì vậy chúng ta cần phải chuyển sang format phù hợp với thuật toán. Cleaning: đây là bước mà chúng ta thường tập trung xử lý dữ liệu nhiễu, mising values\u0026hellip; Sampling: Dữ liệu có thể quá lớn dẫn đến thời gian training lâu, việc yêu cầu về tính toán và bộ nhớ cần yêu cầu cao hơn, chính vì vậy chúng ta có thể chọn một mẫu đại diện nhỏ hơn dữ liệu ban đầu để cải thiện tình trạng trên, nếu không quan tâm nhiều đến độ chính xác.  Bước 3: Transform Data\nBước này cũng được gọi là feature engineering, một khái niệm khá quen thuộc trong machine learning.\nChuyển đổi tập dữ liệu đã được xử lý ở bước 2 để sẵn sàng cho thuật toán machine learning sử dụng scaling, attribute decomposition và attribute aggregation.\nNhư vậy chúng ta đã giới thiệu sơ qua về lý thuyết. Bước 1 và bước 2 mình đã giới thiệu ở bài trước, các bạn có thể tìm hiểu tại đây . Hôm nay mình sẽ giới thiệu chi tiết về bước 3 và cũng là bước cuối cùng trong Data preparation.\nĐể transform data( biến đổi dữ liệu) mình sẽ giới thiệu 4 phương pháp thường dùng Rescale Data, Standardize Data, Normalize Data, Binarize Data.\nTại sao chúng ta phải biến đổi dữ liệu: trong machine learning có khái niệm mà hầu như ai cũng đã nghe đó là cost function, nó có hình dạng như một cái bát. Nếu chúng ta không biến đổi dữ liệu trước khi train thì hình dạng của nó giống như một cái bát thon dài -\u0026gt; thời gian training lâu, hội tụ chậm và đặc biệt khi sử dụng Gradient Descent thì việc chuẩn hóa dữ liệu đặc biệt rất cần thiết.\n2. Thực hiện Bây giờ chúng ta sẽ đi từng phương pháp một nhé. Trước hết chúng ta sẽ load dữ liệu housing mà ở bài hôm trước chúng ta đang sử dụng. Trong bài này mình sẽ tiếp tục sử dụng bộ dữ liệu này để thực hiện.\n1 2 3 4 5 6 7 8  import os\rimport tarfile\rfrom six.moves import urllib\rimport pandas as pd\rurl = \u0026#34;C:\\\\Users\\\\LYTRAN\\\\Desktop\\\\Tensorflow\\\\AID\\\\housing.csv\u0026#34;\r#names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;]\r housing = pd.read_csv(url)\rhousing.head()\r      longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    2.1. Rescale Data Mọi thuật toán machhine learning không thực hiện tốt khi mà tập dữ liệu của chúng ta( các giá trị trong các attributes) có những phạm vi khác nhau.\nVí dụ ở bảng dữ liệu trên chúng ta thấy: \u0026ldquo;total_bedrooms\u0026rdquo; attribute có giá trị thuộc phạm vi từ 6 - 39320 trong khi đó \u0026ldquo;median_income\u0026rdquo; attribute có giá trị thuộc phạm vi từ 0 - 15, tương tự như những attributes khác cũng vậy. Mặc dù bài viết hôm trước mình đã cleaning data có nghĩa là chỉ xử lý những missing values, nếu chúng ta đưa bộ dữ liệu này vào training thì thời gian training sẽ lâu hơn, lost function hội tụ chậm hơn. Vậy chúng ta sẽ scaling chúng về cùng một phạm vi từ [0-1] hoặc [-1 - 1]. Trong sklearn có hàm MinMaxScaler làm cho chúng ta việc này, chúng ta chỉ việc áp dụng và xem kết quả thôi.\n1 2 3 4 5 6 7 8 9  import pandas\rimport scipy\rimport numpy\rfrom sklearn.preprocessing import MinMaxScaler\rscaler = MinMaxScaler(feature_range=(0, 1))\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rrescaled[0:5, :]\r   [[0.211 0.567 0.784 0.022 0.02 0.009 0.021 0.54 0.75 ]\n[0.212 0.565 0.392 0.181 0.171 0.067 0.187 0.538 0.75 ]\n[0.21 0.564 1. 0.037 0.029 0.014 0.029 0.466 0.75 ]\n[0.209 0.564 1. 0.032 0.036 0.016 0.036 0.355 0.75 ]\n[0.209 0.564 1. 0.041 0.043 0.016 0.042 0.231 0.75 ]]\n Như chúng ta thấy toàn bộ giá trị bây giờ nằm trong khoảng từ 0-1.\n2.2. Standardize Data Trong lĩnh vực học máy, chúng ta có thể sẽ phải xử lý một lượng lớn các kiểu dữ liệu khác nhau, ví dụ như dữ liệu dạng tín hiệu âm thanh, các điểm ảnh trong một bức ảnh,… và những dữ liệu này có thể là các dữ liệu nhiều chiều. Việc chính quy hóa dữ liệu giúp cho giá trị của mỗi đặc trưng có trung bình bằng 0 và phương sai bằng 1. Phương pháp này được sử dụng rộng rãi trong việc chuẩn hóa dữ liệu của nhiều thuật toán học máy (SVM, logistic regression và ANNs).\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import StandardScaler\rimport pandas\rimport numpy\rscaler = StandardScaler().fit(housing)\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(rescaled[0:5,:])\r   [[-1.328 1.053 0.982 -0.805 -0.97 -0.974 -0.977 2.345 1.291]\n[-1.323 1.043 -0.607 2.046 1.348 0.861 1.67 2.332 1.291]\n[-1.333 1.039 1.856 -0.536 -0.826 -0.821 -0.844 1.783 1.291]\n[-1.338 1.039 1.856 -0.624 -0.719 -0.766 -0.734 0.933 1.291]\n[-1.338 1.039 1.856 -0.462 -0.612 -0.76 -0.629 -0.013 1.291]]\n 2.3. Normalize Data Một lựa chọn khác để co giãn các thành phần của các véc-tơ đặc trưng là biến đổi sao cho véc-tơ đặc trưng sau khi biến đổi có độ dài bằng 1.\nQuá trình Normalization rất hữu ích cho các bộ dữ liệu thưa thớt (có nhiều giá trị 0) với các thuộc tính có tỷ lệ khác nhau khi sử dụng các thuật toán có trọng số đầu vào như mạng thần kinh và thuật toán sử dụng các thước đo khoảng cách như K-Nearest Neighbors.\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import Normalizer\rimport pandas\rimport numpy\rscaler = Normalizer().fit(housing)\rnormalized = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(normalized[0:5,:])\r   [[-1.268e-01 3.931e-02 4.254e-02 9.131e-01 1.339e-01 3.341e-01\n1.307e-01 8.639e-03 3.113e-03]\n[-1.595e-02 4.942e-03 2.741e-03 9.266e-01 1.444e-01 3.134e-01\n1.485e-01 1.084e-03 3.916e-04]\n[-7.755e-02 2.401e-02 3.299e-02 9.307e-01 1.205e-01 3.147e-01\n1.123e-01 4.604e-03 1.903e-03]\n[-8.524e-02 2.639e-02 3.626e-02 8.883e-01 1.639e-01 3.891e-01\n1.527e-01 3.935e-03 2.092e-03]\n[-6.909e-02 2.139e-02 2.939e-02 9.195e-01 1.582e-01 3.193e-01\n1.464e-01 2.174e-03 1.695e-03]]\n Ở phương pháp này, nếu dữ liệu của chúng ta mà còn tồn tại giá trị NaN thì nó sẽ báo lỗi.\n2.4. Binarize Data Mục tiêu của mã hóa nhị phân là sử dụng mã nhị phân để băm các giá trị của đặc trưng dạng nhóm thành các giá trị nhị phân.\nChúng ta có thể biến đổi dữ liệu sử dụng ngưỡng nhị phân. Toàn bộ giá trị lớn hơn ngưỡng này thì gắn cho nó là 1 và ngược lại thì gắn cho nó là 0. Phương pháp này rất hữu ích trong feature engineering và khi bạn muốn thêm một feature mới nào mà bạn thấy có ý nghĩa cho dữ liệu.\n1 2 3 4 5 6 7 8 9  # binarization\r from sklearn.preprocessing import Binarizer\rimport pandas\rimport numpy\rbinarizer = Binarizer(threshold=0.0).fit(housing)\rbinary = binarizer.transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(binary[0:5,:])\r   [[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]]\n Như vậy chúng ta đã đi qua 4 phương pháp thường sử dụng nhất trong biến đổi Data, một trong những bước cực kỳ quan trọng của Data preparation, đây là công việc mà đòi hỏi chúng ta có nhiều kinh nghiệm để chọn ra một phương pháp tốt nhất cho bài toán cụ thể.\n3. Tổng kết Khi đối mặt với một vấn đề mà bạn muốn giải quyết nó bằng Học Máy, chuẩn bị dữ liệu cực kỳ quan trọng và cần có rất nhiều thời gian và kinh nghiệm. Nếu làm tốt việc chuẩn bị dữ liệu sẽ giúp bạn trở thành bậc thầy về học máy.\nĐể áp dụng cho việc training thì trước hết bạn cần đặt ra và trả lời nhiều câu hỏi ( vấn đề của bạn là gì, đầu ra như thế nào\u0026hellip;) để từ đó lựa chọn một bộ data với format phù hợp và thực hiện việc chuẩn bị dữ liệu như thế nào để áp dụng nó một cách hiệu quả và phù hợp với yêu cầu thực tế.\nQua bài viết này hy vọng các bạn đã hiểu một cách tổng quát việc chuẩn bị một bộ dữ liệu để sẵn sàng cho việc training như thế nào và áp dụng nó vào các dự án thực tế một cách hiệu quả.\n4. Tài liệu tham khảo   https://scholar.google.com.vn/scholar?q=data+prepare+machine+learning\u0026amp;hl=vi\u0026amp;as_sdt=0\u0026amp;as_vis=1\u0026amp;oi=scholart\n  https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80\n  ","description":"Xây dựng hoàn chỉnh bộ dữ liệu cho machine learning","id":2,"section":"posts","tags":["data preparation"],"title":"Chuẩn bị dữ liệu trong Machine Learning ","uri":"https://tranvanly107.github.io/posts/data-preparation/"},{"content":"Để tiếp tục cho chuỗi các bài viết về machine learning thì hôm nay chúng ta sẽ tiếp tục tìm hiểu một khái niệm rất quan trọng trong bài toán machine learning đó là làm sạch dữ liệu(data cleaning).\nĐể hiểu được tổng quan về data cleaning thì đầu tiên chúng ta sẽ tìm hiểu về khái niệm và những lợi ích của việc làm sạch dữ liệu nhé.\n1. Data cleaning Data cleaning thuộc một trong các giai đoạn của Data preparation ( bài tiếp theo chúng ta sẽ tìm hiểu về khái niệm này) còn gọi là làm sạch dữ liệu, đây là bước đầu tiên và cũng là bước quan trọng nhất mà mỗi cá nhân đều phải thực hiện sau khi thu thập được dữ liệu để có một kết quả dự đoán chính xác. Mục đích của bước này là loại bỏ các dữ liệu \u0026ldquo;nhiễu\u0026rdquo;, dữ liệu không cần thiết, không đầy đủ thông tin - đây được xem là những vấn đề luôn hiện hữu trong mọi bộ dữ liệu.\nCông việc cụ thể trong Data cleaning là xử lý các \u0026ldquo;missing value\u0026rdquo;, các nhiễu và dữ liệu không nhất quán, không cần thiết sẽ được loại bỏ.\nKết quả của bước Data cleaning là một bộ dữ liệu đã được làm sạch, không còn tác nhân gây ảnh hưởng đến các bước sau trong việc chuẩn bị dữ liệu ( Data preparation).\n2. Thực hiện 2.1. Tải dữ liệu Hôm nay mình sẽ sử dụng một bộ dữ liệu thật là dự đoán giá nhà tại tiểu bang California Hoa Kỳ các bạn có thể tải dữ liệu tại đây\nChúng ta vẫn sử dụng pandas để load dữ liệu:\n1 2 3 4 5 6 7 8  import os import tarfile from six.moves import urllib import pandas as pd url = \u0026#34;https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\u0026#34; names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;] housing = pd.read_csv(url)   Ở bài trước mình đã giới thiệu một số phương pháp để hiểu được dữ liệu, nếu các bạn chưa đọc thì tham khảo tại đây\nNhưng để phục vụ cho các bước tiếp theo thì chúng ta cần phải nhìn qua về dữ liệu một chút.\n1  housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY    1  housing.shape    (20640, 10)\n Dữ liệu chúng ta bao gồm 10 cột (longitude, latitude, housing_median_age, total_rooms, total_bed rooms, population, households, median_income, median_house_value, and ocean_proximity) và 20640 hàng ( lưu ý: mỗi hàng ở đây có nghĩa là mỗi instance, và mỗi cột là một attribute) .\n1  housing.info()   Hàm info rất hứu ích trong việc mô tả dữ liệu, nó giúp chúng ta biết được là số hàng, kiểu dữ liệu của mỗi attribute và số lượng non-null values. Bạn có thể thấy kết quả ở phía trên. Chú ý rằng \u0026ldquo;total_bedrooms\u0026rdquo; attribute chỉ có 20433 non-null values, có nghĩa là có 207 hàng không có giá trị và đây gọi là missing value.\nMột chú ý khác là hầu hết các giá trị của các attributes đều là dạng số, chỉ có \u0026ldquo;ocean_proximity\u0026rdquo; là dạng object, nó là một kiểu của python object nhưng khi chúng ta loaded dữ liệu ở dạng CSV thì nó sẽ phải là ở dạng text. Dữ liệu mà chúng ta đang thấy chỉ là 5 hàng đầu tiên, có thể bạn thấy nó chỉ toàn là NEAR BAY nhưng thực tế có nhiều categorical khác nhau:\n1  housing[\u0026#34;ocean_proximity\u0026#34;].value_counts()    \u0026lt;1H OCEAN: 9136\nINLAND: 6551\nNEAR OCEAN: 2658\nNEAR BAY : 2290\nISLAND : 5\nName: ocean_proximity, dtype: int64\n Như vậy chúng ta thấy có 9136 hàng là OCEAN, 6551 là INLAND\u0026hellip; Chúng ta bắt đầu vào phần chính của bài viết.\nNhìn vào dữ liệu chúng ta biết sẽ có một cột là label trong tập dữ liệu được gọi là nhãn, cụ thể ở đây là \u0026ldquo;median_house_value \u0026quot; hay còn goi là giá nhà. Chính vì vậy mình sẽ tách cột này riêng ra để dễ dàng trong việc xử lý, còn bài sau chúng ta sẽ xử lý nó trong bài viết về chuẩn bị dữ liệu.\n1 2  housing = housing.drop(\u0026#34;median_house_value\u0026#34;, axis=1) # drop labels for training set housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 NEAR BAY    Chúng ta đã loại bỏ cột giá nhà đi, bây giờ chỉ còn 9 cột.\nChúng ta lưu ý một điều rằng hầu hết các thuật toán Machine Learning không thể làm việc được với những features mà thiếu dữ liệu ( missing features) chính vì vậy Data cleaning là bước rất quan trọng mà chúng ta sẽ không thể bỏ qua nếu chúng ta không muốn có một kết quả dự đoán tồi.\nNhư mình đã đề cập trước đó là \u0026ldquo;total_bedrooms\u0026rdquo; attribute có một số dữ liệu bị thiếu. Để được rõ hơn chúng ta sẽ show kết quả sau đây:\n1 2  sample_incomplete_rows = housing[housing.isnull().any(axis=1)] sample_incomplete_rows.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 NaN 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 NaN 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 NaN 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 NaN 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 NaN 387.0 161.0 3.9063 NEAR BAY    Như chúng ta thấy ở trên, cột nào có NaN có nghĩa là đang không có giá trị nào, một là đang để trống hai là mặc định là NaN và công việc của chúng ta là sẽ đi giải quyết chúng.\nĐể xử lý các missing value thì chúng ta có 3 lựa chọn:\n Lựa chọn 1: loại bỏ những hàng nào mà giá trị đó là NaN Lựa chọn 2: loại bỏ toàn bộ cột nào mà có bất kỳ một giá trị NaN nào Lựa chọn 3: thay một giá trị bất kỳ vào giá trị NaN( ví dụ: giá trị 0, giá trị mean, giá trị median\u0026hellip;).  1 2 3 4  housing.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1  housing.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1) # option 2  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() # option 3  housing[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True)   Nếu chúng ta chọn lựa chọn 3 thì cần phải tính giá trị median trên tập training set và lấy giá trị đó để thay vào các giá trị NaN trong training set, nhưng bạn đừng quên lưu chúng lại để sử dụng cho tập test khi bạn muốn đánh giá model sử dụng tập test set. Ok, chúng ta bắt đầu xem kết quả của từng lựa chọn nhé:\nTrong lựa chọn một chúng ta sử dụng hàm dropna tức là xóa những hàng nào mà có ít nhất một giá trị NaN.\n1  sample_incomplete_rows.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity    Như vậy là chúng ta đã xóa tất cả các hàng có giá trị NaN. Các bạn lưu ý rằng là đây là chúng ta đang xử lý trên những hàng có giá trị NaN, chứ nhiều bạn nhìn vào bảng trên có thể hiểu nhầm là toàn bộ data đã bị xóa sạch nhưng không phải, mình đang xét những hàng có giá trị NaN và dữ liệu của chúng ta chỉ mất toàn bộ hàng chứa giá trị NaN mà thôi(cụ thể ở đây có 207 hàng) còn các hàng còn lại vẫn giữ nguyên và tương tự như các lựa chọn khác.\n1  sample_incomplete_rows.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1).head() # option 2   Chúng ta sử dụng hàm drop để xóa cột bất kỳ chúng ta muốn, ở đây chúng ta xóa cột có chứa NaN \u0026ldquo;total_bedrooms\u0026rdquo;.\n    longitude latitude housing_median_age total_rooms population households median_income ocean_proximity      290 -122.16 37.77 47.0 1256.0 570.0 218.0 4.3750 NEAR BAY    341 -122.17 37.75 38.0 992.0 732.0 259.0 1.6196 NEAR BAY    538 -122.28 37.78 29.0 5154.0 3741.0 1273.0 2.5762 NEAR BAY    563 -122.24 37.75 45.0 891.0 384.0 146.0 4.9489 NEAR BAY    696 -122.10 37.69 41.0 746.0 387.0 161.0 3.9063 NEAR BAY     Ok, như vậy cột \u0026ldquo;total_bedrooms\u0026rdquo; của chúng ta đã bị xóa.\nLựa chọn 3 thì như mình đã nói ở trên và bây giờ chúng ta sẽ xem kết quả nó như thế nào nhé.\n1 2 3  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() sample_incomplete_rows[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True) # option 3 sample_incomplete_rows.head()   Hàm fillna là hàm mà lấy giá trị bất kỳ mà chúng ta muốn để thay thế cho toàn bộ giá trị NaN của cột đó, ở đây là median ( chúng có thể là giá trị 0, hoặc mean)\n    longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 435.0 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 435.0 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 435.0 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 435.0 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 435.0 387.0 161.0 3.9063 NEAR BAY    Chúng ta thấy giá trị median là 435.0 và đã được thay thế cho giá trị NaN rồi nhé.\nViệc lựa chọn 3 phương án trên là tùy vào bài toán, và lượng giá trị NaN. Nhưng hầu hết người ta khuyến khích sử dụng lựa chọn 3 nếu trong trường hợp lượng giá trị NaN quá nhỏ so với toàn bộ dữ liệu.\nOK, vậy chúng ta đã xong việc xử lý \u0026ldquo;missing value\u0026rdquo;, thế là tạm ổn 😄. Mới chỉ tạm ổn thôi nhé chứ chưa ổn đâu, còn tiếp tục.\nNhư chúng ta thấy hầu hết toàn bộ các attributes đều là giá trị số, nhưng lại có một attribute là text, tới đây có nhiều bạn thắc mắc rằng nếu là text thì sao tính được median, thật may là ở trên mình chỉ sử dụng cột total_bedrooms. Mình trả lời cho các bạn là text thì không thể tính được giá trị median nhé, còn nếu các bạn muốn tính giá trị median trên toàn bộ attributes thì các bạn sẽ phải loại bỏ attribute nào mà giá trị của nó thuộc dạng text. Nhưng bây giờ chúng ta sẽ xử lý nó.\n1 2  housing_text = housing[[\u0026#39;ocean_proximity\u0026#39;]] housing_text.head()       ocean_proximity     0 NEAR BAY   1 NEAR BAY   2 NEAR BAY   3 NEAR BAY   4 NEAR BAY    Hầu hết các thuật toán machine learning làm việc với số hơn là text chính vì vậy việc của chúng ta bây giờ là chuyển text thành số thôi.\nTrong thư viện Scikit-Learn có hai hàm để làm việc này cho chúng ta đó là LabelEncoder và OrdinalEncoder, bản chất của hai hàm này là chuyển dạng text sang dạng số, ở đây mình dùng OrdinalEncoder.\n1 2 3 4  from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_text_encoded = ordinal_encoder.fit_transform(housing_text) housing_text_encoded[:10]    array([[0.],\n[0.],\n[4.],\n[1.],\n[0.],\n[1.],\n[0.],\n[1.],\n[0.],\n[0.]])\n OK, bây giờ chúng ta xem dữ liệu của chúng ta trông như thế nào sau khi làm sạch nhé:\n1 2  housing[\u0026#34;ocean_proximity\u0026#34;] = housing_text_encoded housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    1  housing[\u0026#34;ocean_proximity\u0026#34;].dtypes    dtype(\u0026lsquo;float64\u0026rsquo;)\n Kết quả chúng ta thấy \u0026ldquo;ocean_proximity\u0026rdquo; attribute đã chuyển từ text ban đầu thành kiểu dữ liệu số (float).\nNhư vậy chúng ta đã hoàn thành được mục tiêu của chúng ta trong bài viết này. Kết quả là một bộ dữ liệu mà chúng ta mong muốn.\n3. Tổng kết Kết quả nghiên cứu cho thấy để làm một bài toán Machine Learning thành công thì việc xử lý dữ liệu chiếm khoảng 70% thời gian của chúng ta, để có một kết quả với độ chính xác như mong muốn thì việc làm sạch dữ liệu là bước cực kỳ quan trọng và không thể bỏ qua khi chúng ta làm một ứng dụng machine learning.\nLàm sạch dữ liệu là việc đòi hỏi chúng ta phải có kinh nghiệm kết hợp với nhiều kiến thức. Qua bài viết này mình đã hướng dẫn tới các bạn cách thực hiện để chúng ta có một bộ dữ liệu sạch là như thế nào. Hy vọng rằng các bạn sẽ áp dụng hiệu quả trong quá trình xây dựng và huấn luyện các thuật toán machine learning.\nBài viết tiếp theo mình sẽ hướng dẫn cách để chuẩn bị một bộ dữ liệu hoàn chỉnh để áp dụng vào các thuật toán machine learning một cách hiệu quả nhất. Các bạn đón đọc trong bài viết tiếp theo nhé\n4. Tài liệu tham khảo   https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n  https://www.tutorialspoint.com/python_data_science/python_data_cleansing.htm\n  ","description":"Kỹ thuật xử lý dữ liệu cho bài toán machine learning","id":3,"section":"posts","tags":["data cleaning"],"title":"Data Cleaning trong Machine Learning ","uri":"https://tranvanly107.github.io/posts/data-cleaning/"},{"content":"1. Kiến thức cần có Trước khi bắt đầu với bài viết này, vui lòng chắc chắn rằng bạn có kiến thức về Python. Các thư viện cần thiết như:\n Pandas Numpy Sklearn\nCác bạn có thể dùng google colab hoặc jupyter để code nhé.  2. Các bước để làm một bài toán ML Hầu hết toàn bộ bài toán về Machine Learning đều trải qua các bước cơ bản sau: ( Mình sẽ không nói sâu về từng bước, các bạn cần tìm hiểu thêm nhé)\n Xác định vấn đề. Chuẩn bị dữ liệu Đánh giá thuật toán Cải thiện kết quả Hiển thị kết quả dự đoán  3. Bài toán Machine Learning Các bước chính để chúng ta xây dựng bài toán như sau:\n Cài đặt Python và SciPy platform. Tải dữ liệu về Hiểu dữ liệu Trực quan hóa dữ liệu Đánh giá một số thuật toán Dự đoán kết quả\nBây giờ chúng ta bắt đầu nhé.  3.1. Cài đặt thư viện cần thiết Các bạn tự cài đặt Python ( 3.x) và các thư viện sau:\n scipy numpy matplotlib pandas sklearn\nViệc cài đặt các thự viện này rất đơn giản nên các bạn tự tìm hiểu.\nCài đặt scipy tại đây\nSau khi các bạn cài xong thì các bạn test xem đã thành công hay chưa:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import sys\r# Python version\r print(\u0026#39;Python: {}\u0026#39;.format(sys.version))\r# scipy\r import scipy\rprint(\u0026#39;scipy: {}\u0026#39;.format(scipy.__version__))\r# numpy\r import numpy\rprint(\u0026#39;numpy: {}\u0026#39;.format(numpy.__version__))\r# matplotlib\r import matplotlib\rprint(\u0026#39;matplotlib: {}\u0026#39;.format(matplotlib.__version__))\r# pandas\r import pandas\rprint(\u0026#39;pandas: {}\u0026#39;.format(pandas.__version__))\r# scikit-learn\r import sklearn\rprint(\u0026#39;sklearn: {}\u0026#39;.format(sklearn.__version__))\r  Nếu các bạn đã cài thành công thì kết quả có dạng thế này:\n Python: 3.7.4 (default, Oct 19 2019, 05:21:45) [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]\nscipy: 1.4.1\nnumpy: 1.17.3\nmatplotlib: 3.2.0\npandas: 0.25.1\nsklearn: 0.22.2\n Còn nếu các bạn cài đặt bị lỗi thì có thể fix lỗi tại đây.\n3.2. Tải dữ liệu Trong bài viết này chúng ta sẽ sử dụng dữ liệu có sẵn trên google.\nCác bạn có thể tìm hiểu dữ liệu tại đây.\n3.2.1. import các thư viện cần thiết Tiếp theo chúng ta sẽ import các thư viện dưới đây để sự dụng trong bài toán.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Load libraries\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn.metrics import accuracy_score\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r  3.2.2. Tải dữ liệu về Chúng ta sẽ sử dụng pandas để tải dữ liệu về. Và cũng sẽ sử dụng pandas để khai phá và trực quan hóa dữ liệu.\n1 2 3 4  # Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r  Chúng ta sử dụng hàm read_csv và được trả về một dataframe, chúng ta sẽ xem kết quả sau.\nNếu các bạn gặp lỗi khi load thì bạn có thể download iris.csv.\n3.3. Khai phá dữ liệu Bây giờ chúng ta xem dữ liệu nó có gì nhé.\nChúng ta sẽ thực hiện một số cách tiếp cận dưới đây:\n Chiều của dữ liệu Tổng quan về dữ liệu Thống kê tất cả các thuộc tính của dữ liệu Phân bố của dữ liệu\nBây giờ chúng tiến hành đi thực hiện từng mục một nhé:  3.3.1. Chiều của dữ liệu Chiều của dữ liệu cho chúng ta biết là có bao nhiêu hàng và bao nhiêu cột của toàn bộ dữ liệu. Để xem chiều dữ liệu chúng ta sử dụng hàm shape trong pandas.\nprint(dataset.shape)\rOutput: (150,5)\nCó nghĩa là dữ liệu của chúng ta có 150 hàng và 5 cột. Nói cách khác là có 150 điểm dữ liệu và 4 features, 1 class chính là nhãn mà model cần dự đoán.\n3.3.2. Tổng quan dữ liệu Bây giờ chúng ta sẽ xem rõ hơn về dữ liệu nhé. Để xem chúng ta chỉ cần dùng hàm head trong pandas như sau:\nprint(dataset.head(20))\rHàm head giúp chúng ta hiển thị n bản ghi đầu tiên, ở đây chúng sẽ hiển thị 20 hàng đầu tiên của toàn bộ dữ liệu.\n    sepal-length sepal-width petal-length petal-width class     0 5.1 3.5 1.4 0.2 Iris-setosa   1 4.9 3.0 1.4 0.2 Iris-setosa   2 4.7 3.2 1.3 0.2 Iris-setosa   3 4.6 3.1 1.5 0.2 Iris-setosa   4 5.0 3.6 1.4 0.2 Iris-setosa   5 5.4 3.9 1.7 0.4 Iris-setosa   6 4.6 3.4 1.4 0.3 Iris-setosa   7 5.0 3.4 1.5 0.2 Iris-setosa   8 4.4 2.9 1.4 0.2 Iris-setosa   9 4.9 3.1 1.5 0.1 Iris-setosa   10 5.4 3.7 1.5 0.2 Iris-setosa   11 4.8 3.4 1.6 0.2 Iris-setosa   12 4.8 3.0 1.4 0.1 Iris-setosa   13 4.3 3.0 1.1 0.1 Iris-setosa   14 5.8 4.0 1.2 0.2 Iris-setosa   15 5.7 4.4 1.5 0.4 Iris-setosa   16 5.4 3.9 1.3 0.4 Iris-setosa   17 5.1 3.5 1.4 0.3 Iris-setosa   18 5.7 3.8 1.7 0.3 Iris-setosa   19 5.1 3.8 1.5 0.3 Iris-setosa    Đây là hình dạng của một dataframe. Các features của dữ liệu bao gồm: chiều dài đài hoa (sepal-length), chiều rộng đài hoa(sepal-width ), chiều dài cánh hoa (petal-length), chiều rộng cánh hoa(petal-width ).Và có 3 classes mà chúng ta cần dự đoán đó là: Iris-setosa, Iris-versicolor, Iris-virginica. Tất nhiên mỗi loài hoa sẽ có các features khác nhau để mô hình nó phân biệt và đưa ra dự đoán chính xác.\n3.3.3. Thống kê các thuộc tính của dữ liệu Bây giờ chúng ta sẽ xem chi tiết hơn về các features: Bao gồm count(số lượng), mean(chiều dài, chiều rộng trung bình), min và max. Cụ thể như sau:\nprint(dataset.describe())\rVà kết quả là:\n    sepal-length sepal-width petal-length petal-width     count 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667   std 0.828066 0.433594 1.764420 0.763161   min 4.300000 2.000000 1.000000 0.100000   25% 5.100000 2.800000 1.600000 0.300000   50% 5.800000 3.000000 4.350000 1.300000   75% 6.400000 3.300000 5.100000 1.800000   max 7.900000 4.400000 6.900000 2.500000    Như chung ta thấy ở trên độ dài và rộng của từng feature thuộc cùng đơn vị (cm) và cùng nằm trong khoảng từ 0-8 cm.\n3.3.4. Phân bố dữ liệu Bây giờ chúng ta xem số lượng của từng loài hoa ( từng class) bằng cách:\nprint(dataset.groupby('class').size()\rVà kết quả sẽ là:\n class\nIris-setosa 50\nIris-versicolor 50\nIris-virginica 50\n Chúng ta thấy số lượng hoa ( class) đồng đều nhau và cùng là 50.\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # summarize the data\r from pandas import read_csv\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# shape\r print(dataset.shape)\r# head\r print(dataset.head(20))\r# descriptions\r print(dataset.describe())\r# class distribution\r print(dataset.groupby(\u0026#39;class\u0026#39;).size())\r  3.4. Trực quan dữ liệu Chúng ta đã biết được cơ bản về dữ liệu, bây giờ là lúc chúng ta trực quan hóa bằng biểu đồ.\nBước này giúp chúng ta hiểu rõ hơn phân bố chi tiết từng loài hoa(class) bằng cách nhìn vào các biểu đồ sau:\n3.4.1. Univariate Plots Biểu đồ này giúp chúng ta hiểu được phân bố của từng feature.\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\rKết quả hiển thị như sau:\nChúng ta cũng có thể tạo ra các biểu đồ histogram với dữ liệu đầu vào:\n# histograms\rdataset.hist()\rpyplot.show()\rKết quả hiển thị như sau:\n3.4.2. Multivariate Plots Bây giờ chúng ta có thể thấy sự liên quan giữa các features. Chúng ta sử dụng scatter_matrix trong pandas để thấy được quan hệ của các features trên toàn bộ dữ liệu:\n# scatter plot matrix\rscatter_matrix(dataset)\rpyplot.show()\rChúng ta thấy được sự tương quan giữa chúng thông qua biểu đồ sau:\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # visualize the data\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# box and whisker plots\r dataset.plot(kind=\u0026#39;box\u0026#39;, subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\r# histograms\r dataset.hist()\rpyplot.show()\r# scatter plot matrix\r scatter_matrix(dataset)\rpyplot.show()\r  3.5. Đánh giá thuật toán Bây giờ là lúc chúng ta sẽ thử chọn một vài model để xem độ chính xác của từng model trên tập test của chúng ta nhé.\nCác bước thực hiện như sau:\n Tách một phần dữ liệu để đánh giá model ( validation dataset) Thiết lập test harness để sử dụng 10-fold cross validation Chạy và dự đoán kết quả trên từng model Chọn ra model có kết quả tốt nhất để sử dụng cuối cùng\nok, bây giờ sẽ vào chi tiết nhé:  3.5.1. Tách một phần dữ liệu (validation Dataset) Chúng ta sẽ chia dữ liệu của chúng ta ra thành 2 phần: 80% dữ liệu dùng để huấn luyện model ( training data), 20% còn lại là dùng để đánh giá model(validation data)\n1 2 3 4 5  # Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\r  Chúng ta lấy bốn cột đầu tiên (X) là các features hay còn gọi là đặc tính của dữ liệu(attributes), cột cuối cùng là (y) nhãn của dữ liệu.\nBây giờ chúng ta đã chuẩn bị được dữ liệu đầy đủ cho việc training và testing rồi.\n3.5.2. Cross validation Chúng ta sẽ tạo ra 10 fold cross validation để estimate model.\nCụ thể là chúng ta chia dữ liệu train ra làm 10 phần bằng nhau, lấy 9 phần cho việc train và phần còn lại là để test trong lúc train. Để hiểu rõ hơn về cross validation thì các bạn có thể tìm hiểu tại đây nhé.\n3.5.3. Build models Chúng ta bây giờ sẽ không thể biết được model nào sẽ tốt nhất.\nSau đây là những model mình dùng để chạy và dự đoán độ chính xác.\n Logistic Regression (LR) Linear Discriminant Analysis (LDA) K-Nearest Neighbors (KNN). Classification and Regression Trees (CART). Gaussian Naive Bayes (NB). Support Vector Machines (SVM).\nCụ thể từng model thì mình sẽ cập nhật sau nếu có thời gian nhé.\nNào bây giờ chúng ta sẽ code và chạy nhé:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r  3.5.4. Chọn model có kết quả tốt nhất Bây giờ chúng ta đã chạy được 6 model và mỗi model sẽ cho ra độ chính xác khác nhau. Giờ là lúc chúng ta so sánh từng kết quả và chọn ra model có độ chính xác tốt nhất.\nChạy xong đoạn code trên thì chúng ta nhận được kết quả sau:\n LR: 0.960897 (0.052113)\nLDA: 0.973974 (0.040110)\nKNN: 0.957191 (0.043263)\nCART: 0.957191 (0.043263)\nNB: 0.948858 (0.056322)\nSVM: 0.983974 (0.032083)\n Như chúng ta thấy độ chính xác của model Support Vector Machine(SVM)\nđạt kết quả với độ chính xác lên đến hơn 98% và là kết quả tốt nhất.\nToàn bộ code cho bài viết như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # compare algorithms\r from pandas import read_csv\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\r# Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r# Compare Algorithms\r pyplot.boxplot(results, labels=names)\rpyplot.title(\u0026#39;Algorithm Comparison\u0026#39;)\rpyplot.show()\r  3.5.5. Dự đoán kết quả Bây giờ chúng ta chọn một model tốt nhất để dự đoán. Thì theo như kết quả ở trên thì chúng ta sẽ chọn SVM là model cuối cùng để dự đoán.\nChúng ta sẽ dự đoán dựa trên tập validation set mà chúng ta đã tách ở mục 3.5.1.\nĐây là dữ liệu không hề liên quan tới model trong quá trình training. Mục đích của việc dự đoán trên tập dữ liệu này để đánh giá xem model chúng ta chọn có thực sự dự đoán tốt hay không, để từ đó biết được nó có bị overfitting hay không.\nĐầu tiên chúng ta fit model trên toàn bộ tập train và dự đoán trên toàn bộ tập validation.\n1 2 3 4 5 6  # Make predictions on validation dataset\r model = SVC(gamma=\u0026#39;auto\u0026#39;)\rmodel.fit(X_train, Y_train)\rpredictions = model.predict(X_validation)\r# Evaluate predictions\r print(accuracy_score(Y_validation, predictions))\r  Kết quả của chúng ta là:\n 0.9666666666666667\n[[11 0 0]\n[ 0 12 1]\n[ 0 0 6]]\n Hoặc chúng ta có thể lên google download ảnh của một loài hoa thuộc một trong 3 loại hoa ở trên và cho vào model để xem model dự đoán xem là loài hoa gì nhé.\nChúng ta có thể sử dụng precision, recall\u0026hellip; để show kết quả.\nprint(confusion_matrix(Y_validation, predictions))\rprint(classification_report(Y_validation, predictions))\rVà kết quả:\n    precision recall f1-score support     Iris-setosa 1.00 1.00 1.00 11   Iris-versicolor 1.00 0.92 0.96 13   Iris-virginica 0.86 1.00 0.92 6       accuracy  0.97 30     macro avg 0.95 0.97 0.96 30   weighted avg 0.97 0.97 0.97 30    Vậy là chúng ta đã cơ bản hoàn thành bài toán đầu tiên của Machine Learning.\n4. Tổng Kết  Bài viết này nhằm mục đích giúp các bạn hiểu được luồng hoạt động hay nói cách khác là cách để làm một bài toán machine learning nên mình sẽ không đi chi tiết vào từng dòng code. Mặc định ban đầu là các bạn đã có kiến thức cơ bản mà bài viết đã yêu cầu nên mình cũng không nói rõ từng thư viện, từng khái niệm trong bài viết nữa. Đây là bài viết đầu tiên trong loạt bài ML cơ bản nên có rất nhiều hạn chế và thiếu sót, rất mong được sự góp ý của các bạn( mình sẽ cải thiện dần trong những bài viết sau). Nhưng hy vọng qua bài viết này các bạn đã biết cách xây dựng một bài toán machine learning.  Cảm ơn tất cả các bạn đã đọc bài viết. Bài viết tiếp theo sẽ là cách xử lý dữ liệu, cleaning data như thế nào. Mong các bạn đón đọc!!!\n","description":"Đây là bài toán đầu tiên trong loạt bài ML cơ bản","id":4,"section":"posts","tags":["tutorial","machine learning"],"title":"Giải bài toán Machine Learning đầu tiên ","uri":"https://tranvanly107.github.io/posts/first-machine-learning/"}]