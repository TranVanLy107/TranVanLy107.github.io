[{"content":"1. Kiến thức cần có Trước khi bắt đầu với bài viết này, vui lòng chắc chắn rằng bạn có kiến thức về Python. Các thư viện cần thiết như:\n Pandas Numpy Sklearn\nCác bạn có thể dùng google colab hoặc jupyter để code nhé.  2. Các bước để làm một bài toán ML Hầu hết toàn bộ bài toán về Machine Learning đều trải qua các bước cơ bản sau: ( Mình sẽ không nói sâu về từng bước, các bạn cần tìm hiểu thêm nhé)\n Xác định vấn đề. Chuẩn bị dữ liệu Đánh giá thuật toán Cải thiện kết quả Hiển thị kết quả dự đoán  3. Bài toán Machine Learning Các bước chính để chúng ta xây dựng bài toán như sau:\n Cài đặt Python và SciPy platform. Tải dữ liệu về Hiểu dữ liệu Trực quan hóa dữ liệu Đánh giá một số thuật toán Dự đoán kết quả\nBây giờ chúng ta bắt đầu nhé.  3.1. Cài đặt thư viện cần thiết Các bạn tự cài đặt Python ( 3.x) và các thư viện sau:\n scipy numpy matplotlib pandas sklearn\nViệc cài đặt các thự viện này rất đơn giản nên các bạn tự tìm hiểu.\nCài đặt scipy tại đây\nSau khi các bạn cài xong thì các bạn test xem đã thành công hay chưa:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  # Python version\r import sys\rprint(\u0026#39;Python: {}\u0026#39;.format(sys.version))\r# scipy\r import scipy\rprint(\u0026#39;scipy: {}\u0026#39;.format(scipy.__version__))\r# numpy\r import numpy\rprint(\u0026#39;numpy: {}\u0026#39;.format(numpy.__version__))\r# matplotlib\r import matplotlib\rprint(\u0026#39;matplotlib: {}\u0026#39;.format(matplotlib.__version__))\r# pandas\r import pandas\rprint(\u0026#39;pandas: {}\u0026#39;.format(pandas.__version__))\r# scikit-learn\r import sklearn\rprint(\u0026#39;sklearn: {}\u0026#39;.format(sklearn.__version__))\r  Nếu các bạn đã cài thành công thì kết quả có dạng thế này:\nPython: 3.6.9 (default, Oct 19 2019, 05:21:45) [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]\n scipy: 1.3.1\nnumpy: 1.17.3\nmatplotlib: 3.1.1\npandas: 0.25.1\nsklearn: 0.21.3\n Còn nếu các bạn cài đặt bị lỗi thì có thể fix lỗi tại đây.\n3.2. Tải dữ liệu Trng bài viết này chúng ta sẽ sử dụng dữ liệu có sẵn trên google.\nCác bạn có thể tìm hiểu dữ liệu tại đây.\n3.2.1. import các thư viện cần thiết Tiếp theo chúng ta sẽ import các thư viện dưới đây để sự dụng trong bài toán.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Load libraries\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn.metrics import accuracy_score\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r  3.2.2. Tải dữ liệu về Chúng ta sẽ sử dụng pandas để tải dữ liệu về. Và cũng sẽ sử dụng pandas để khai phá và trực quan hóa dữ liệu.\n1 2 3 4  # Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r  Nếu các bạn gặp lỗi khi load thì bạn có thể download iris.csv.\n3.3. Khai phá dữ liệu Bây giờ chúng ta xem dữ liệu nó có gì nhé.\n*Chúng ta sẽ thực hiện một số cách tiếp cận dưới đây:\n Chiều của dữ liệu Tổng quan về dữ liệu Thống kê tất cả các thuộc tính của dữ liệu Phân bố của dữ liệu\nChúng ta đi vào chi tiết:  3.3.1. Chiều của dữ liệu Chiều của dữ liệu cho chúng ta biết là có bao nhiều hàng và bao nhiêu cột của toàn bộ dữ liệu.\nprint(dataset.shape)\rChúng ta sẽ thấy là 150 hàng và 5 cột. Có nghĩa là có 50 điểm dữ liệu và 4 features, 1 class chín là nhãn mà model cần dự đoán.\n(150,5)\n3.3.2. Tổng quan dữ liệu Bây giờ chúng ta sẽ xem rõ hơn về dữ liệu nhé. Để xem chúng ta chỉ cần dùng head như sau:\nprint(dataset.head(20))\rChúng sẽ hiển thị 20 hàng đầu tiên của toàn bộ dữ liệu.\n    sepal-length sepal-width petal-length petal-width class     0 5.1 3.5 1.4 0.2 Iris-setosa   1 4.9 3.0 1.4 0.2 Iris-setosa   2 4.7 3.2 1.3 0.2 Iris-setosa   3 4.6 3.1 1.5 0.2 Iris-setosa   4 5.0 3.6 1.4 0.2 Iris-setosa   5 5.4 3.9 1.7 0.4 Iris-setosa   6 4.6 3.4 1.4 0.3 Iris-setosa   7 5.0 3.4 1.5 0.2 Iris-setosa   8 4.4 2.9 1.4 0.2 Iris-setosa   9 4.9 3.1 1.5 0.1 Iris-setosa   10 5.4 3.7 1.5 0.2 Iris-setosa   11 4.8 3.4 1.6 0.2 Iris-setosa   12 4.8 3.0 1.4 0.1 Iris-setosa   13 4.3 3.0 1.1 0.1 Iris-setosa   14 5.8 4.0 1.2 0.2 Iris-setosa   15 5.7 4.4 1.5 0.4 Iris-setosa   16 5.4 3.9 1.3 0.4 Iris-setosa   17 5.1 3.5 1.4 0.3 Iris-setosa   18 5.7 3.8 1.7 0.3 Iris-setosa   19 5.1 3.8 1.5 0.3 Iris-setosa    Các feature của dữ liệu bao gồm: chiều dài đài hoa (sepal-length), chiều rộng đài hoa(sepal-width ), chiều dài cánh hoa (petal-length), chiều rộng cánh hoa(petal-width ).Và có 3 classes mà chúng ta cần dự đoán đó là: Iris-setosa, Iris-versicolor, Iris-virginica. Tất nhiên mỗi loài hoa sẽ có các feature khác nhau để mô hình nó phân biệt và đưa ra dự đoán chính xác.\n3.3.3. Thống kê các thuộc tính của dữ liệu Bây giờ chúng ta sẽ xem chi tiết hơn về các features: Bao gồm count(số lượng), mean(chiều dài, chiều rộng trung bình), min và max. Cụ thể như sau:\nprint(dataset.describe())\rVà kết quả là:\n    sepal-length sepal-width petal-length petal-width     count 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667   std 0.828066 0.433594 1.764420 0.763161   min 4.300000 2.000000 1.000000 0.100000   25% 5.100000 2.800000 1.600000 0.300000   50% 5.800000 3.000000 4.350000 1.300000   75% 6.400000 3.300000 5.100000 1.800000   max 7.900000 4.400000 6.900000 2.500000    Như chung ta thấy ở trên độ dài và rộng của từng feature thuộc cùng đơn vị (cm) và cùng nằm trong khoảng từ 0-8 cm.\n3.3.4. Phân bố dữ liệu Bây giờ chúng ta xem số lượng của từng loài hoa ( từng class) bằng cách:\nprint(dataset.groupby('class').size()\rVà kết quả sẽ là:\n class\nIris-setosa 50\nIris-versicolor 50\nIris-virginica 50\n Chúng ta thấy số lượng hoa ( class) đồng đều nhau và cùng là 50.\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # summarize the data\r from pandas import read_csv\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# shape\r print(dataset.shape)\r# head\r print(dataset.head(20))\r# descriptions\r print(dataset.describe())\r# class distribution\r print(dataset.groupby(\u0026#39;class\u0026#39;).size())\r  3.4. Trực quan dữ liệu Chúng ta đã biết được cơ bản về dữ liệu, bây giờ là lúc chúng ta trực quan hóa bằng biểu đồ.\nBước này giúp chúng ta hiểu rõ hơn phân bố chi tiết từng loài hoa(class) bằng cách nhìn vào các biểu đồ sau:\n3.4.1. Univariate Plots Biểu đồ này giúp chúng ta hiểu được phân bố của từng feature.\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\rKết quả hiển thị như sau:\nChúng ta cũng có thể tạo ra các biểu đồ histogram với dữ liệu đầu vào:\n# histograms\rdataset.hist()\rpyplot.show()\rKết quả hiển thị như sau:\n3.4.2. Multivariate Plots Bây giờ chúng ta có thể thấy sự liên quan giữa các feature. Chúng ta sử dụng scatterplots trên tất cả dữ liệu để thấy được quan hệ của các feture trên toàn bộ dữ liệu:\n# scatter plot matrix\rscatter_matrix(dataset)\rpyplot.show()\rChúng ta thấy được sự tương quan giữa chúng thông qua biểu đồ sau:\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # visualize the data\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# box and whisker plots\r dataset.plot(kind=\u0026#39;box\u0026#39;, subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\r# histograms\r dataset.hist()\rpyplot.show()\r# scatter plot matrix\r scatter_matrix(dataset)\rpyplot.show()\r  3.5. Đánh giá thuật toán Bây giờ là lúc chúng ta sẽ thử chọn một vài model để xem độ chính xác của từng model trên tập test của chúng ta nhé.\nCác bước thực hiện như sau:\n Tách một phần dữ liệu để đánh giá model ( validation dataset) Thiết lập validation dataset để sử dụng 10-fold cross validation Chạy và dự đoán kết quả trên từng model Chọn ra model có kết quả tốt nhất để sử dụng cuối cùng\nok, bây giờ sẽ vào chi tiết nhé:  3.5.1. Tách một phần dữ liệu (validation Dataset) Chúng ta sẽ chia dữ liệu của chúng ta ra thành 2 phần: 80% dữ liệu dùng để huấn luyện model ( training data), 20% còn lại là dùng để đánh giá model(validation data)\n1 2 3 4 5  # Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\r  Bây giờ chúng ta đã chuẩn bị được dữ liệu đầy đủ cho việc training và testing rồi.\n3.5.2. Cross validation Chúng ta sẽ tạo ra 10 fold cross validation để estimate model.\nCụ thể là chúng ta chia dữ liệu train ra làm 10 phần bằng nhau, lấy 9 phần cho việc train và phần còn lại là để test trong lúc train.\n3.5.3. Chạy model Chúng ta bây giờ sẽ không thể biết được model nào sẽ tốt nhất.\nSau đây là những model mình dùng để chạy và dự đoán độ chính xác\n Logistic Regression (LR) Linear Discriminant Analysis (LDA) K-Nearest Neighbors (KNN). Classification and Regression Trees (CART). Gaussian Naive Bayes (NB). Support Vector Machines (SVM).\nCụ thể từng model thì mình sẽ cập nhật sau nếu có thời gian nhé.\nNào bây giờ chúng ta sẽ code và chạy nhé:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r  3.5.4. Chọn model có kết quả tốt nhất Bây giờ chúng ta đã chạy được 6 model và mỗi model sẽ cho ra độ chính xác khác nhau. Giờ là lúc chúng ta so sánh từng kết quả và chọn ra model có độ chính xác tốt nhất.\nChạy xong đoạn code trên thì chúng ta nhận được kết quả sau:\n LR: 0.960897 (0.052113)\nLDA: 0.973974 (0.040110)\nKNN: 0.957191 (0.043263)\nCART: 0.957191 (0.043263)\nNB: 0.948858 (0.056322)\nSVM: 0.983974 (0.032083)\n Như chúng ta thấy độ chính xác của model Support Vector Machine(SVM)\nđạt kết quả với độ chính xác lên đến hơn 98% và là kết quả tốt nhất.\nToàn bộ code cho bài viết như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # compare algorithms\r from pandas import read_csv\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\r# Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r# Compare Algorithms\r pyplot.boxplot(results, labels=names)\rpyplot.title(\u0026#39;Algorithm Comparison\u0026#39;)\rpyplot.show()\r  3.5.5. Dự đoán kết quả Bây giờ chúng ta chọn một model tốt nhất để dự đoán. Thì theo như kết quả ở trên thì chúng ta sẽ chọn SVM là model cuối cùng để dự đoán.\nChúng ta sẽ dự đoán dựa trên tập validation set mà chúng ta đã tách ở mục 3.5.1.\nĐây là dữ liệu không hề liên quan tới model trong quá trình training. Mục đích của việc dự đoán trên tập dữ liệu này để đánh giá xem model chúng ta chọn có thực sự dự đoán tốt hay không, để từ đó biết được nó có bị overfitting hay không.\nĐầu tiên chúng ta fit model trên toàn bộ tập train và dự đoán trên toàn bộ tập validation.\n1 2 3 4 5 6  # Make predictions on validation dataset\r model = SVC(gamma=\u0026#39;auto\u0026#39;)\rmodel.fit(X_train, Y_train)\rpredictions = model.predict(X_validation)\r# Evaluate predictions\r print(accuracy_score(Y_validation, predictions))\r  Kết quả của chúng ta là:\n 0.9666666666666667\n[[11 0 0]\n[ 0 12 1]\n[ 0 0 6]]\n Chúng ta có thể sử dụng precision, recall\u0026hellip; để show kwwts quả\nprint(confusion_matrix(Y_validation, predictions))\rprint(classification_report(Y_validation, predictions))\rVà kết quả:\n    precision recall f1-score support     Iris-setosa 1.00 1.00 1.00 11   Iris-versicolor 1.00 0.92 0.96 13   Iris-virginica 0.86 1.00 0.92 6       accuracy  0.97 30     macro avg 0.95 0.97 0.96 30   weighted avg 0.97 0.97 0.97 30    Vậy là chúng ta đã cơ bản hoàn thành bài toán đầu tiên của Machine Learning.\nMột vài lưu ý của bài viết\n Baì viết này nhằm mục đích giúp các bạn hiểu được luồng hoạt động hay nói cách khác là cách để làm một bài toán machine learning nên mình sẽ không đi chi tiết vào từng dòng code. Mặc định ban đầu là các bạn đã có kiến thức cơ bản mà bài viết đã yêu cầu. nên mình cũng không cần phải nói rõ từng thư viện, từng khái niệm nữa. Đây là bài viết đầu tiên trong loạt bài ML cơ bản nên có rất nhiều hạn chế và thiếu sót, rất mong được sự góp ý của các bạn( mình sẽ cải thiện dần trong những bài viết sau). Nhưng hy vọng qua bài viết này các bạn đã biết cách xây dựng một bài toán machine learning.  Cảm ơn tất cả các bạn đã đọc bài viết. Bài viết tiếp theo sẽ là cách xử lý dữ liệu, cleaning data như thế nào. Mong các bạn đón đọc!!!\n","description":"Đây là bài toán đầu tiên trong loạt bài ML cơ bản","id":0,"section":"posts","tags":["tutorial","machine learning"],"title":"Giải bài toán Machine Learning đầu tiên ","uri":"https://tranvanly107.github.io/posts/first-machine-learning/"}]