[{"content":"Ở bài trước chúng ta đã tìm hiểu một khái niệm đó là Data clearning (làm sạch dữ liệu) , đây là một giai đoạn trong của Data preparation ( chuẩn bị dữ liệu). Vậy Data preparation là gì thì hôm nay chúng ta sẽ tìm hiểu rõ hơn về nó nhé.\n1. Khái niệm Chuẩn bị dữ liệu là làm cho dữ liệu có giá trị, hiệu chỉnh dữ liệu, cấu trúc và mã hoá dữ liệu phù hợp với mô hình machine learning, đầu ra là một bộ dữ liệu hoàn chỉnh đưa vào mô hình machine learning để đưa ra một kết quả dự đoán tốt nhất.\nĐể chuẩn bị một bộ dữ liệu sẵn sàng cho thuật toán machine learning thì cần có 3 bước chính:\n Bước 1: Select Data Bước 2: Preprocess Data Bước 3: Transform Data  Bước 1: Select Data\nỞ bước này chúng ta đi lựa chọn một bộ dữ liệu trên rất nhiều bộ dữ liệu mà hiện nay được public trên internet để làm việc với vấn đề mà chúng ta cần giải quyết. Chúng ta cần xem xét vấn đề cần giải quyết, bái toán của chúng ta là gì, đầu ra dự đoán như thế nào\u0026hellip; để từ đó chúng ta đưa ra lựa chọn bộ dữ liệu phù hợp cho yêu cầu bài toán.\nBước 2: Preproces Data\nSau khi đã lựa chọn được bộ dữ liệu, chúng ta cần xem xét vấn đề là sử dụng bộ dữ liệu đó như thế nào. Vậy ở bước này là công việc của chúng ta để trả lời câu hỏi đó, đó là tiền xử lý dữ liệu (preprocess Data).\nTiền xử lý dữ liệu là cách mà chúng ta xử lý dữ liệu đã được chọn để phù hợp với thuật toán cần dự đoán.\nBa bước phổ biến trong tiền xử lý dữ liệu đó là formatting, cleaning và sampling.\n Formatting: bộ dữ liệu mà chúng ta chọn có thể chưa phù hợp với format mà thuật toán cần dự đoán. Dữ liệu có thể là dataframe, text file\u0026hellip; Chính vì vậy chúng ta cần phải chuyển sang format phù hợp với thuật toán. Cleaning: đây là bước mà chúng ta thường tập trung xử lý dữ liệu nhiễu, mising values\u0026hellip; Sampling: Dữ liệu có thể quá lớn dẫn đến thời gian training lâu, việc yêu cầu về tính toán và bộ nhớ cần yêu cầu cao hơn, chính vì vậy chúng ta có thể chọn một mẫu đại diện nhỏ hơn dữ liệu ban đầu để cải thiện tình trạng trên, nếu không quan tâm nhiều đến độ chính xác.  Bước 3: Transform Data\nBước này cũng được gọi là feature engineering, một khái niệm khá quen thuộc trong machine learning.\nChuyển đổi tập dữ liệu đã được xử lý ở bước 2 để sẵn sàng cho thuật toán machine learning sử dụng scaling, attribute decomposition và attribute aggregation.\nNhư vậy chúng ta đã giới thiệu sơ qua về lý thuyết. Bước 1 và bước 2 mình đã giới thiệu ở bài trước, các bạn có thể tìm hiểu tại đây . Hôm nay mình sẽ giới thiệu chi tiết về bước 3 và cũng là bước cuối cùng trong Data preparation.\nĐể transform data( biến đổi dữ liệu) mình sẽ giới thiệu 4 phương pháp thường dùng Rescale Data, Standardize Data, Normalize Data, Binarize Data.\nTại sao chúng ta phải biến đổi dữ liệu: trong machine learning có khái niệm mà hầu như ai cũng đã nghe đó là cost function, nó có hình dạng như một cái bát. Nếu chúng ta không biến đổi dữ liệu trước khi train thì hình dạng của nó giống như một cái bát thon dài -\u0026gt; thời gian training lâu, hội tụ chậm và đặc biệt khi sử dụng Gradient Descent thì việc chuẩn hóa dữ liệu đặc biệt rất cần thiết.\n2. Thực hiện Bây giờ chúng ta sẽ đi từng phương pháp một nhé. Trước hết chúng ta sẽ load dữ liệu housing mà ở bài hôm trước chúng ta đang sử dụng. Trong bài này mình sẽ tiếp tục sử dụng bộ dữ liệu này để thực hiện.\n1 2 3 4 5 6 7 8  import os\rimport tarfile\rfrom six.moves import urllib\rimport pandas as pd\rurl = \u0026#34;C:\\\\Users\\\\LYTRAN\\\\Desktop\\\\Tensorflow\\\\AID\\\\housing.csv\u0026#34;\r#names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;]\r housing = pd.read_csv(url)\rhousing.head()\r      longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    2.1. Rescale Data Mọi thuật toán machhine learning không thực hiện tốt khi mà tập dữ liệu của chúng ta( các giá trị trong các attributes) có những phạm vi khác nhau.\nVí dụ ở bảng dữ liệu trên chúng ta thấy: \u0026ldquo;total_bedrooms\u0026rdquo; attribute có giá trị thuộc phạm vi từ 6 - 39320 trong khi đó \u0026ldquo;median_income\u0026rdquo; attribute có giá trị thuộc phạm vi từ 0 - 15, tương tự như những attributes khác cũng vậy. Mặc dù bài viết hôm trước mình đã cleaning data có nghĩa là chỉ xử lý những missing values, nếu chúng ta đưa bộ dữ liệu này vào training thì thời gian training sẽ lâu hơn, lost function hội tụ chậm hơn. Vậy chúng ta sẽ scaling chúng về cùng một phạm vi từ [0-1] hoặc [-1 - 1]. Trong sklearn có hàm MinMaxScaler làm cho chúng ta việc này, chúng ta chỉ việc áp dụng và xem kết quả thôi.\n1 2 3 4 5 6 7 8 9  import pandas\rimport scipy\rimport numpy\rfrom sklearn.preprocessing import MinMaxScaler\rscaler = MinMaxScaler(feature_range=(0, 1))\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rrescaled[0:5, :]\r   [[0.211 0.567 0.784 0.022 0.02 0.009 0.021 0.54 0.75 ]\n[0.212 0.565 0.392 0.181 0.171 0.067 0.187 0.538 0.75 ]\n[0.21 0.564 1. 0.037 0.029 0.014 0.029 0.466 0.75 ]\n[0.209 0.564 1. 0.032 0.036 0.016 0.036 0.355 0.75 ]\n[0.209 0.564 1. 0.041 0.043 0.016 0.042 0.231 0.75 ]]\n Như chúng ta thấy toàn bộ giá trị bây giờ nằm trong khoảng từ 0-1.\n2.2. Standardize Data Trong lĩnh vực học máy, chúng ta có thể sẽ phải xử lý một lượng lớn các kiểu dữ liệu khác nhau, ví dụ như dữ liệu dạng tín hiệu âm thanh, các điểm ảnh trong một bức ảnh,… và những dữ liệu này có thể là các dữ liệu nhiều chiều. Việc chính quy hóa dữ liệu giúp cho giá trị của mỗi đặc trưng có trung bình bằng 0 và phương sai bằng 1. Phương pháp này được sử dụng rộng rãi trong việc chuẩn hóa dữ liệu của nhiều thuật toán học máy (SVM, logistic regression và ANNs).\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import StandardScaler\rimport pandas\rimport numpy\rscaler = StandardScaler().fit(housing)\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(rescaled[0:5,:])\r   [[-1.328 1.053 0.982 -0.805 -0.97 -0.974 -0.977 2.345 1.291]\n[-1.323 1.043 -0.607 2.046 1.348 0.861 1.67 2.332 1.291]\n[-1.333 1.039 1.856 -0.536 -0.826 -0.821 -0.844 1.783 1.291]\n[-1.338 1.039 1.856 -0.624 -0.719 -0.766 -0.734 0.933 1.291]\n[-1.338 1.039 1.856 -0.462 -0.612 -0.76 -0.629 -0.013 1.291]]\n 2.3. Normalize Data Một lựa chọn khác để co giãn các thành phần của các véc-tơ đặc trưng là biến đổi sao cho véc-tơ đặc trưng sau khi biến đổi có độ dài bằng 1.\nQuá trình Normalization rất hữu ích cho các bộ dữ liệu thưa thớt (có nhiều giá trị 0) với các thuộc tính có tỷ lệ khác nhau khi sử dụng các thuật toán có trọng số đầu vào như mạng thần kinh và thuật toán sử dụng các thước đo khoảng cách như K-Nearest Neighbors.\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import Normalizer\rimport pandas\rimport numpy\rscaler = Normalizer().fit(housing)\rnormalized = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(normalized[0:5,:])\r   [[-1.268e-01 3.931e-02 4.254e-02 9.131e-01 1.339e-01 3.341e-01\n1.307e-01 8.639e-03 3.113e-03]\n[-1.595e-02 4.942e-03 2.741e-03 9.266e-01 1.444e-01 3.134e-01\n1.485e-01 1.084e-03 3.916e-04]\n[-7.755e-02 2.401e-02 3.299e-02 9.307e-01 1.205e-01 3.147e-01\n1.123e-01 4.604e-03 1.903e-03]\n[-8.524e-02 2.639e-02 3.626e-02 8.883e-01 1.639e-01 3.891e-01\n1.527e-01 3.935e-03 2.092e-03]\n[-6.909e-02 2.139e-02 2.939e-02 9.195e-01 1.582e-01 3.193e-01\n1.464e-01 2.174e-03 1.695e-03]]\n Ở phương pháp này, nếu dữ liệu của chúng ta mà còn tồn tại giá trị NaN thì nó sẽ báo lỗi.\n2.4. Binarize Data Mục tiêu của mã hóa nhị phân là sử dụng mã nhị phân để băm các giá trị của đặc trưng dạng nhóm thành các giá trị nhị phân.\nChúng ta có thể biến đổi dữ liệu sử dụng ngưỡng nhị phân. Toàn bộ giá trị lớn hơn ngưỡng này thì gắn cho nó là 1 và ngược lại thì gắn cho nó là 0. Phương pháp này rất hữu ích trong feature engineering và khi bạn muốn thêm một feature mới nào mà bạn thấy có ý nghĩa cho dữ liệu.\n1 2 3 4 5 6 7 8 9  # binarization\r from sklearn.preprocessing import Binarizer\rimport pandas\rimport numpy\rbinarizer = Binarizer(threshold=0.0).fit(housing)\rbinary = binarizer.transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(binary[0:5,:])\r   [[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]]\n Như vậy chúng ta đã đi qua 4 phương pháp thường sử dụng nhất trong biến đổi Data, một trong những bước cực kỳ quan trọng của Data preparation, đây là công việc mà đòi hỏi chúng ta có nhiều kinh nghiệm để chọn ra một phương pháp tốt nhất cho bài toán cụ thể.\n3. Tổng kết Khi đối mặt với một vấn đề mà bạn muốn giải quyết nó bằng Học Máy, chuẩn bị dữ liệu cực kỳ quan trọng và cần có rất nhiều thời gian và kinh nghiệm. Nếu làm tốt việc chuẩn bị dữ liệu sẽ giúp bạn trở thành bậc thầy về học máy.\nĐể áp dụng cho việc training thì trước hết bạn cần đặt ra và trả lời nhiều câu hỏi ( vấn đề của bạn là gì, đầu ra như thế nào\u0026hellip;) để từ đó lựa chọn một bộ data với format phù hợp và thực hiện việc chuẩn bị dữ liệu như thế nào để áp dụng nó một cách hiệu quả và phù hợp với yêu cầu thực tế.\nQua bài viết này hy vọng các bạn đã hiểu một cách tổng quát việc chuẩn bị một bộ dữ liệu để sẵn sàng cho việc training như thế nào và áp dụng nó vào các dự án thực tế một cách hiệu quả.\n4. Tài liệu tham khảo   https://scholar.google.com.vn/scholar?q=data+prepare+machine+learning\u0026amp;hl=vi\u0026amp;as_sdt=0\u0026amp;as_vis=1\u0026amp;oi=scholart\n  https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80\n  ","description":"Xây dựng hoàn chỉnh bộ dữ liệu cho machine learning","id":0,"section":"posts","tags":["data preparation"],"title":"Chuẩn bị dữ liệu trong machine learning ","uri":"https://tranvanly107.github.io/posts/data-preparation/"},{"content":"Để tiếp tục cho chuỗi các bài viết về machine learning thì hôm nay chúng ta sẽ tiếp tục tìm hiểu một khái niệm rất quan trọng trong bài toán machine learning đó là làm sạch dữ liệu(data cleaning).\nĐể hiểu được tổng quan về data cleaning thì đầu tiên chúng ta sẽ tìm hiểu về khái niệm và những lợi ích của việc làm sạch dữ liệu nhé.\n1. Data cleaning Data cleaning thuộc một trong các giai đoạn của Data preparation ( bài tiếp theo chúng ta sẽ tìm hiểu về khái niệm này) còn gọi là làm sạch dữ liệu, đây là bước đầu tiên và cũng là bước quan trọng nhất mà mỗi cá nhân đều phải thực hiện sau khi thu thập được dữ liệu để có một kết quả dự đoán chính xác. Mục đích của bước này là loại bỏ các dữ liệu \u0026ldquo;nhiễu\u0026rdquo;, dữ liệu không cần thiết, không đầy đủ thông tin - đây được xem là những vấn đề luôn hiện hữu trong mọi bộ dữ liệu.\nCông việc cụ thể trong Data cleaning là xử lý các \u0026ldquo;missing value\u0026rdquo;, các nhiễu và dữ liệu không nhất quán, không cần thiết sẽ được loại bỏ.\nKết quả của bước Data cleaning là một bộ dữ liệu đã được làm sạch, không còn tác nhân gây ảnh hưởng đến các bước sau trong việc chuẩn bị dữ liệu ( Data preparation).\n2. Thực hiện 2.1. Tải dữ liệu Hôm nay mình sẽ sử dụng một bộ dữ liệu thật là dự đoán giá nhà tại tiểu bang California Hoa Kỳ các bạn có thể tải dữ liệu tại đây\nChúng ta vẫn sử dụng pandas để load dữ liệu:\n1 2 3 4 5 6 7 8  import os import tarfile from six.moves import urllib import pandas as pd url = \u0026#34;https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\u0026#34; names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;] housing = pd.read_csv(url)   Ở bài trước mình đã giới thiệu một số phương pháp để hiểu được dữ liệu, nếu các bạn chưa đọc thì tham khảo tại đây\nNhưng để phục vụ cho các bước tiếp theo thì chúng ta cần phải nhìn qua về dữ liệu một chút.\n1  housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY    1  housing.shape    (20640, 10)\n Dữ liệu chúng ta bao gồm 10 cột (longitude, latitude, housing_median_age, total_rooms, total_bed rooms, population, households, median_income, median_house_value, and ocean_proximity) và 20640 hàng ( lưu ý: mỗi hàng ở đây có nghĩa là mỗi instance, và mỗi cột là một attribute) .\n1  housing.info()   Hàm info rất hứu ích trong việc mô tả dữ liệu, nó giúp chúng ta biết được là số hàng, kiểu dữ liệu của mỗi attribute và số lượng non-null values. Bạn có thể thấy kết quả ở phía trên. Chú ý rằng \u0026ldquo;total_bedrooms\u0026rdquo; attribute chỉ có 20433 non-null values, có nghĩa là có 207 hàng không có giá trị và đây gọi là missing value.\nMột chú ý khác là hầu hết các giá trị của các attributes đều là dạng số, chỉ có \u0026ldquo;ocean_proximity\u0026rdquo; là dạng object, nó là một kiểu của python object nhưng khi chúng ta loaded dữ liệu ở dạng CSV thì nó sẽ phải là ở dạng text. Dữ liệu mà chúng ta đang thấy chỉ là 5 hàng đầu tiên, có thể bạn thấy nó chỉ toàn là NEAR BAY nhưng thực tế có nhiều categorical khác nhau:\n1  housing[\u0026#34;ocean_proximity\u0026#34;].value_counts()    \u0026lt;1H OCEAN: 9136\nINLAND: 6551\nNEAR OCEAN: 2658\nNEAR BAY : 2290\nISLAND : 5\nName: ocean_proximity, dtype: int64\n Như vậy chúng ta thấy có 9136 hàng là OCEAN, 6551 là INLAND\u0026hellip; Chúng ta bắt đầu vào phần chính của bài viết.\nNhìn vào dữ liệu chúng ta biết sẽ có một cột là label trong tập dữ liệu được gọi là nhãn, cụ thể ở đây là \u0026ldquo;median_house_value \u0026quot; hay còn goi là giá nhà. Chính vì vậy mình sẽ tách cột này riêng ra để dễ dàng trong việc xử lý, còn bài sau chúng ta sẽ xử lý nó trong bài viết về chuẩn bị dữ liệu.\n1 2  housing = housing.drop(\u0026#34;median_house_value\u0026#34;, axis=1) # drop labels for training set housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 NEAR BAY    Chúng ta đã loại bỏ cột giá nhà đi, bây giờ chỉ còn 9 cột.\nChúng ta lưu ý một điều rằng hầu hết các thuật toán Machine Learning không thể làm việc được với những features mà thiếu dữ liệu ( missing features) chính vì vậy Data cleaning là bước rất quan trọng mà chúng ta sẽ không thể bỏ qua nếu chúng ta không muốn có một kết quả dự đoán tồi.\nNhư mình đã đề cập trước đó là \u0026ldquo;total_bedrooms\u0026rdquo; attribute có một số dữ liệu bị thiếu. Để được rõ hơn chúng ta sẽ show kết quả sau đây:\n1 2  sample_incomplete_rows = housing[housing.isnull().any(axis=1)] sample_incomplete_rows.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 NaN 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 NaN 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 NaN 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 NaN 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 NaN 387.0 161.0 3.9063 NEAR BAY    Như chúng ta thấy ở trên, cột nào có NaN có nghĩa là đang không có giá trị nào, một là đang để trống hai là mặc định là NaN và công việc của chúng ta là sẽ đi giải quyết chúng.\nĐể xử lý các missing value thì chúng ta có 3 lựa chọn:\n Lựa chọn 1: loại bỏ những hàng nào mà giá trị đó là NaN Lựa chọn 2: loại bỏ toàn bộ cột nào mà có bất kỳ một giá trị NaN nào Lựa chọn 3: thay một giá trị bất kỳ vào giá trị NaN( ví dụ: giá trị 0, giá trị mean, giá trị median\u0026hellip;).  1 2 3 4  housing.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1  housing.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1) # option 2  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() # option 3  housing[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True)   Nếu chúng ta chọn lựa chọn 3 thì cần phải tính giá trị median trên tập training set và lấy giá trị đó để thay vào các giá trị NaN trong training set, nhưng bạn đừng quên lưu chúng lại để sử dụng cho tập test khi bạn muốn đánh giá model sử dụng tập test set. Ok, chúng ta bắt đầu xem kết quả của từng lựa chọn nhé:\nTrong lựa chọn một chúng ta sử dụng hàm dropna tức là xóa những hàng nào mà có ít nhất một giá trị NaN.\n1  sample_incomplete_rows.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity    Như vậy là chúng ta đã xóa tất cả các hàng có giá trị NaN. Các bạn lưu ý rằng là đây là chúng ta đang xử lý trên những hàng có giá trị NaN, chứ nhiều bạn nhìn vào bảng trên có thể hiểu nhầm là toàn bộ data đã bị xóa sạch nhưng không phải, mình đang xét những hàng có giá trị NaN và dữ liệu của chúng ta chỉ mất toàn bộ hàng chứa giá trị NaN mà thôi(cụ thể ở đây có 207 hàng) còn các hàng còn lại vẫn giữ nguyên và tương tự như các lựa chọn khác.\n1  sample_incomplete_rows.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1).head() # option 2   Chúng ta sử dụng hàm drop để xóa cột bất kỳ chúng ta muốn, ở đây chúng ta xóa cột có chứa NaN \u0026ldquo;total_bedrooms\u0026rdquo;.\n    longitude latitude housing_median_age total_rooms population households median_income ocean_proximity      290 -122.16 37.77 47.0 1256.0 570.0 218.0 4.3750 NEAR BAY    341 -122.17 37.75 38.0 992.0 732.0 259.0 1.6196 NEAR BAY    538 -122.28 37.78 29.0 5154.0 3741.0 1273.0 2.5762 NEAR BAY    563 -122.24 37.75 45.0 891.0 384.0 146.0 4.9489 NEAR BAY    696 -122.10 37.69 41.0 746.0 387.0 161.0 3.9063 NEAR BAY     Ok, như vậy cột \u0026ldquo;total_bedrooms\u0026rdquo; của chúng ta đã bị xóa.\nLựa chọn 3 thì như mình đã nói ở trên và bây giờ chúng ta sẽ xem kết quả nó như thế nào nhé.\n1 2 3  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() sample_incomplete_rows[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True) # option 3 sample_incomplete_rows.head()   Hàm fillna là hàm mà lấy giá trị bất kỳ mà chúng ta muốn để thay thế cho toàn bộ giá trị NaN của cột đó, ở đây là median ( chúng có thể là giá trị 0, hoặc mean)\n    longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 435.0 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 435.0 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 435.0 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 435.0 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 435.0 387.0 161.0 3.9063 NEAR BAY    Chúng ta thấy giá trị median là 435.0 và đã được thay thế cho giá trị NaN rồi nhé.\nViệc lựa chọn 3 phương án trên là tùy vào bài toán, và lượng giá trị NaN. Nhưng hầu hết người ta khuyến khích sử dụng lựa chọn 3 nếu trong trường hợp lượng giá trị NaN quá nhỏ so với toàn bộ dữ liệu.\nOK, vậy chúng ta đã xong việc xử lý \u0026ldquo;missing value\u0026rdquo;, thế là tạm ổn 😄. Mới chỉ tạm ổn thôi nhé chứ chưa ổn đâu, còn tiếp tục.\nNhư chúng ta thấy hầu hết toàn bộ các attributes đều là giá trị số, nhưng lại có một attribute là text, tới đây có nhiều bạn thắc mắc rằng nếu là text thì sao tính được median, thật may là ở trên mình chỉ sử dụng cột total_bedrooms. Mình trả lời cho các bạn là text thì không thể tính được giá trị median nhé, còn nếu các bạn muốn tính giá trị median trên toàn bộ attributes thì các bạn sẽ phải loại bỏ attribute nào mà giá trị của nó thuộc dạng text. Nhưng bây giờ chúng ta sẽ xử lý nó.\n1 2  housing_text = housing[[\u0026#39;ocean_proximity\u0026#39;]] housing_text.head()       ocean_proximity     0 NEAR BAY   1 NEAR BAY   2 NEAR BAY   3 NEAR BAY   4 NEAR BAY    Hầu hết các thuật toán machine learning làm việc với số hơn là text chính vì vậy việc của chúng ta bây giờ là chuyển text thành số thôi.\nTrong thư viện Scikit-Learn có hai hàm để làm việc này cho chúng ta đó là LabelEncoder và OrdinalEncoder, bản chất của hai hàm này là chuyển dạng text sang dạng số, ở đây mình dùng OrdinalEncoder.\n1 2 3 4  from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_text_encoded = ordinal_encoder.fit_transform(housing_text) housing_text_encoded[:10]    array([[0.],\n[0.],\n[4.],\n[1.],\n[0.],\n[1.],\n[0.],\n[1.],\n[0.],\n[0.]])\n OK, bây giờ chúng ta xem dữ liệu của chúng ta trông như thế nào sau khi làm sạch nhé:\n1 2  housing[\u0026#34;ocean_proximity\u0026#34;] = housing_text_encoded housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    1  housing[\u0026#34;ocean_proximity\u0026#34;].dtypes    dtype(\u0026lsquo;float64\u0026rsquo;)\n Kết quả chúng ta thấy \u0026ldquo;ocean_proximity\u0026rdquo; attribute đã chuyển từ text ban đầu thành kiểu dữ liệu số (float).\nNhư vậy chúng ta đã hoàn thành được mục tiêu của chúng ta trong bài viết này. Kết quả là một bộ dữ liệu mà chúng ta mong muốn.\n3. Tổng kết Kết quả nghiên cứu cho thấy để làm một bài toán Machine Learning thành công thì việc xử lý dữ liệu chiếm khoảng 70% thời gian của chúng ta, để có một kết quả với độ chính xác như mong muốn thì việc làm sạch dữ liệu là bước cực kỳ quan trọng và không thể bỏ qua khi chúng ta làm một ứng dụng machine learning.\nLàm sạch dữ liệu là việc đòi hỏi chúng ta phải có kinh nghiệm kết hợp với nhiều kiến thức. Qua bài viết này mình đã hướng dẫn tới các bạn cách thực hiện để chúng ta có một bộ dữ liệu sạch là như thế nào. Hy vọng rằng các bạn sẽ áp dụng hiệu quả trong quá trình xây dựng và huấn luyện các thuật toán machine learning.\nBài viết tiếp theo mình sẽ hướng dẫn cách để chuẩn bị một bộ dữ liệu hoàn chỉnh để áp dụng vào các thuật toán machine learning một cách hiệu quả nhất. Các bạn đón đọc trong bài viết tiếp theo nhé\n4. Tài liệu tham khảo   https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n  https://www.tutorialspoint.com/python_data_science/python_data_cleansing.htm\n  ","description":"Kỹ thuật xử lý dữ liệu cho bài toán Machine Learning","id":1,"section":"posts","tags":["data cleaning"],"title":"Data Cleaning trong Machine Learning ","uri":"https://tranvanly107.github.io/posts/data-clearning/"},{"content":"1. Kiến thức cần có Trước khi bắt đầu với bài viết này, vui lòng chắc chắn rằng bạn có kiến thức về Python. Các thư viện cần thiết như:\n Pandas Numpy Sklearn\nCác bạn có thể dùng google colab hoặc jupyter để code nhé.  2. Các bước để làm một bài toán ML Hầu hết toàn bộ bài toán về Machine Learning đều trải qua các bước cơ bản sau: ( Mình sẽ không nói sâu về từng bước, các bạn cần tìm hiểu thêm nhé)\n Xác định vấn đề. Chuẩn bị dữ liệu Đánh giá thuật toán Cải thiện kết quả Hiển thị kết quả dự đoán  3. Bài toán Machine Learning Các bước chính để chúng ta xây dựng bài toán như sau:\n Cài đặt Python và SciPy platform. Tải dữ liệu về Hiểu dữ liệu Trực quan hóa dữ liệu Đánh giá một số thuật toán Dự đoán kết quả\nBây giờ chúng ta bắt đầu nhé.  3.1. Cài đặt thư viện cần thiết Các bạn tự cài đặt Python ( 3.x) và các thư viện sau:\n scipy numpy matplotlib pandas sklearn\nViệc cài đặt các thự viện này rất đơn giản nên các bạn tự tìm hiểu.\nCài đặt scipy tại đây\nSau khi các bạn cài xong thì các bạn test xem đã thành công hay chưa:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import sys\r# Python version\r print(\u0026#39;Python: {}\u0026#39;.format(sys.version))\r# scipy\r import scipy\rprint(\u0026#39;scipy: {}\u0026#39;.format(scipy.__version__))\r# numpy\r import numpy\rprint(\u0026#39;numpy: {}\u0026#39;.format(numpy.__version__))\r# matplotlib\r import matplotlib\rprint(\u0026#39;matplotlib: {}\u0026#39;.format(matplotlib.__version__))\r# pandas\r import pandas\rprint(\u0026#39;pandas: {}\u0026#39;.format(pandas.__version__))\r# scikit-learn\r import sklearn\rprint(\u0026#39;sklearn: {}\u0026#39;.format(sklearn.__version__))\r  Nếu các bạn đã cài thành công thì kết quả có dạng thế này:\n Python: 3.7.4 (default, Oct 19 2019, 05:21:45) [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]\nscipy: 1.4.1\nnumpy: 1.17.3\nmatplotlib: 3.2.0\npandas: 0.25.1\nsklearn: 0.22.2\n Còn nếu các bạn cài đặt bị lỗi thì có thể fix lỗi tại đây.\n3.2. Tải dữ liệu Trong bài viết này chúng ta sẽ sử dụng dữ liệu có sẵn trên google.\nCác bạn có thể tìm hiểu dữ liệu tại đây.\n3.2.1. import các thư viện cần thiết Tiếp theo chúng ta sẽ import các thư viện dưới đây để sự dụng trong bài toán.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Load libraries\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn.metrics import accuracy_score\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r  3.2.2. Tải dữ liệu về Chúng ta sẽ sử dụng pandas để tải dữ liệu về. Và cũng sẽ sử dụng pandas để khai phá và trực quan hóa dữ liệu.\n1 2 3 4  # Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r  Chúng ta sử dụng hàm read_csv và được trả về một dataframe, chúng ta sẽ xem kết quả sau.\nNếu các bạn gặp lỗi khi load thì bạn có thể download iris.csv.\n3.3. Khai phá dữ liệu Bây giờ chúng ta xem dữ liệu nó có gì nhé.\nChúng ta sẽ thực hiện một số cách tiếp cận dưới đây:\n Chiều của dữ liệu Tổng quan về dữ liệu Thống kê tất cả các thuộc tính của dữ liệu Phân bố của dữ liệu\nBây giờ chúng tiến hành đi thực hiện từng mục một nhé:  3.3.1. Chiều của dữ liệu Chiều của dữ liệu cho chúng ta biết là có bao nhiêu hàng và bao nhiêu cột của toàn bộ dữ liệu. Để xem chiều dữ liệu chúng ta sử dụng hàm shape trong pandas.\nprint(dataset.shape)\rOutput: (150,5)\nCó nghĩa là dữ liệu của chúng ta có 150 hàng và 5 cột. Nói cách khác là có 150 điểm dữ liệu và 4 features, 1 class chính là nhãn mà model cần dự đoán.\n3.3.2. Tổng quan dữ liệu Bây giờ chúng ta sẽ xem rõ hơn về dữ liệu nhé. Để xem chúng ta chỉ cần dùng hàm head trong pandas như sau:\nprint(dataset.head(20))\rHàm head giúp chúng ta hiển thị n bản ghi đầu tiên, ở đây chúng sẽ hiển thị 20 hàng đầu tiên của toàn bộ dữ liệu.\n    sepal-length sepal-width petal-length petal-width class     0 5.1 3.5 1.4 0.2 Iris-setosa   1 4.9 3.0 1.4 0.2 Iris-setosa   2 4.7 3.2 1.3 0.2 Iris-setosa   3 4.6 3.1 1.5 0.2 Iris-setosa   4 5.0 3.6 1.4 0.2 Iris-setosa   5 5.4 3.9 1.7 0.4 Iris-setosa   6 4.6 3.4 1.4 0.3 Iris-setosa   7 5.0 3.4 1.5 0.2 Iris-setosa   8 4.4 2.9 1.4 0.2 Iris-setosa   9 4.9 3.1 1.5 0.1 Iris-setosa   10 5.4 3.7 1.5 0.2 Iris-setosa   11 4.8 3.4 1.6 0.2 Iris-setosa   12 4.8 3.0 1.4 0.1 Iris-setosa   13 4.3 3.0 1.1 0.1 Iris-setosa   14 5.8 4.0 1.2 0.2 Iris-setosa   15 5.7 4.4 1.5 0.4 Iris-setosa   16 5.4 3.9 1.3 0.4 Iris-setosa   17 5.1 3.5 1.4 0.3 Iris-setosa   18 5.7 3.8 1.7 0.3 Iris-setosa   19 5.1 3.8 1.5 0.3 Iris-setosa    Đây là hình dạng của một dataframe. Các features của dữ liệu bao gồm: chiều dài đài hoa (sepal-length), chiều rộng đài hoa(sepal-width ), chiều dài cánh hoa (petal-length), chiều rộng cánh hoa(petal-width ).Và có 3 classes mà chúng ta cần dự đoán đó là: Iris-setosa, Iris-versicolor, Iris-virginica. Tất nhiên mỗi loài hoa sẽ có các features khác nhau để mô hình nó phân biệt và đưa ra dự đoán chính xác.\n3.3.3. Thống kê các thuộc tính của dữ liệu Bây giờ chúng ta sẽ xem chi tiết hơn về các features: Bao gồm count(số lượng), mean(chiều dài, chiều rộng trung bình), min và max. Cụ thể như sau:\nprint(dataset.describe())\rVà kết quả là:\n    sepal-length sepal-width petal-length petal-width     count 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667   std 0.828066 0.433594 1.764420 0.763161   min 4.300000 2.000000 1.000000 0.100000   25% 5.100000 2.800000 1.600000 0.300000   50% 5.800000 3.000000 4.350000 1.300000   75% 6.400000 3.300000 5.100000 1.800000   max 7.900000 4.400000 6.900000 2.500000    Như chung ta thấy ở trên độ dài và rộng của từng feature thuộc cùng đơn vị (cm) và cùng nằm trong khoảng từ 0-8 cm.\n3.3.4. Phân bố dữ liệu Bây giờ chúng ta xem số lượng của từng loài hoa ( từng class) bằng cách:\nprint(dataset.groupby('class').size()\rVà kết quả sẽ là:\n class\nIris-setosa 50\nIris-versicolor 50\nIris-virginica 50\n Chúng ta thấy số lượng hoa ( class) đồng đều nhau và cùng là 50.\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # summarize the data\r from pandas import read_csv\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# shape\r print(dataset.shape)\r# head\r print(dataset.head(20))\r# descriptions\r print(dataset.describe())\r# class distribution\r print(dataset.groupby(\u0026#39;class\u0026#39;).size())\r  3.4. Trực quan dữ liệu Chúng ta đã biết được cơ bản về dữ liệu, bây giờ là lúc chúng ta trực quan hóa bằng biểu đồ.\nBước này giúp chúng ta hiểu rõ hơn phân bố chi tiết từng loài hoa(class) bằng cách nhìn vào các biểu đồ sau:\n3.4.1. Univariate Plots Biểu đồ này giúp chúng ta hiểu được phân bố của từng feature.\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\rKết quả hiển thị như sau:\nChúng ta cũng có thể tạo ra các biểu đồ histogram với dữ liệu đầu vào:\n# histograms\rdataset.hist()\rpyplot.show()\rKết quả hiển thị như sau:\n3.4.2. Multivariate Plots Bây giờ chúng ta có thể thấy sự liên quan giữa các features. Chúng ta sử dụng scatter_matrix trong pandas để thấy được quan hệ của các features trên toàn bộ dữ liệu:\n# scatter plot matrix\rscatter_matrix(dataset)\rpyplot.show()\rChúng ta thấy được sự tương quan giữa chúng thông qua biểu đồ sau:\nCode đầy đủ cho các bước trên như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # visualize the data\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# box and whisker plots\r dataset.plot(kind=\u0026#39;box\u0026#39;, subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\r# histograms\r dataset.hist()\rpyplot.show()\r# scatter plot matrix\r scatter_matrix(dataset)\rpyplot.show()\r  3.5. Đánh giá thuật toán Bây giờ là lúc chúng ta sẽ thử chọn một vài model để xem độ chính xác của từng model trên tập test của chúng ta nhé.\nCác bước thực hiện như sau:\n Tách một phần dữ liệu để đánh giá model ( validation dataset) Thiết lập test harness để sử dụng 10-fold cross validation Chạy và dự đoán kết quả trên từng model Chọn ra model có kết quả tốt nhất để sử dụng cuối cùng\nok, bây giờ sẽ vào chi tiết nhé:  3.5.1. Tách một phần dữ liệu (validation Dataset) Chúng ta sẽ chia dữ liệu của chúng ta ra thành 2 phần: 80% dữ liệu dùng để huấn luyện model ( training data), 20% còn lại là dùng để đánh giá model(validation data)\n1 2 3 4 5  # Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\r  Chúng ta lấy bốn cột đầu tiên (X) là các features hay còn gọi là đặc tính của dữ liệu(attributes), cột cuối cùng là (y) nhãn của dữ liệu.\nBây giờ chúng ta đã chuẩn bị được dữ liệu đầy đủ cho việc training và testing rồi.\n3.5.2. Cross validation Chúng ta sẽ tạo ra 10 fold cross validation để estimate model.\nCụ thể là chúng ta chia dữ liệu train ra làm 10 phần bằng nhau, lấy 9 phần cho việc train và phần còn lại là để test trong lúc train. Để hiểu rõ hơn về cross validation thì các bạn có thể tìm hiểu tại đây nhé.\n3.5.3. Build models Chúng ta bây giờ sẽ không thể biết được model nào sẽ tốt nhất.\nSau đây là những model mình dùng để chạy và dự đoán độ chính xác.\n Logistic Regression (LR) Linear Discriminant Analysis (LDA) K-Nearest Neighbors (KNN). Classification and Regression Trees (CART). Gaussian Naive Bayes (NB). Support Vector Machines (SVM).\nCụ thể từng model thì mình sẽ cập nhật sau nếu có thời gian nhé.\nNào bây giờ chúng ta sẽ code và chạy nhé:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r  3.5.4. Chọn model có kết quả tốt nhất Bây giờ chúng ta đã chạy được 6 model và mỗi model sẽ cho ra độ chính xác khác nhau. Giờ là lúc chúng ta so sánh từng kết quả và chọn ra model có độ chính xác tốt nhất.\nChạy xong đoạn code trên thì chúng ta nhận được kết quả sau:\n LR: 0.960897 (0.052113)\nLDA: 0.973974 (0.040110)\nKNN: 0.957191 (0.043263)\nCART: 0.957191 (0.043263)\nNB: 0.948858 (0.056322)\nSVM: 0.983974 (0.032083)\n Như chúng ta thấy độ chính xác của model Support Vector Machine(SVM)\nđạt kết quả với độ chính xác lên đến hơn 98% và là kết quả tốt nhất.\nToàn bộ code cho bài viết như sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # compare algorithms\r from pandas import read_csv\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\r# Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r# Compare Algorithms\r pyplot.boxplot(results, labels=names)\rpyplot.title(\u0026#39;Algorithm Comparison\u0026#39;)\rpyplot.show()\r  3.5.5. Dự đoán kết quả Bây giờ chúng ta chọn một model tốt nhất để dự đoán. Thì theo như kết quả ở trên thì chúng ta sẽ chọn SVM là model cuối cùng để dự đoán.\nChúng ta sẽ dự đoán dựa trên tập validation set mà chúng ta đã tách ở mục 3.5.1.\nĐây là dữ liệu không hề liên quan tới model trong quá trình training. Mục đích của việc dự đoán trên tập dữ liệu này để đánh giá xem model chúng ta chọn có thực sự dự đoán tốt hay không, để từ đó biết được nó có bị overfitting hay không.\nĐầu tiên chúng ta fit model trên toàn bộ tập train và dự đoán trên toàn bộ tập validation.\n1 2 3 4 5 6  # Make predictions on validation dataset\r model = SVC(gamma=\u0026#39;auto\u0026#39;)\rmodel.fit(X_train, Y_train)\rpredictions = model.predict(X_validation)\r# Evaluate predictions\r print(accuracy_score(Y_validation, predictions))\r  Kết quả của chúng ta là:\n 0.9666666666666667\n[[11 0 0]\n[ 0 12 1]\n[ 0 0 6]]\n Hoặc chúng ta có thể lên google download ảnh của một loài hoa thuộc một trong 3 loại hoa ở trên và cho vào model để xem model dự đoán xem là loài hoa gì nhé.\nChúng ta có thể sử dụng precision, recall\u0026hellip; để show kết quả.\nprint(confusion_matrix(Y_validation, predictions))\rprint(classification_report(Y_validation, predictions))\rVà kết quả:\n    precision recall f1-score support     Iris-setosa 1.00 1.00 1.00 11   Iris-versicolor 1.00 0.92 0.96 13   Iris-virginica 0.86 1.00 0.92 6       accuracy  0.97 30     macro avg 0.95 0.97 0.96 30   weighted avg 0.97 0.97 0.97 30    Vậy là chúng ta đã cơ bản hoàn thành bài toán đầu tiên của Machine Learning.\n4. Tổng Kết  Bài viết này nhằm mục đích giúp các bạn hiểu được luồng hoạt động hay nói cách khác là cách để làm một bài toán machine learning nên mình sẽ không đi chi tiết vào từng dòng code. Mặc định ban đầu là các bạn đã có kiến thức cơ bản mà bài viết đã yêu cầu nên mình cũng không nói rõ từng thư viện, từng khái niệm trong bài viết nữa. Đây là bài viết đầu tiên trong loạt bài ML cơ bản nên có rất nhiều hạn chế và thiếu sót, rất mong được sự góp ý của các bạn( mình sẽ cải thiện dần trong những bài viết sau). Nhưng hy vọng qua bài viết này các bạn đã biết cách xây dựng một bài toán machine learning.  Cảm ơn tất cả các bạn đã đọc bài viết. Bài viết tiếp theo sẽ là cách xử lý dữ liệu, cleaning data như thế nào. Mong các bạn đón đọc!!!\n","description":"Đây là bài toán đầu tiên trong loạt bài ML cơ bản","id":2,"section":"posts","tags":["tutorial","machine learning"],"title":"Giải bài toán Machine Learning đầu tiên ","uri":"https://tranvanly107.github.io/posts/first-machine-learning/"}]