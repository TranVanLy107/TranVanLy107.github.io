[{"content":"Khi ch√∫ng ta l√†m vi·ªác v·ªõi Machine Learning v√† Deep Learning th√¨ ch√∫ng ta th∆∞·ªùng xuy√™n b·∫Øt g·∫∑p c√°c thu·∫≠t ng·ªØ Epoch - Batch size v√† Iterations. V√† th∆∞·ªùng s·∫Ω kh√¥ng hi·ªÉu r√µ ch√∫ng l√† g√¨, v√† d√πng ƒë·ªÉ l√†m g√¨. V·∫≠y trong b√†i vi·∫øt n√†y ch√∫ng ta s·∫Ω ƒëi t√¨m hi·ªÉu r√µ h∆°n v·ªÅ 3 kh√°i ni·ªám n√†y nh√©.\nTr∆∞·ªõc khi ƒëi v√†o chi ti·∫øt 3 kh√°i ni·ªám n√†y th√¨ m√¨nh s·∫Ω gi·ªõi thi·ªáu cho c√°c b·∫°n m·ªôt kh√°i ni·ªám kh√¥ng th·ªÉ thi·∫øu trong c√°c d·ª± √°n machine learning ƒë√≥ l√† Gradient Descent v√† c√°c thu·∫≠t to√°n ph·ªï bi·∫øn c·ªßa n√≥.\nV·∫≠y Gradient Descent l√† g√¨?\nGradient Descent l√† m·ªôt thu·∫≠t to√°n t·ªëi ∆∞u l·∫∑p (iterative optimization algorithm) ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c b√†i to√°n Machine Learning v√† Deep Learning (th∆∞·ªùng l√† c√°c b√†i to√°n t·ªëi ∆∞u l·ªìi ‚Äî Convex Optimization) v·ªõi m·ª•c ti√™u l√† t√¨m m·ªôt t·∫≠p c√°c bi·∫øn n·ªôi t·∫°i (internal parameters) cho vi·ªác t·ªëi ∆∞u models. Trong ƒë√≥:\n Gradient: l√† t·ª∑ l·ªá ƒë·ªô nghi√™ng c·ªßa ƒë∆∞·ªùng d·ªëc (rate of inclination or declination of a slope). V·ªÅ m·∫∑t to√°n h·ªçc, Gradient c·ªßa m·ªôt h√†m s·ªë l√† ƒë·∫°o h√†m c·ªßa h√†m s·ªë ƒë√≥ t∆∞∆°ng ·ª©ng v·ªõi m·ªói bi·∫øn c·ªßa h√†m. ƒê·ªëi v·ªõi h√†m s·ªë ƒë∆°n bi·∫øn, ch√∫ng ta s·ª≠ d·ª•ng kh√°i ni·ªám Derivative thay cho Gradient. Descent: l√† t·ª´ vi·∫øt t·∫Øt c·ªßa descending, nghƒ©a l√† gi·∫£m d·∫ßn.  Gradient Descent c√≥ nhi·ªÅu d·∫°ng kh√°c nhau nh∆∞: Batch Gradient Descent (ho·∫∑c l√† Gradient Descent), Stochastic Gradient Descent, Mini-batch Gradient Descent. V·ªÅ c∆° b·∫£n th√¨ ch√∫ng th·ª±c thi nh∆∞ nhau\n Kh·ªüi t·∫°o bi·∫øn n·ªôi t·∫°i. ƒê√°nh gi√° model d·ª±a v√†o bi·∫øn n·ªôi t·∫°i v√† h√†m m·∫•t m√°t (Loss function). C·∫≠p nh·∫≠t c√°c bi·∫øn n·ªôi t·∫°i theo h∆∞·ªõng t·ªëi ∆∞u h√†m m·∫•t m√°t (finding optimal points). L·∫∑p l·∫°i b∆∞·ªõc 2, 3 cho t·ªõi khi th·ªèa ƒëi·ªÅu ki·ªán d·ª´ng.  C√¥ng th·ª©c c·∫≠p nh·∫≠t cho GD c√≥ th·ªÉ ƒë∆∞·ª£c vi·∫øt l√†:\ntrong ƒë√≥ Œ∏ l√† t·∫≠p c√°c bi·∫øn c·∫ßn c·∫≠p nh·∫≠t, Œ∑ l√† t·ªëc ƒë·ªô h·ªçc (learning rate), ‚ñΩ”®f(Œ∏) l√† Gradient c·ªßa h√†m m·∫•t m√°t f theo t·∫≠p Œ∏.\nGradient Descent c√≥ th·ªÉ ƒë∆∞·ª£c minh h·ªça trong h√¨nh d∆∞·ªõi ƒë√¢y:\nNh∆∞ng n√≥ kh√°c nhau khi c·∫≠p nh·∫≠t th√¥ng s·ªë sau c√°c l·∫ßn train v√† ph·ª• thu·ªôc v√†o l∆∞·ª£ng d·ªØ li·ªáu cho m·ªói l·∫ßn c·∫≠p nh·∫≠t Œ∏.\n  Batch Gradient Descent(BGD): Batch ·ªü ƒë√¢y c√≥ nghƒ©a l√† t·∫•t c·∫£ t·ª©c l√† khi c·∫≠p nh·∫≠t Œ∏ = w, ch√∫ng ta s·ª≠ d·ª•ng t·∫•t c·∫£ c√°c ƒëi·ªÉm d·ªØ li·ªáu c√≥ s·∫µn (xi). C√≥ nghƒ©a l√† m·ªói epoch ·ª©ng v·ªõi m·ªôt l·∫ßn c·∫≠p nh·∫≠t Œ∏.\nH·∫°n ch·∫ø: Vi·ªác ph·∫£i t√≠nh to√°n ƒë·∫°o h√†m v·ªõi t·∫•t c·∫£ c√°c ƒëi·ªÉm sau m·ªói l·∫ßn l·∫∑p tr·ªü n√™n kh√≥ khƒÉn v√† kh√¥ng hi·ªáu qu·∫£ v·ªõi online learning.\nOnline learning: khi c∆° s·ªü d·ªØ li·ªáu c·∫≠p nh·∫≠t li√™n t·ª•c, m·ªói l·∫ßn th√™m v√†i ƒëi·ªÉm d·ªØ li·ªáu m·ªõi th√¨ b·∫Øt bu·ªôc m√¥ h√¨nh ch√∫ng ta ph·∫£i c·∫≠p nh·∫≠t, thay ƒë·ªïi ƒë·ªÉ ph√π h·ª£p v·ªõi l∆∞·ª£ng d·ªØ li·ªáu m·ªõi n√†y.\n  Stochastic Gradient Descent(SGD): t·∫°i m·ªôt th·ªùi ƒëi·ªÉm, ta ch·ªâ t√≠nh ƒë·∫°o h√†m c·ªßa h√†m m·∫•t m√°t d·ª±a tr√™n ch·ªâ 1 ƒëi·ªÉm d·ªØ li·ªáu xi r·ªìi c·∫≠p nh·∫≠t Œ∏ d·ª±a tr√™n ƒë·∫°o h√†m n√†y. C√≥ nghƒ©a l√† m·ªói epoch ·ª©ng v·ªõi N l·∫ßn c·∫≠p nh·∫≠t Œ∏ ( N l√† s·ªë ƒëi·ªÉm d·ªØ li·ªáu)\nH·∫°n ch·∫ø: vi·ªác c·∫≠p nh·∫≠t t·ª´ng ƒëi·ªÉm m·ªôt nh∆∞ th·∫ø n√†y th√¨ c√≥ th·ªÉ l√†m gi·∫£m ƒëi t·ªëc ƒë·ªô th·ª±c hi·ªán 1 epoch.\n∆Øu ƒëi·ªÉm: v·ªõi ph∆∞∆°ng ph√°p n√†y ch√∫ng ta ch·ªâ c·∫ßn m·ªôt l∆∞·ª£ng nh·ªè s·ªë epoch sau ƒë√≥ n·∫øu c√≥ d·ªØ li·ªáu m·ªõi th√¨ ch·ªâ c·∫ßn ch·∫°y 1 epoch. Ph√π h·ª£p v·ªõi c√°c b√†i to√°n c√≥ d·ªØ li·ªáu l·ªõn.\n  Mini-batch Gradient Descent(M-BGD): kh√°c v·ªõi SGD, mini-batch s·ª≠ d·ª•ng m·ªôt l∆∞·ª£ng n l·ªõn h∆°n 1(nh∆∞ng nh·ªè h∆°n t·ªïng s·ªë d·ªØ li·ªáu ban ƒë·∫ßu r·∫•t nhi·ªÅu). Mini-batch b·∫Øt ƒë·∫ßu v·ªõi m·ªói epoch b·∫±ng vi·ªác x√°o tr·ªôn ng·∫´u nhi√™n r·ªìi chia d·ªØ li·ªáu th√†nh mini-batch, m·ªói mini-batch c√≥ n ƒëi·ªÉm d·ªØ li·ªáu(tr·ª´ mini-batch cu·ªëi c√πng c√≥ th·ªÉ √≠t h∆°n v√¨ N c√≥ th·ªÉ kh√¥ng chia h·∫øt cho n). Thu·∫≠ to√°n n√†y l·∫•y ra m·ªôt mini-batch ƒë·ªÉ c·∫≠p nh·∫≠t Œ∏, n√≥ ƒë∆∞·ª£c s·ª≠ d·ª•ng trong h·∫ßu h·∫øt c√°c thu·∫≠t to√°n machine learning v√† ƒë·∫∑c bi·ªát l√† deep learning.\n  ok, b√¢y gi·ªù ch√∫ng ta s·∫Ω t√¨m hi·ªÉu chi ti·∫øt v·ªÅ n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt nh√©.\n  Sample: sample l√† m·ªôt d√≤ng d·ªØ li·ªáu ri√™ng bi·ªát. Bao g·ªìm c√°c inputs ƒë·ªÉ ƒë∆∞a v√†o thu·∫≠t to√°n, m·ªôt output (ground-truth) ƒë·ªÉ so s√°nh v·ªõi gi√° tr·ªã d·ª± ƒëo√°n v√† t√≠nh gi√° tr·ªã c·ªßa h√†m m·∫•t m√°t. D·ªØ li·ªáu hu·∫•n luy·ªán th∆∞·ªùng bao g·ªìm nhi·ªÅu samples. Sample c√≤n ƒë∆∞·ª£c g·ªçi l√† instance, an observation, an input vector, hay a feature vector.\n  Batch\nNh∆∞ ƒë√£ n√≥i ·ªü tr√™n, m·ªôt t·∫≠p training set c√≥ th·ªÉ ƒë∆∞·ª£c chia nh·ªè th√†nh c√°c batchs. M·ªói batch s·∫Ω ch·ª©a c√°c training samples, v√† s·ªë l∆∞·ª£ng c√°c samples n√†y ƒë∆∞·ª£c g·ªçi l√† batch size. Ch√∫ng ta c·∫ßn l∆∞u √Ω r·∫±ng batch size v√† number of batches ( s·ªë l∆∞·ª£ng c√°c batches) l√† ho√†n to√†n kh√°c nhau. T√πy thu·ªôc v√† l∆∞·ª£ng batch size m√† GD s·∫Ω c√≥ nhi·ªÅu bi·∫øn th·ªÉ kh√°c nhau:\n  Batch Gradient Descent: Batch Size = Size of Training Dataset\n  Stochastic Gradient Descent: Batch Size = 1\n  Mini-Batch Gradient Descent: 1 \u0026lt; Batch Size \u0026laquo; Size of Training Set\nƒê·ªëi v·ªõi Mini-Batch Gradient Descent th√¨ batch size th∆∞·ªùng ƒë∆∞·ª£c ch·ªçn l√† l≈©y th·ª´a c·ªßa 2 (32, 64, 128, 256‚Ä¶) v√¨ t·ªëc ƒë·ªô t√≠nh to√°n s·∫Ω t·ªëi ∆∞u cho c√°c arithmetic algorithms c·ªßa CPU v√† GPU. C√°ch ch·ªçn batch size c≈©ng t√πy theo y√™u c·∫ßu c·ªßa b√†i to√°n.\nv√≠ d·ª•: B·∫°n c√≥ m·ªôt t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán g·ªìm 64.000 images, b·∫°n mu·ªën c√≥ s·ªë l∆∞·ª£ng batches l√† 1000 th√¨ khi ƒë√≥ batch size s·∫Ω l√† 64 (64 images)\n    Epoch\nEpoch l√† m·ªôt hyperparameter trong ANN, ƒë∆∞·ª£c d√πng ƒë·ªÉ ƒë·ªãnh nghƒ©a s·ªë l·∫ßn learning algorithm ho·∫°t ƒë·ªông tr√™n model, m·ªôt epoch ho√†n th√†nh l√† khi t·∫•t c·∫£ d·ªØ li·ªáu training ƒë∆∞·ª£c ƒë∆∞a v√†o m·∫°ng neural network m·ªôt l·∫ßn (ƒë√£ bao g·ªìm c·∫£ 2 b∆∞·ªõc forward v√† backward cho vi·ªác c·∫≠p nh·∫≠t internal model parameters).\nTuy nhi√™n khi d·ªØ li·ªáu training l√† qu√° l·ªõn (v√≠ d·ª• training images t·ª´ ImageNet, Google Open Images), vi·ªác ƒë∆∞a t·∫•t c·∫£ training data v√†o trong 1 epoch l√† kh√¥ng kh·∫£ thi v√† kh√¥ng hi·ªáu qu·∫£. Tr∆∞·ªùng h·ª£p s·ªë epoch nh·ªè th√¨ d·ªÖ d·∫´n ƒë·∫øn underfitting v√¨ model kh√¥ng ‚Äúh·ªçc‚Äù ƒë∆∞·ª£c nhi·ªÅu t·ª´ GD ƒë·ªÉ c·∫≠p nh·∫≠t c√°c bi·∫øn n·ªôi t·∫°i. ƒê·ªëi v·ªõi c√°c tr∆∞·ªùng h·ª£p n√†y th√¨ gi·∫£i ph√°p l√† chia nh·ªè training dataset ra th√†nh c√°c batches cho m·ªói epoch th√¨ c∆° h·ªôi model h·ªçc ƒë∆∞·ª£c t·ª´ GD s·∫Ω nhi·ªÅu h∆°n v√† t·ªëc ƒë·ªô t√≠nh to√°n s·∫Ω t·ªëi ∆∞u h∆°n.\nCh·ªçn s·ªë epoch nh∆∞ th·∫ø n√†o? Th∆∞·ªùng ch√∫ng ta c·∫ßn m·ªôt s·ªë l∆∞·ª£ng l·ªõn epoch ƒë·ªÉ training cho ANN (10, 100, 500, 1000‚Ä¶) tuy nhi√™n c≈©ng c√≤n t√πy thu·ªôc v√†o b√†i to√°n v√† t√†i nguy√™n m√°y t√≠nh. M·ªôt c√°ch kh√°c l√† s·ª≠ d·ª•ng Learning Curve ƒë·ªÉ t√¨m s·ªë epoch.\n  Iterations\nIteration l√† s·ªë l∆∞·ª£ng batches (number of batches) c·∫ßn thi·∫øt ƒë·ªÉ ho√†n th√†nh m·ªôt epoch. C√¥ng th·ª©c t√≠nh l√† iterations = training samples/batch size. V√≠ d·ª•: Gi·∫£ s·ª≠ b·∫°n c√≥ t·∫≠p hu·∫•n luy·ªán g·ªìm 64.000 images, l·ª±a ch·ªçn batch size c√≥ gi√° tr·ªã l√† 64 images. ƒê·ªìng nghƒ©a: m·ªói l·∫ßn c·∫≠p nh·∫≠t tr·ªçng s·ªë, b·∫°n s·ª≠ d·ª•ng 64 images. Khi ƒë√≥, b·∫°n m·∫•t 64.000/64 = 1000 iterations ƒë·ªÉ c√≥ th·ªÉ duy·ªát qua h·∫øt ƒë∆∞·ª£c t·∫≠p hu·∫•n luy·ªán (ƒë·ªÉ ho√†n th√†nh 1 epoch). Khi ƒë√≥ model s·∫Ω c√≥ c∆° h·ªôi c·∫≠p nh·∫≠t c√°c bi·∫øn n·ªôi t·∫°i 1000 l·∫ßn, nh√¢n v·ªõi s·ªë l∆∞·ª£ng epoch m√† ch√∫ng ta mu·ªën th√¨ s·∫Ω ƒë∆∞·ª£c t·ªïng s·ªë l·∫ßn c·∫≠p nh·∫≠t cho vi·ªác ho√†n th√†nh m·ªôt m√¥ h√¨nh hu·∫•n luy·ªán.\n  T·ªïng k·∫øt\nNh∆∞ v·∫≠y ch√∫ng ta ƒë√£ t√¨m hi·ªÉu r√µ v·ªÅ 3 thu·∫≠t ng·ªØ s·∫Ω th∆∞·ªùng xuy√™n g·∫∑p khi l√†m vi·ªác v·ªõi machine learning v√† deep learning. ƒê√≥ m·ªõi ch·ªâ l√† vi·ªác hi·ªÉu v·ªÅ kh√°i ni·ªám, c√≤n vi·ªác √°p d·ª•ng th√¨ c√≤n ph·∫£i t√πy thu·ªôc v√†o b√†i to√°n th·ª±c t·∫ø m√† ch√∫ng ta l√†m, ph·ª• thu·ªôc v√†o t√†i nguy√™n m√°y t√≠nh, l∆∞·ª£ng d·ªØ li·ªáu ch√∫ng ta c√≥, v√† k·∫øt qu·∫£ ch√∫ng ta mong mu·ªën\u0026hellip; ƒê·ªÉ l√†m ƒë∆∞·ª£c th√¨ ch√∫ng ta ph·∫£i th∆∞·ªùng xuy√™n practice.\nHy v·ªçng qua b√†i vi·∫øt n√†y ch√∫ng ta ƒë√£ hi·ªÉu r√µ v·ªÅ c√°c thu·∫≠t ng·ªØ tr√™n v√† √°p d·ª•ng ch√∫ng v√†o c√°c d·ª± √°n th·ª±c t·∫ø m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t.\nT√†i li·ªáu tham kh·∫£o\n  https://medium.com/@ewuramaminka/epoch-iterations-batch-size-11fbbd4f0771\n  https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n  https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9\n  ","description":"T√¨m hi·ªÉu v·ªÅ kh√°i ni·ªám batch v√† epoch v√† s·ª± kh√°c nhau gi·ªØa ch√∫ng","id":0,"section":"posts","tags":["batch size","epoch"],"title":"Ph√¢n bi·ªát Batch v√† Epoch trong Neural Network","uri":"https://tranvanly107.github.io/posts/batch-and-epoch/"},{"content":"Nh∆∞ ch√∫ng ta ƒë√£ bi·∫øt ƒë·ªÉ hu·∫•n luy·ªán v√† ƒë√°nh gi√° m·ªôt m√¥ h√¨nh machine learning ho√†n ch·ªânh ch√∫ng ta c·∫ßn chia d·ªØ li·ªáu th√†nh 3 ph·∫ßn: training, validation v√† test. V·∫≠y tr∆∞·ªõc khi t√¨m hi·ªÉu v·ªÅ K-Fold Cross Validation (K-Fold CV) th√¨ ch√∫ng ta s·∫Ω t√¨m hi·ªÉu kh√°i ni·ªám c·ªßa 3 th√†nh ph·∫ßn ·ªü tr√™n.\n Training set: training set l√† t·∫≠p d·ªØ li·ªáu d√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh, bao g·ªìm d·ªØ li·ªáu ƒë·∫ßu v√†o v√† nh√£n. V·ªõi training set, m√¥ h√¨nh c√≥ th·ªÉ th·∫•y t·∫•t c·∫£ d·ªØ li·ªáu v√† nh√£n. N√≥ s·ª≠ d·ª•ng d·ªØ li·ªáu n√†y ƒë·ªÉ t·ªëi ∆∞u loss function th√¥ng qua vi·ªác ƒëi·ªÅu ch·ªânh parameter. Validation set: d·ªØ li·ªáu t·∫≠p n√†y gi·ªëng v·ªõi training set, nh∆∞ng m√¥ h√¨nh kh√¥ng h·ªÅ ƒë∆∞·ª£c nh√¨n th·∫•y nh√£n. M√¥ h√¨nh ƒë∆°n thu·∫ßn d√πng d·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa validation set ƒë·ªÉ t√≠nh to√°n ra output, sau ƒë√≥ n√≥ so s√°nh v·ªõi nh√£n ƒë·ªÉ t√≠nh ra loss function. Parameter ho√†n to√†n kh√¥ng ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh ·ªü b∆∞·ªõc n√†y. Validation set l√† b·ªô d·ªØ li·ªáu ƒë·ªÉ ch√∫ng ta gi√°m s√°t m√¥ h√¨nh. Ch√∫ng ta s·ª≠ d·ª•ng k·∫øt qu·∫£ m√¥ h√¨nh ·ªü training set ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh nh∆∞ ƒëi·ªÅu ch·ªânh hyperparameter, b·ªï sung th√™m d·ªØ li·ªáu ƒë·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh t·ªët nh·∫•t v√† tr√°nh overfiting. Test set: test set ch·ªâ c√≥ d·ªØ li·ªáu ƒë·∫ßu v√†o m√† kh√¥ng c√≥ nh√£n. M·ª•c ƒë√≠ch ch√≠nh c·ªßa test set l√† ƒë√°nh gi√° k·∫øt qu·∫£ m√¥ h√¨nh cu·ªëi c√πng, n√≥ ƒë√°nh gi√° xem m√¥ h√¨nh c√≥ th·ª±c s·ª± t·ªët trong th·ª±c t·∫ø hay kh√¥ng. N·∫øu m√¥ h√¨nh l√†m t·ªët ·ªü training set v√† validation set m√† kh√¥ng l√†m t·ªët ·ªü test set th√¨ kh√¥ng c√≥ √Ω nghƒ©a g√¨ trong th·ª±c t·∫ø.  T·ªõi ƒë√¢y s·∫Ω c√≥ nhi·ªÅu b·∫°n th·∫Øc m·∫Øc r·∫±ng t·∫°i sao kh√¥ng hu·∫•n luy·ªán tr√™n t·∫≠p training set v√† evaluate tr√™n t·∫≠p test set?\n L√Ω do l√† nh∆∞ sau: vi·ªác ph√°t tri·ªÉn m·ªôt m√¥ h√¨nh d·ª±a tr√™n vi·ªác ƒëi·ªÅu ch·ªânh c√°c si√™u tham s·ªë ( vd: ch·ªçn s·ªë l∆∞·ª£ng layers, k√≠ch th∆∞·ªõc c·ªßa t·ª´ng layer\u0026hellip;), th·ª±c hi·ªán ƒëi·ªÅu ch·ªânh c√°c si√™u tham s·ªë n√†y d·ª±a tr√™n t√≠n hi·ªáu ph·∫£n h·ªìi v·ªÅ performance c·ªßa validation set. Sau khi c√≥ m·ªôt m√¥ h√¨nh ƒë·ªß t·ªët d·ª±a tr√™n validation set th√¨ b·∫°n quan t√¢m ƒë·∫øn performance c·ªßa m√¥ h√¨nh tr√™n m·ªôt b·ªô d·ªØ li·ªáu ho√†n to√†n m·ªõi m√† m√¥ h√¨nh ch∆∞a nh√¨n th·∫•y bao gi·ªù, v√† b·ªô d·ªØ li·ªáu ƒë√≥ l√† t·∫≠p test set.\n=\u0026gt; Ch√≠nh v√¨ th·∫ø ch√∫ng ta kh√¥ng n√™n d√πng test set ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh trong l√∫c train ƒë·ªÉ ngƒÉn ch·∫∑n vi·ªác r√≤ r·ªâ th√¥ng tin. Ngo√†i ra s·ª≠ d·ª•ng test set ƒë·ªÉ ƒëi·ªÅu ch·ªânh m√¥ h√¨nh v√† sau khi ho√†n th√†nh m√¥ h√¨nh b·∫°n ƒë√°nh gi√° m√¥ h√¨nh cu·ªëi c√πng b·∫±ng test set th√¨ s·∫Ω b·ªã thi·∫øu s√≥t, n√≥ kh√¥ng ƒë√∫ng v·ªõi m·ª•c ƒë√≠ch c·ªßa vi·ªác x√¢y d·ª±ng m√¥ h√¨nh t·ªët nh·∫•t.  OK, b√¢y gi·ªù ch√∫ng ta s·∫Ω ƒëi v√†o n·ªôi dung ch√≠nh c·ªßa ti√™u ƒë·ªÅ b√†i vi·∫øt.\nk-Fold Cross-Validation l√† g√¨?\nCross validation l√† m·ªôt k·ªπ thu·∫≠t l·∫•y m·∫´u ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh h·ªçc m√°y trong tr∆∞·ªùng h·ª£p d·ªØ li·ªáu kh√° h·∫°n ch·∫ø.\nTham s·ªë quan tr·ªçng trong k·ªπ thu·∫≠t n√†y ƒë∆∞·ª£c g·ªçi l√† k, ƒë·∫°i di·ªán cho s·ªë nh√≥m m√† d·ªØ li·ªáu s·∫Ω ƒë∆∞·ª£c chia ra. V√¨ l√Ω do ƒë√≥ m√† n√≥ ƒë∆∞·ª£c g·ªçi l√† k-fold cross-validation.\nK·ªπ thu·∫≠t n√†y ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi c√°c b∆∞·ªõc nh∆∞ sau:\n X√°o tr·ªôn data set m·ªôt c√°ch ng·∫´u nhi√™n Chia data set th√†nh k nh√≥m V·ªõi m·ªói nh√≥m  S·ª≠ d·ª•ng m·ªôt nh√≥m g·ªçi l√† test set ƒë·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ m√¥ h√¨nh C√°c nh√≥m c√≤n l·∫°i ƒë∆∞·ª£c g·ªçi l√† training set s·ª≠ d·ª•ng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n training set v√† ƒë√°nh gi√° n√≥ tr√™n test set Nh·∫≠n k·∫øt qu·∫£ ƒë√°nh gi√° v√† lo·∫°i b·ªè m√¥ h√¨nh   T·ªïng h·ª£p l·∫°i k·∫øt qu·∫£ c·ªßa m√¥ h√¨nh d·ª±a tr√™n c√°c s·ªë li·ªáu ƒë√°nh gi√°  M·ªôt l∆∞u √Ω quan tr·ªçng: M·ªói m·∫´u ch·ªâ ƒë∆∞·ª£c g√°n cho duy nh·∫•t 1 nh√≥m v√† ph·∫£i ·ªü nguy√™n trong nh√≥m ƒë√≥ cho ƒë·∫øn h·∫øt qu√° tr√¨nh.\nVi·ªác h·ªßy m√¥ h√¨nh sau m·ªói l·∫ßn ƒë√°nh gi√° l√† b·∫Øt bu·ªôc, tr√°nh tr∆∞·ªùng h·ª£p m√¥ h√¨nh ghi nh·ªõ nh√£n c·ªßa t·∫≠p test trong l·∫ßn ƒë√°nh gi√° tr∆∞·ªõc. C√°c l·ªói thi·∫øt l·∫≠p n√†y d·ªÖ x·∫£y ra v√† ƒë·ªÅu d·∫´n ƒë·∫øn k·∫øt qu·∫£ ƒë√°nh gi√° kh√¥ng ch√≠nh x√°c (th∆∞·ªùng l√† t√≠ch c·ª±c h∆°n so v·ªõi th·ª±c t·∫ø).\nK·∫øt qu·∫£ t·ªïng h·ª£p th∆∞·ªùng l√† trung b√¨nh c·ªßa c√°c l·∫ßn ƒë√°nh gi√°. Ngo√†i ra vi·ªác b·ªï sung th√¥ng tin v·ªÅ ph∆∞∆°ng sai v√† ƒë·ªô l·ªách chu·∫©n v√†o k·∫øt qu·∫£ t·ªïng h·ª£p c≈©ng ƒë∆∞·ª£c s·ª≠ d·ª•ng trong th·ª±c t·∫ø.\nƒê·ªÉ cho c√°c b·∫°n d·ªÖ hi·ªÉu m√¨nh s·∫Ω l·∫•y m·ªôt h√¨nh ·∫£nh tr√™n google.\nNh√¨n h√¨nh tr√™n ch√∫ng ta th·∫•y, d·ªØ li·ªáu ƒë∆∞·ª£c chia th√†nh 10 nh√≥m c√≥ c√πng k√≠ch c·ª° hay c√≤n g·ªçi l√† 10-fold cross-validation.\n Ch√∫ng ta l√¢y 9 nh√≥m: d√πng ƒë·ªÉ hu·∫•n luy·ªán ( training set) ƒë∆∞·ª£c th·ªÉ hi·ªán b·ªüi c√°c √¥ m√†u x√°m tr·∫Øng ·ªü tr√™n. Nh√≥m c√≤n l·∫°i l√† ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh( gi·ªëng validation set) ƒë∆∞·ª£c th·ªÉ hi·ªán b·ªüi √¥ m√†u xanh ƒë·∫≠m.\nCh√∫ng ta s·∫Ω hu·∫•n luy·ªán 10 l·∫ßn v√† t√≠nh to√°n s·ª± kh√°c nhau gi·ªØ s·ªë fold tr√™n t·ª´ng l·∫ßn training. K·∫øt qu·∫£ c·ªßa m√¥ h√¨nh l√† trung b√¨nh 10 k·∫øt qu·∫£ c·ªßa 10 l·∫ßn trained.  V·∫≠y ch·ªçn gi√° tr·ªã k nh∆∞ th·∫ø n√†o?\nCh·ªçn gi√° tr·ªã k l√† m·ªôt vi·ªác c·ª±c k·ª≥ quan tr·ªçng, n·∫øu ch·ªçn kh√¥ng ph√π h·ª£p s·∫Ω ·∫£nh ∆∞·ªüng ƒë·∫øn hi·ªáu qu·∫£ qu·∫£ m√¥ h√¨nh th·∫≠m ch√≠ s·∫Ω d·∫´n ƒë·∫øn overfitting ho·∫∑c underfitting.\nC√≥ 3 chi·∫øn thu·∫≠t ph·ªï bi·∫øn ƒë·ªÉ l·ª±a ch·ªçn gi√° tr·ªã k:\n ƒê·∫°i di·ªán: Gi√° tr·ªã c·ªßa k ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ m·ªói t·∫≠p train/test ƒë·ªß l·ªõn, c√≥ th·ªÉ ƒë·∫°i di·ªán v·ªÅ m·∫∑t th·ªëng k√™ cho dataset ch·ª©a n√≥. k = 10: Th√¥ng qua th·ª±c nghi·ªám cho th·∫•y gi√° tr·ªã k th∆∞·ªùng ƒë∆∞·ª£c l·ª±a ch·ªçn l√† 10 v√† ƒë√£ ƒë∆∞·ª£c ch·ª©ng minh l√† n√≥ cho ra m·ªôt k·∫øt qu·∫£ ch√≠nh x√°c h∆°n, √≠t sai s·ªë( overfitting v√† underfitting). k=n: Gi√° tr·ªã c·ªßa k ƒë∆∞·ª£c g√°n c·ªë ƒë·ªãnh b·∫±ng n , v·ªõi n l√† k√≠ch th∆∞·ªõc c·ªßa dataset, nh∆∞ v·∫≠y m·ªói m·∫´u s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh m·ªôt l·∫ßn. C√°ch ti·∫øp c·∫≠n n√†y c√≤n c√≥ t√™n g·ªçi l√† leave-one-out cross-validation.  L·ª±a ch·ªçn gi√° tr·ªã k c√≤n ph·ª• thu·ªôc v√†o nhi·ªÅu y·∫øu t·ªë quan tr·ªçng kh√°c nh∆∞ l∆∞·ª£ng d·ªØ li·ªáu m√† b·∫°n ƒëang s·ª≠ d·ª•ng\u0026hellip; Nh∆∞ng k =10 l√† m·ªôt l·ª±a ch·ªçn r·∫•t ph·ªï bi·∫øn. B·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng gi√° tr·ªã n√†y n·∫øu nh∆∞ g·∫∑p kh√≥ khƒÉn ƒë·ªÉ l·ª±a ch·ªçn m·ªôt gi√° tr·ªã th√≠ch h·ª£p cho b√†i to√°n c·ªßa b·∫°n. Th∆∞ vi·ªán scikit-learnin cung c·∫•p c√°c c√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß c·ªßa cross-validation. B·∫°n c√≥ th·ªÉ tham kh·∫£o t·∫°i ƒë√¢y\nKhi n√†o th√¨ n√™n d√πng K-Fold CV?\nNh∆∞ m√¨nh ƒë√£ n√≥i ·ªü tr√™n, K-Fold CV l√† m·ªôt k·ªπ thu·∫≠t ƒë·ªÉ so s√°nh c√°c m√¥ h√¨nh m√† kh√¥ng c·∫ßn chia validation set, ƒëi·ªÅu n√†y ti·∫øt ki·ªám ƒë∆∞·ª£c d·ªØ li·ªáu training. V·∫≠y K-Fold CV th∆∞·ªùng xuy√™n s·ª≠ d·ª•ng khi ch√∫ng ta kh√¥ng c√≥ 1 t·∫≠p d·ªØ li·ªáu ƒë·ªß l·ªõn ƒë·ªÉ hu·∫•n luy·ªán. ƒê√¢y c≈©ng ƒë∆∞·ª£c xem l√† m·ªôt c√°ch ƒë·ªÉ h·∫°n ch·∫ø overfitting.\nok, v·∫≠y ch√∫ng ta ƒë√£ t√¨m hi·ªÉu kh√° k·ªπ v·ªÅ l√Ω thuy·∫øt, b√¢y gi·ªù ch√∫ng ta s·∫Ω l√†m m·ªôt v√≠ d·ª• nh·ªè ƒë·ªÉ hi·ªÉu h∆°n v·ªÅ K-Fold CV nh√©, ·ªü ƒë√¢y m√¨nh s·∫Ω d√πng th∆∞ vi·ªán sklearn ƒë·ªÉ th·ª±c hi·ªán.\n Gi·∫£ s·ª≠ ch√∫ng ta ƒëang c√≥ t·∫≠p d·ªØ li·ªáu g·ªìm 6 samples: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\nƒê·∫ßu ti√™n ch√∫ng ta s·∫Ω l·ª±a ch·ªçn gi√° tr·ªã k ƒë·ªÉ x√°c ƒë·ªãnh s·ªë nh√≥m m√† ch√∫ng ta c·∫ßn chia ra. V√¨ ·ªü ƒë√¢y ch·ªâ c√≥ 6 samples n√™n ch√∫ng ta s·∫Ω ch·ªçn gi√° tr·ªã k = 3(3 nh√≥m) v√† m·ªói nh√≥m c√≥ 2 samples.\nv√≠ d·ª•:   Fold1: [0.5, 0.2]\nFold2: [0.1, 0.3]\nFold3: [0.4, 0.6]\n V√† ch√∫ng ta s·∫Ω train 3 l·∫ßn nh∆∞ sau:\n model1: Trained on Fold1 + Fold2, Tested on Fold3 model2: Trained on Fold2 + Fold3, Tested on Fold1 model3: Trained on Fold1 + Fold3, Tested on Fold2\nM·ªói model s·∫Ω b·ªã lo·∫°i b·ªè khi n√≥ ho√†n th√†nh l∆∞·ª£t train v√† ƒë√°nh gi√°.\nCode s·∫Ω nh∆∞ sau:  1 2 3 4 5 6 7 8 9 10  # scikit-learn k-fold cross-validation\r from numpy import array\rfrom sklearn.model_selection import KFold\r# data sample\r data = array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6])\r# prepare cross validation\r kfold = KFold(3, True, 1)\r# enumerate splits\r for train, test in kfold.split(data):\rprint(\u0026#39;train: %s, test: %s\u0026#39; % (data[train], data[test]))\r  V√† k·∫øt qu·∫£ l√†:\n train: [0.1 0.4 0.5 0.6], test: [0.2 0.3]\ntrain: [0.2 0.3 0.4 0.6], test: [0.1 0.5]\ntrain: [0.1 0.2 0.3 0.5], test: [0.4 0.6]\n T·ªïng k·∫øt\n Vi·ªác ph√¢n chia d·ªØ li·ªáu l√† m·ªôt trong nh·ªØng b∆∞·ªõc quan tr·ªçng nh·∫•t c·ªßa m·ªôt d·ª± √°n machine learning. ƒê·ªÉ l·ª±a ch·ªçn ph∆∞∆°ng ph√°p, c√°ch th·ª©c ph√π h·ª£p cho t·ª´ng b√†i to√°n th√¨ c·∫ßn c√≥ m·ªôt s·ªë kinh nghi·ªám nh·∫•t ƒë·ªãnh. B√†i bi·∫øt n√†y nh·∫±m gi·∫£i th√≠ch cho c√°c b·∫°n hi·ªÉu h∆°n v·ªÅ m·ªôt trong nh·ªØng c√°ch ƒë·ªÉ chia d·ªØ li·ªáu trong vi·ªác hu·∫•n luy·ªán m·ªôt m√¥ h√¨nh machine learning. ƒê·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ c√°ch l√†m th·∫ø n√†o ƒë·ªÉ ch·ªçn t·∫≠p validation set th√¨ c√°c b·∫°n n√™n ƒë·ªçc b√†i vi·∫øt n√†y. Hy v·ªçng qua b√†i vi·∫øt n√†y c√°c b·∫°n ƒë√£ hi·ªÉu r√µ h∆°n v·ªÅ K-Fold CV ƒë·ªÉ √°p d·ª•ng n√≥ m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t trong c√°c d·ª± √°n machine learning c·ªßa m√¨nh. V√† m√¨nh s·∫Ω li√™n t·ª•c c·∫≠p nh·∫≠t c√°c b√†i vi·∫øt trong series n√†y mong c√°c b·∫°n ƒë√≥n ƒë·ªçc.!!!  T√†i li·ªáu tham kh·∫£o:\n https://medium.com/datadriveninvestor/k-fold-cross-validation-6b8518070833 https://en.wikipedia.org/wiki/Cross-validation_%28statistics%29#k-fold_cross-validation  ","description":"T√¨m hi·ªÉu v·ªÅ k-fold Cross-Validation v√† c√°ch th·ª±c hi·ªán","id":1,"section":"posts","tags":["k-fold introduction"],"title":"T√¨m hi·ªÉu v·ªÅ K-Fold Cross-Validation","uri":"https://tranvanly107.github.io/posts/kfold-introduction/"},{"content":"·ªû b√†i tr∆∞·ªõc ch√∫ng ta ƒë√£ t√¨m hi·ªÉu m·ªôt kh√°i ni·ªám ƒë√≥ l√† Data clearning (l√†m s·∫°ch d·ªØ li·ªáu) , ƒë√¢y l√† m·ªôt giai ƒëo·∫°n trong c·ªßa Data preparation ( chu·∫©n b·ªã d·ªØ li·ªáu). V·∫≠y Data preparation l√† g√¨ th√¨ h√¥m nay ch√∫ng ta s·∫Ω t√¨m hi·ªÉu r√µ h∆°n v·ªÅ n√≥ nh√©.\n1. Kh√°i ni·ªám Chu·∫©n b·ªã d·ªØ li·ªáu l√† l√†m cho d·ªØ li·ªáu c√≥ gi√° tr·ªã, hi·ªáu ch·ªânh d·ªØ li·ªáu, c·∫•u tr√∫c v√† m√£ ho√° d·ªØ li·ªáu ph√π h·ª£p v·ªõi m√¥ h√¨nh machine learning, ƒë·∫ßu ra l√† m·ªôt b·ªô d·ªØ li·ªáu ho√†n ch·ªânh ƒë∆∞a v√†o m√¥ h√¨nh machine learning ƒë·ªÉ ƒë∆∞a ra m·ªôt k·∫øt qu·∫£ d·ª± ƒëo√°n t·ªët nh·∫•t.\nƒê·ªÉ chu·∫©n b·ªã m·ªôt b·ªô d·ªØ li·ªáu s·∫µn s√†ng cho thu·∫≠t to√°n machine learning th√¨ c·∫ßn c√≥ 3 b∆∞·ªõc ch√≠nh:\n B∆∞·ªõc 1: Select Data B∆∞·ªõc 2: Preprocess Data B∆∞·ªõc 3: Transform Data  B∆∞·ªõc 1: Select Data\n·ªû b∆∞·ªõc n√†y ch√∫ng ta ƒëi l·ª±a ch·ªçn m·ªôt b·ªô d·ªØ li·ªáu tr√™n r·∫•t nhi·ªÅu b·ªô d·ªØ li·ªáu m√† hi·ªán nay ƒë∆∞·ª£c public tr√™n internet ƒë·ªÉ l√†m vi·ªác v·ªõi v·∫•n ƒë·ªÅ m√† ch√∫ng ta c·∫ßn gi·∫£i quy·∫øt. Ch√∫ng ta c·∫ßn xem x√©t v·∫•n ƒë·ªÅ c·∫ßn gi·∫£i quy·∫øt, b√°i to√°n c·ªßa ch√∫ng ta l√† g√¨, ƒë·∫ßu ra d·ª± ƒëo√°n nh∆∞ th·∫ø n√†o\u0026hellip; ƒë·ªÉ t·ª´ ƒë√≥ ch√∫ng ta ƒë∆∞a ra l·ª±a ch·ªçn b·ªô d·ªØ li·ªáu ph√π h·ª£p cho y√™u c·∫ßu b√†i to√°n.\nB∆∞·ªõc 2: Preproces Data\nSau khi ƒë√£ l·ª±a ch·ªçn ƒë∆∞·ª£c b·ªô d·ªØ li·ªáu, ch√∫ng ta c·∫ßn xem x√©t v·∫•n ƒë·ªÅ l√† s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu ƒë√≥ nh∆∞ th·∫ø n√†o. V·∫≠y ·ªü b∆∞·ªõc n√†y l√† c√¥ng vi·ªác c·ªßa ch√∫ng ta ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi ƒë√≥, ƒë√≥ l√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (preprocess Data).\nTi·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu l√† c√°ch m√† ch√∫ng ta x·ª≠ l√Ω d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c ch·ªçn ƒë·ªÉ ph√π h·ª£p v·ªõi thu·∫≠t to√°n c·∫ßn d·ª± ƒëo√°n.\nBa b∆∞·ªõc ph·ªï bi·∫øn trong ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu ƒë√≥ l√† formatting, cleaning v√† sampling.\n Formatting: b·ªô d·ªØ li·ªáu m√† ch√∫ng ta ch·ªçn c√≥ th·ªÉ ch∆∞a ph√π h·ª£p v·ªõi format m√† thu·∫≠t to√°n c·∫ßn d·ª± ƒëo√°n. D·ªØ li·ªáu c√≥ th·ªÉ l√† dataframe, text file\u0026hellip; Ch√≠nh v√¨ v·∫≠y ch√∫ng ta c·∫ßn ph·∫£i chuy·ªÉn sang format ph√π h·ª£p v·ªõi thu·∫≠t to√°n. Cleaning: ƒë√¢y l√† b∆∞·ªõc m√† ch√∫ng ta th∆∞·ªùng t·∫≠p trung x·ª≠ l√Ω d·ªØ li·ªáu nhi·ªÖu, mising values\u0026hellip; Sampling: D·ªØ li·ªáu c√≥ th·ªÉ qu√° l·ªõn d·∫´n ƒë·∫øn th·ªùi gian training l√¢u, vi·ªác y√™u c·∫ßu v·ªÅ t√≠nh to√°n v√† b·ªô nh·ªõ c·∫ßn y√™u c·∫ßu cao h∆°n, ch√≠nh v√¨ v·∫≠y ch√∫ng ta c√≥ th·ªÉ ch·ªçn m·ªôt m·∫´u ƒë·∫°i di·ªán nh·ªè h∆°n d·ªØ li·ªáu ban ƒë·∫ßu ƒë·ªÉ c·∫£i thi·ªán t√¨nh tr·∫°ng tr√™n, n·∫øu kh√¥ng quan t√¢m nhi·ªÅu ƒë·∫øn ƒë·ªô ch√≠nh x√°c.  B∆∞·ªõc 3: Transform Data\nB∆∞·ªõc n√†y c≈©ng ƒë∆∞·ª£c g·ªçi l√† feature engineering, m·ªôt kh√°i ni·ªám kh√° quen thu·ªôc trong machine learning.\nChuy·ªÉn ƒë·ªïi t·∫≠p d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ·ªü b∆∞·ªõc 2 ƒë·ªÉ s·∫µn s√†ng cho thu·∫≠t to√°n machine learning s·ª≠ d·ª•ng scaling, attribute decomposition v√† attribute aggregation.\nNh∆∞ v·∫≠y ch√∫ng ta ƒë√£ gi·ªõi thi·ªáu s∆° qua v·ªÅ l√Ω thuy·∫øt. B∆∞·ªõc 1 v√† b∆∞·ªõc 2 m√¨nh ƒë√£ gi·ªõi thi·ªáu ·ªü b√†i tr∆∞·ªõc, c√°c b·∫°n c√≥ th·ªÉ t√¨m hi·ªÉu t·∫°i ƒë√¢y . H√¥m nay m√¨nh s·∫Ω gi·ªõi thi·ªáu chi ti·∫øt v·ªÅ b∆∞·ªõc 3 v√† c≈©ng l√† b∆∞·ªõc cu·ªëi c√πng trong Data preparation.\nƒê·ªÉ transform data( bi·∫øn ƒë·ªïi d·ªØ li·ªáu) m√¨nh s·∫Ω gi·ªõi thi·ªáu 4 ph∆∞∆°ng ph√°p th∆∞·ªùng d√πng Rescale Data, Standardize Data, Normalize Data, Binarize Data.\nT·∫°i sao ch√∫ng ta ph·∫£i bi·∫øn ƒë·ªïi d·ªØ li·ªáu: trong machine learning c√≥ kh√°i ni·ªám m√† h·∫ßu nh∆∞ ai c≈©ng ƒë√£ nghe ƒë√≥ l√† cost function, n√≥ c√≥ h√¨nh d·∫°ng nh∆∞ m·ªôt c√°i b√°t. N·∫øu ch√∫ng ta kh√¥ng bi·∫øn ƒë·ªïi d·ªØ li·ªáu tr∆∞·ªõc khi train th√¨ h√¨nh d·∫°ng c·ªßa n√≥ gi·ªëng nh∆∞ m·ªôt c√°i b√°t thon d√†i -\u0026gt; th·ªùi gian training l√¢u, h·ªôi t·ª• ch·∫≠m v√† ƒë·∫∑c bi·ªát khi s·ª≠ d·ª•ng Gradient Descent th√¨ vi·ªác chu·∫©n h√≥a d·ªØ li·ªáu ƒë·∫∑c bi·ªát r·∫•t c·∫ßn thi·∫øt.\n2. Th·ª±c hi·ªán B√¢y gi·ªù ch√∫ng ta s·∫Ω ƒëi t·ª´ng ph∆∞∆°ng ph√°p m·ªôt nh√©. Tr∆∞·ªõc h·∫øt ch√∫ng ta s·∫Ω load d·ªØ li·ªáu housing m√† ·ªü b√†i h√¥m tr∆∞·ªõc ch√∫ng ta ƒëang s·ª≠ d·ª•ng. Trong b√†i n√†y m√¨nh s·∫Ω ti·∫øp t·ª•c s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu n√†y ƒë·ªÉ th·ª±c hi·ªán.\n1 2 3 4 5 6 7 8  import os\rimport tarfile\rfrom six.moves import urllib\rimport pandas as pd\rurl = \u0026#34;C:\\\\Users\\\\LYTRAN\\\\Desktop\\\\Tensorflow\\\\AID\\\\housing.csv\u0026#34;\r#names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;]\r housing = pd.read_csv(url)\rhousing.head()\r      longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    2.1. Rescale Data M·ªçi thu·∫≠t to√°n machhine learning kh√¥ng th·ª±c hi·ªán t·ªët khi m√† t·∫≠p d·ªØ li·ªáu c·ªßa ch√∫ng ta( c√°c gi√° tr·ªã trong c√°c attributes) c√≥ nh·ªØng ph·∫°m vi kh√°c nhau.\nV√≠ d·ª• ·ªü b·∫£ng d·ªØ li·ªáu tr√™n ch√∫ng ta th·∫•y: \u0026ldquo;total_bedrooms\u0026rdquo; attribute c√≥ gi√° tr·ªã thu·ªôc ph·∫°m vi t·ª´ 6 - 39320 trong khi ƒë√≥ \u0026ldquo;median_income\u0026rdquo; attribute c√≥ gi√° tr·ªã thu·ªôc ph·∫°m vi t·ª´ 0 - 15, t∆∞∆°ng t·ª± nh∆∞ nh·ªØng attributes kh√°c c≈©ng v·∫≠y. M·∫∑c d√π b√†i vi·∫øt h√¥m tr∆∞·ªõc m√¨nh ƒë√£ cleaning data c√≥ nghƒ©a l√† ch·ªâ x·ª≠ l√Ω nh·ªØng missing values, n·∫øu ch√∫ng ta ƒë∆∞a b·ªô d·ªØ li·ªáu n√†y v√†o training th√¨ th·ªùi gian training s·∫Ω l√¢u h∆°n, lost function h·ªôi t·ª• ch·∫≠m h∆°n. V·∫≠y ch√∫ng ta s·∫Ω scaling ch√∫ng v·ªÅ c√πng m·ªôt ph·∫°m vi t·ª´ [0-1] ho·∫∑c [-1 - 1]. Trong sklearn c√≥ h√†m MinMaxScaler l√†m cho ch√∫ng ta vi·ªác n√†y, ch√∫ng ta ch·ªâ vi·ªác √°p d·ª•ng v√† xem k·∫øt qu·∫£ th√¥i.\n1 2 3 4 5 6 7 8 9  import pandas\rimport scipy\rimport numpy\rfrom sklearn.preprocessing import MinMaxScaler\rscaler = MinMaxScaler(feature_range=(0, 1))\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rrescaled[0:5, :]\r   [[0.211 0.567 0.784 0.022 0.02 0.009 0.021 0.54 0.75 ]\n[0.212 0.565 0.392 0.181 0.171 0.067 0.187 0.538 0.75 ]\n[0.21 0.564 1. 0.037 0.029 0.014 0.029 0.466 0.75 ]\n[0.209 0.564 1. 0.032 0.036 0.016 0.036 0.355 0.75 ]\n[0.209 0.564 1. 0.041 0.043 0.016 0.042 0.231 0.75 ]]\n Nh∆∞ ch√∫ng ta th·∫•y to√†n b·ªô gi√° tr·ªã b√¢y gi·ªù n·∫±m trong kho·∫£ng t·ª´ 0-1.\n2.2. Standardize Data Trong lƒ©nh v·ª±c h·ªçc m√°y, ch√∫ng ta c√≥ th·ªÉ s·∫Ω ph·∫£i x·ª≠ l√Ω m·ªôt l∆∞·ª£ng l·ªõn c√°c ki·ªÉu d·ªØ li·ªáu kh√°c nhau, v√≠ d·ª• nh∆∞ d·ªØ li·ªáu d·∫°ng t√≠n hi·ªáu √¢m thanh, c√°c ƒëi·ªÉm ·∫£nh trong m·ªôt b·ª©c ·∫£nh,‚Ä¶ v√† nh·ªØng d·ªØ li·ªáu n√†y c√≥ th·ªÉ l√† c√°c d·ªØ li·ªáu nhi·ªÅu chi·ªÅu. Vi·ªác ch√≠nh quy h√≥a d·ªØ li·ªáu gi√∫p cho gi√° tr·ªã c·ªßa m·ªói ƒë·∫∑c tr∆∞ng c√≥ trung b√¨nh b·∫±ng 0 v√† ph∆∞∆°ng sai b·∫±ng 1. Ph∆∞∆°ng ph√°p n√†y ƒë∆∞·ª£c s·ª≠ d·ª•ng r·ªông r√£i trong vi·ªác chu·∫©n h√≥a d·ªØ li·ªáu c·ªßa nhi·ªÅu thu·∫≠t to√°n h·ªçc m√°y (SVM, logistic regression v√† ANNs).\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import StandardScaler\rimport pandas\rimport numpy\rscaler = StandardScaler().fit(housing)\rrescaled = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(rescaled[0:5,:])\r   [[-1.328 1.053 0.982 -0.805 -0.97 -0.974 -0.977 2.345 1.291]\n[-1.323 1.043 -0.607 2.046 1.348 0.861 1.67 2.332 1.291]\n[-1.333 1.039 1.856 -0.536 -0.826 -0.821 -0.844 1.783 1.291]\n[-1.338 1.039 1.856 -0.624 -0.719 -0.766 -0.734 0.933 1.291]\n[-1.338 1.039 1.856 -0.462 -0.612 -0.76 -0.629 -0.013 1.291]]\n 2.3. Normalize Data M·ªôt l·ª±a ch·ªçn kh√°c ƒë·ªÉ co gi√£n c√°c th√†nh ph·∫ßn c·ªßa c√°c v√©c-t∆° ƒë·∫∑c tr∆∞ng l√† bi·∫øn ƒë·ªïi sao cho v√©c-t∆° ƒë·∫∑c tr∆∞ng sau khi bi·∫øn ƒë·ªïi c√≥ ƒë·ªô d√†i b·∫±ng 1.\nQu√° tr√¨nh Normalization r·∫•t h·ªØu √≠ch cho c√°c b·ªô d·ªØ li·ªáu th∆∞a th·ªõt (c√≥ nhi·ªÅu gi√° tr·ªã 0) v·ªõi c√°c thu·ªôc t√≠nh c√≥ t·ª∑ l·ªá kh√°c nhau khi s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n c√≥ tr·ªçng s·ªë ƒë·∫ßu v√†o nh∆∞ m·∫°ng th·∫ßn kinh v√† thu·∫≠t to√°n s·ª≠ d·ª•ng c√°c th∆∞·ªõc ƒëo kho·∫£ng c√°ch nh∆∞ K-Nearest Neighbors.\n1 2 3 4 5 6 7 8  from sklearn.preprocessing import Normalizer\rimport pandas\rimport numpy\rscaler = Normalizer().fit(housing)\rnormalized = scaler.fit_transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(normalized[0:5,:])\r   [[-1.268e-01 3.931e-02 4.254e-02 9.131e-01 1.339e-01 3.341e-01\n1.307e-01 8.639e-03 3.113e-03]\n[-1.595e-02 4.942e-03 2.741e-03 9.266e-01 1.444e-01 3.134e-01\n1.485e-01 1.084e-03 3.916e-04]\n[-7.755e-02 2.401e-02 3.299e-02 9.307e-01 1.205e-01 3.147e-01\n1.123e-01 4.604e-03 1.903e-03]\n[-8.524e-02 2.639e-02 3.626e-02 8.883e-01 1.639e-01 3.891e-01\n1.527e-01 3.935e-03 2.092e-03]\n[-6.909e-02 2.139e-02 2.939e-02 9.195e-01 1.582e-01 3.193e-01\n1.464e-01 2.174e-03 1.695e-03]]\n ·ªû ph∆∞∆°ng ph√°p n√†y, n·∫øu d·ªØ li·ªáu c·ªßa ch√∫ng ta m√† c√≤n t·ªìn t·∫°i gi√° tr·ªã NaN th√¨ n√≥ s·∫Ω b√°o l·ªói.\n2.4. Binarize Data M·ª•c ti√™u c·ªßa m√£ h√≥a nh·ªã ph√¢n l√† s·ª≠ d·ª•ng m√£ nh·ªã ph√¢n ƒë·ªÉ bƒÉm c√°c gi√° tr·ªã c·ªßa ƒë·∫∑c tr∆∞ng d·∫°ng nh√≥m th√†nh c√°c gi√° tr·ªã nh·ªã ph√¢n.\nCh√∫ng ta c√≥ th·ªÉ bi·∫øn ƒë·ªïi d·ªØ li·ªáu s·ª≠ d·ª•ng ng∆∞·ª°ng nh·ªã ph√¢n. To√†n b·ªô gi√° tr·ªã l·ªõn h∆°n ng∆∞·ª°ng n√†y th√¨ g·∫Øn cho n√≥ l√† 1 v√† ng∆∞·ª£c l·∫°i th√¨ g·∫Øn cho n√≥ l√† 0. Ph∆∞∆°ng ph√°p n√†y r·∫•t h·ªØu √≠ch trong feature engineering v√† khi b·∫°n mu·ªën th√™m m·ªôt feature m·ªõi n√†o m√† b·∫°n th·∫•y c√≥ √Ω nghƒ©a cho d·ªØ li·ªáu.\n1 2 3 4 5 6 7 8 9  # binarization\r from sklearn.preprocessing import Binarizer\rimport pandas\rimport numpy\rbinarizer = Binarizer(threshold=0.0).fit(housing)\rbinary = binarizer.transform(housing)\r# summarize transformed data\r numpy.set_printoptions(precision=3)\rprint(binary[0:5,:])\r   [[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]\n[0. 1. 1. 1. 1. 1. 1. 1. 1.]]\n Nh∆∞ v·∫≠y ch√∫ng ta ƒë√£ ƒëi qua 4 ph∆∞∆°ng ph√°p th∆∞·ªùng s·ª≠ d·ª•ng nh·∫•t trong bi·∫øn ƒë·ªïi Data, m·ªôt trong nh·ªØng b∆∞·ªõc c·ª±c k·ª≥ quan tr·ªçng c·ªßa Data preparation, ƒë√¢y l√† c√¥ng vi·ªác m√† ƒë√≤i h·ªèi ch√∫ng ta c√≥ nhi·ªÅu kinh nghi·ªám ƒë·ªÉ ch·ªçn ra m·ªôt ph∆∞∆°ng ph√°p t·ªët nh·∫•t cho b√†i to√°n c·ª• th·ªÉ.\n3. T·ªïng k·∫øt Khi ƒë·ªëi m·∫∑t v·ªõi m·ªôt v·∫•n ƒë·ªÅ m√† b·∫°n mu·ªën gi·∫£i quy·∫øt n√≥ b·∫±ng H·ªçc M√°y, chu·∫©n b·ªã d·ªØ li·ªáu c·ª±c k·ª≥ quan tr·ªçng v√† c·∫ßn c√≥ r·∫•t nhi·ªÅu th·ªùi gian v√† kinh nghi·ªám. N·∫øu l√†m t·ªët vi·ªác chu·∫©n b·ªã d·ªØ li·ªáu s·∫Ω gi√∫p b·∫°n tr·ªü th√†nh b·∫≠c th·∫ßy v·ªÅ h·ªçc m√°y.\nƒê·ªÉ √°p d·ª•ng cho vi·ªác training th√¨ tr∆∞·ªõc h·∫øt b·∫°n c·∫ßn ƒë·∫∑t ra v√† tr·∫£ l·ªùi nhi·ªÅu c√¢u h·ªèi ( v·∫•n ƒë·ªÅ c·ªßa b·∫°n l√† g√¨, ƒë·∫ßu ra nh∆∞ th·∫ø n√†o\u0026hellip;) ƒë·ªÉ t·ª´ ƒë√≥ l·ª±a ch·ªçn m·ªôt b·ªô data v·ªõi format ph√π h·ª£p v√† th·ª±c hi·ªán vi·ªác chu·∫©n b·ªã d·ªØ li·ªáu nh∆∞ th·∫ø n√†o ƒë·ªÉ √°p d·ª•ng n√≥ m·ªôt c√°ch hi·ªáu qu·∫£ v√† ph√π h·ª£p v·ªõi y√™u c·∫ßu th·ª±c t·∫ø.\nQua b√†i vi·∫øt n√†y hy v·ªçng c√°c b·∫°n ƒë√£ hi·ªÉu m·ªôt c√°ch t·ªïng qu√°t vi·ªác chu·∫©n b·ªã m·ªôt b·ªô d·ªØ li·ªáu ƒë·ªÉ s·∫µn s√†ng cho vi·ªác training nh∆∞ th·∫ø n√†o v√† √°p d·ª•ng n√≥ v√†o c√°c d·ª± √°n th·ª±c t·∫ø m·ªôt c√°ch hi·ªáu qu·∫£.\n4. T√†i li·ªáu tham kh·∫£o   https://scholar.google.com.vn/scholar?q=data+prepare+machine+learning\u0026amp;hl=vi\u0026amp;as_sdt=0\u0026amp;as_vis=1\u0026amp;oi=scholart\n  https://medium.com/vickdata/four-feature-types-and-how-to-transform-them-for-machine-learning-8693e1c24e80\n  ","description":"X√¢y d·ª±ng ho√†n ch·ªânh b·ªô d·ªØ li·ªáu cho machine learning","id":2,"section":"posts","tags":["data preparation"],"title":"Chu·∫©n b·ªã d·ªØ li·ªáu trong Machine Learning ","uri":"https://tranvanly107.github.io/posts/data-preparation/"},{"content":"ƒê·ªÉ ti·∫øp t·ª•c cho chu·ªói c√°c b√†i vi·∫øt v·ªÅ machine learning th√¨ h√¥m nay ch√∫ng ta s·∫Ω ti·∫øp t·ª•c t√¨m hi·ªÉu m·ªôt kh√°i ni·ªám r·∫•t quan tr·ªçng trong b√†i to√°n machine learning ƒë√≥ l√† l√†m s·∫°ch d·ªØ li·ªáu(data cleaning).\nƒê·ªÉ hi·ªÉu ƒë∆∞·ª£c t·ªïng quan v·ªÅ data cleaning th√¨ ƒë·∫ßu ti√™n ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ kh√°i ni·ªám v√† nh·ªØng l·ª£i √≠ch c·ªßa vi·ªác l√†m s·∫°ch d·ªØ li·ªáu nh√©.\n1. Data cleaning Data cleaning thu·ªôc m·ªôt trong c√°c giai ƒëo·∫°n c·ªßa Data preparation ( b√†i ti·∫øp theo ch√∫ng ta s·∫Ω t√¨m hi·ªÉu v·ªÅ kh√°i ni·ªám n√†y) c√≤n g·ªçi l√† l√†m s·∫°ch d·ªØ li·ªáu, ƒë√¢y l√† b∆∞·ªõc ƒë·∫ßu ti√™n v√† c≈©ng l√† b∆∞·ªõc quan tr·ªçng nh·∫•t m√† m·ªói c√° nh√¢n ƒë·ªÅu ph·∫£i th·ª±c hi·ªán sau khi thu th·∫≠p ƒë∆∞·ª£c d·ªØ li·ªáu ƒë·ªÉ c√≥ m·ªôt k·∫øt qu·∫£ d·ª± ƒëo√°n ch√≠nh x√°c. M·ª•c ƒë√≠ch c·ªßa b∆∞·ªõc n√†y l√† lo·∫°i b·ªè c√°c d·ªØ li·ªáu \u0026ldquo;nhi·ªÖu\u0026rdquo;, d·ªØ li·ªáu kh√¥ng c·∫ßn thi·∫øt, kh√¥ng ƒë·∫ßy ƒë·ªß th√¥ng tin - ƒë√¢y ƒë∆∞·ª£c xem l√† nh·ªØng v·∫•n ƒë·ªÅ lu√¥n hi·ªán h·ªØu trong m·ªçi b·ªô d·ªØ li·ªáu.\nC√¥ng vi·ªác c·ª• th·ªÉ trong Data cleaning l√† x·ª≠ l√Ω c√°c \u0026ldquo;missing value\u0026rdquo;, c√°c nhi·ªÖu v√† d·ªØ li·ªáu kh√¥ng nh·∫•t qu√°n, kh√¥ng c·∫ßn thi·∫øt s·∫Ω ƒë∆∞·ª£c lo·∫°i b·ªè.\nK·∫øt qu·∫£ c·ªßa b∆∞·ªõc Data cleaning l√† m·ªôt b·ªô d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, kh√¥ng c√≤n t√°c nh√¢n g√¢y ·∫£nh h∆∞·ªüng ƒë·∫øn c√°c b∆∞·ªõc sau trong vi·ªác chu·∫©n b·ªã d·ªØ li·ªáu ( Data preparation).\n2. Th·ª±c hi·ªán 2.1. T·∫£i d·ªØ li·ªáu H√¥m nay m√¨nh s·∫Ω s·ª≠ d·ª•ng m·ªôt b·ªô d·ªØ li·ªáu th·∫≠t l√† d·ª± ƒëo√°n gi√° nh√† t·∫°i ti·ªÉu bang California Hoa K·ª≥ c√°c b·∫°n c√≥ th·ªÉ t·∫£i d·ªØ li·ªáu t·∫°i ƒë√¢y\nCh√∫ng ta v·∫´n s·ª≠ d·ª•ng pandas ƒë·ªÉ load d·ªØ li·ªáu:\n1 2 3 4 5 6 7 8  import os import tarfile from six.moves import urllib import pandas as pd url = \u0026#34;https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\u0026#34; names = [\u0026#39;longitude\u0026#39;, \u0026#39;latitude\u0026#39;, \u0026#39;housing_median_age\u0026#39;, \u0026#39;total_rooms\u0026#39;, \u0026#39;total_bedrooms\u0026#39;, \u0026#39;population\u0026#39;, \u0026#39;households\u0026#39;, \u0026#39;median_income\u0026#39;, \u0026#39;median_house_value\u0026#39;, \u0026#39;ocean_proximity\u0026#39;] housing = pd.read_csv(url)   ·ªû b√†i tr∆∞·ªõc m√¨nh ƒë√£ gi·ªõi thi·ªáu m·ªôt s·ªë ph∆∞∆°ng ph√°p ƒë·ªÉ hi·ªÉu ƒë∆∞·ª£c d·ªØ li·ªáu, n·∫øu c√°c b·∫°n ch∆∞a ƒë·ªçc th√¨ tham kh·∫£o t·∫°i ƒë√¢y\nNh∆∞ng ƒë·ªÉ ph·ª•c v·ª• cho c√°c b∆∞·ªõc ti·∫øp theo th√¨ ch√∫ng ta c·∫ßn ph·∫£i nh√¨n qua v·ªÅ d·ªØ li·ªáu m·ªôt ch√∫t.\n1  housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 452600.0 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 358500.0 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 352100.0 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 341300.0 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 342200.0 NEAR BAY    1  housing.shape    (20640, 10)\n D·ªØ li·ªáu ch√∫ng ta bao g·ªìm 10 c·ªôt (longitude, latitude, housing_median_age, total_rooms, total_bed rooms, population, households, median_income, median_house_value, and ocean_proximity) v√† 20640 h√†ng ( l∆∞u √Ω: m·ªói h√†ng ·ªü ƒë√¢y c√≥ nghƒ©a l√† m·ªói instance, v√† m·ªói c·ªôt l√† m·ªôt attribute) .\n1  housing.info()   H√†m info r·∫•t h·ª©u √≠ch trong vi·ªác m√¥ t·∫£ d·ªØ li·ªáu, n√≥ gi√∫p ch√∫ng ta bi·∫øt ƒë∆∞·ª£c l√† s·ªë h√†ng, ki·ªÉu d·ªØ li·ªáu c·ªßa m·ªói attribute v√† s·ªë l∆∞·ª£ng non-null values. B·∫°n c√≥ th·ªÉ th·∫•y k·∫øt qu·∫£ ·ªü ph√≠a tr√™n. Ch√∫ √Ω r·∫±ng \u0026ldquo;total_bedrooms\u0026rdquo; attribute ch·ªâ c√≥ 20433 non-null values, c√≥ nghƒ©a l√† c√≥ 207 h√†ng kh√¥ng c√≥ gi√° tr·ªã v√† ƒë√¢y g·ªçi l√† missing value.\nM·ªôt ch√∫ √Ω kh√°c l√† h·∫ßu h·∫øt c√°c gi√° tr·ªã c·ªßa c√°c attributes ƒë·ªÅu l√† d·∫°ng s·ªë, ch·ªâ c√≥ \u0026ldquo;ocean_proximity\u0026rdquo; l√† d·∫°ng object, n√≥ l√† m·ªôt ki·ªÉu c·ªßa python object nh∆∞ng khi ch√∫ng ta loaded d·ªØ li·ªáu ·ªü d·∫°ng CSV th√¨ n√≥ s·∫Ω ph·∫£i l√† ·ªü d·∫°ng text. D·ªØ li·ªáu m√† ch√∫ng ta ƒëang th·∫•y ch·ªâ l√† 5 h√†ng ƒë·∫ßu ti√™n, c√≥ th·ªÉ b·∫°n th·∫•y n√≥ ch·ªâ to√†n l√† NEAR BAY nh∆∞ng th·ª±c t·∫ø c√≥ nhi·ªÅu categorical kh√°c nhau:\n1  housing[\u0026#34;ocean_proximity\u0026#34;].value_counts()    \u0026lt;1H OCEAN: 9136\nINLAND: 6551\nNEAR OCEAN: 2658\nNEAR BAY : 2290\nISLAND : 5\nName: ocean_proximity, dtype: int64\n Nh∆∞ v·∫≠y ch√∫ng ta th·∫•y c√≥ 9136 h√†ng l√† OCEAN, 6551 l√† INLAND\u0026hellip; Ch√∫ng ta b·∫Øt ƒë·∫ßu v√†o ph·∫ßn ch√≠nh c·ªßa b√†i vi·∫øt.\nNh√¨n v√†o d·ªØ li·ªáu ch√∫ng ta bi·∫øt s·∫Ω c√≥ m·ªôt c·ªôt l√† label trong t·∫≠p d·ªØ li·ªáu ƒë∆∞·ª£c g·ªçi l√† nh√£n, c·ª• th·ªÉ ·ªü ƒë√¢y l√† \u0026ldquo;median_house_value \u0026quot; hay c√≤n goi l√† gi√° nh√†. Ch√≠nh v√¨ v·∫≠y m√¨nh s·∫Ω t√°ch c·ªôt n√†y ri√™ng ra ƒë·ªÉ d·ªÖ d√†ng trong vi·ªác x·ª≠ l√Ω, c√≤n b√†i sau ch√∫ng ta s·∫Ω x·ª≠ l√Ω n√≥ trong b√†i vi·∫øt v·ªÅ chu·∫©n b·ªã d·ªØ li·ªáu.\n1 2  housing = housing.drop(\u0026#34;median_house_value\u0026#34;, axis=1) # drop labels for training set housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 NEAR BAY   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 NEAR BAY   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 NEAR BAY   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 NEAR BAY   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 NEAR BAY    Ch√∫ng ta ƒë√£ lo·∫°i b·ªè c·ªôt gi√° nh√† ƒëi, b√¢y gi·ªù ch·ªâ c√≤n 9 c·ªôt.\nCh√∫ng ta l∆∞u √Ω m·ªôt ƒëi·ªÅu r·∫±ng h·∫ßu h·∫øt c√°c thu·∫≠t to√°n Machine Learning kh√¥ng th·ªÉ l√†m vi·ªác ƒë∆∞·ª£c v·ªõi nh·ªØng features m√† thi·∫øu d·ªØ li·ªáu ( missing features) ch√≠nh v√¨ v·∫≠y Data cleaning l√† b∆∞·ªõc r·∫•t quan tr·ªçng m√† ch√∫ng ta s·∫Ω kh√¥ng th·ªÉ b·ªè qua n·∫øu ch√∫ng ta kh√¥ng mu·ªën c√≥ m·ªôt k·∫øt qu·∫£ d·ª± ƒëo√°n t·ªìi.\nNh∆∞ m√¨nh ƒë√£ ƒë·ªÅ c·∫≠p tr∆∞·ªõc ƒë√≥ l√† \u0026ldquo;total_bedrooms\u0026rdquo; attribute c√≥ m·ªôt s·ªë d·ªØ li·ªáu b·ªã thi·∫øu. ƒê·ªÉ ƒë∆∞·ª£c r√µ h∆°n ch√∫ng ta s·∫Ω show k·∫øt qu·∫£ sau ƒë√¢y:\n1 2  sample_incomplete_rows = housing[housing.isnull().any(axis=1)] sample_incomplete_rows.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 NaN 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 NaN 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 NaN 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 NaN 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 NaN 387.0 161.0 3.9063 NEAR BAY    Nh∆∞ ch√∫ng ta th·∫•y ·ªü tr√™n, c·ªôt n√†o c√≥ NaN c√≥ nghƒ©a l√† ƒëang kh√¥ng c√≥ gi√° tr·ªã n√†o, m·ªôt l√† ƒëang ƒë·ªÉ tr·ªëng hai l√† m·∫∑c ƒë·ªãnh l√† NaN v√† c√¥ng vi·ªác c·ªßa ch√∫ng ta l√† s·∫Ω ƒëi gi·∫£i quy·∫øt ch√∫ng.\nƒê·ªÉ x·ª≠ l√Ω c√°c missing value th√¨ ch√∫ng ta c√≥ 3 l·ª±a ch·ªçn:\n L·ª±a ch·ªçn 1: lo·∫°i b·ªè nh·ªØng h√†ng n√†o m√† gi√° tr·ªã ƒë√≥ l√† NaN L·ª±a ch·ªçn 2: lo·∫°i b·ªè to√†n b·ªô c·ªôt n√†o m√† c√≥ b·∫•t k·ª≥ m·ªôt gi√° tr·ªã NaN n√†o L·ª±a ch·ªçn 3: thay m·ªôt gi√° tr·ªã b·∫•t k·ª≥ v√†o gi√° tr·ªã NaN( v√≠ d·ª•: gi√° tr·ªã 0, gi√° tr·ªã mean, gi√° tr·ªã median\u0026hellip;).  1 2 3 4  housing.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1  housing.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1) # option 2  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() # option 3  housing[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True)   N·∫øu ch√∫ng ta ch·ªçn l·ª±a ch·ªçn 3 th√¨ c·∫ßn ph·∫£i t√≠nh gi√° tr·ªã median tr√™n t·∫≠p training set v√† l·∫•y gi√° tr·ªã ƒë√≥ ƒë·ªÉ thay v√†o c√°c gi√° tr·ªã NaN trong training set, nh∆∞ng b·∫°n ƒë·ª´ng qu√™n l∆∞u ch√∫ng l·∫°i ƒë·ªÉ s·ª≠ d·ª•ng cho t·∫≠p test khi b·∫°n mu·ªën ƒë√°nh gi√° model s·ª≠ d·ª•ng t·∫≠p test set. Ok, ch√∫ng ta b·∫Øt ƒë·∫ßu xem k·∫øt qu·∫£ c·ªßa t·ª´ng l·ª±a ch·ªçn nh√©:\nTrong l·ª±a ch·ªçn m·ªôt ch√∫ng ta s·ª≠ d·ª•ng h√†m dropna t·ª©c l√† x√≥a nh·ªØng h√†ng n√†o m√† c√≥ √≠t nh·∫•t m·ªôt gi√° tr·ªã NaN.\n1  sample_incomplete_rows.dropna(subset=[\u0026#34;total_bedrooms\u0026#34;]) # option 1       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity    Nh∆∞ v·∫≠y l√† ch√∫ng ta ƒë√£ x√≥a t·∫•t c·∫£ c√°c h√†ng c√≥ gi√° tr·ªã NaN. C√°c b·∫°n l∆∞u √Ω r·∫±ng l√† ƒë√¢y l√† ch√∫ng ta ƒëang x·ª≠ l√Ω tr√™n nh·ªØng h√†ng c√≥ gi√° tr·ªã NaN, ch·ª© nhi·ªÅu b·∫°n nh√¨n v√†o b·∫£ng tr√™n c√≥ th·ªÉ hi·ªÉu nh·∫ßm l√† to√†n b·ªô data ƒë√£ b·ªã x√≥a s·∫°ch nh∆∞ng kh√¥ng ph·∫£i, m√¨nh ƒëang x√©t nh·ªØng h√†ng c√≥ gi√° tr·ªã NaN v√† d·ªØ li·ªáu c·ªßa ch√∫ng ta ch·ªâ m·∫•t to√†n b·ªô h√†ng ch·ª©a gi√° tr·ªã NaN m√† th√¥i(c·ª• th·ªÉ ·ªü ƒë√¢y c√≥ 207 h√†ng) c√≤n c√°c h√†ng c√≤n l·∫°i v·∫´n gi·ªØ nguy√™n v√† t∆∞∆°ng t·ª± nh∆∞ c√°c l·ª±a ch·ªçn kh√°c.\n1  sample_incomplete_rows.drop(\u0026#34;total_bedrooms\u0026#34;, axis=1).head() # option 2   Ch√∫ng ta s·ª≠ d·ª•ng h√†m drop ƒë·ªÉ x√≥a c·ªôt b·∫•t k·ª≥ ch√∫ng ta mu·ªën, ·ªü ƒë√¢y ch√∫ng ta x√≥a c·ªôt c√≥ ch·ª©a NaN \u0026ldquo;total_bedrooms\u0026rdquo;.\n    longitude latitude housing_median_age total_rooms population households median_income ocean_proximity      290 -122.16 37.77 47.0 1256.0 570.0 218.0 4.3750 NEAR BAY    341 -122.17 37.75 38.0 992.0 732.0 259.0 1.6196 NEAR BAY    538 -122.28 37.78 29.0 5154.0 3741.0 1273.0 2.5762 NEAR BAY    563 -122.24 37.75 45.0 891.0 384.0 146.0 4.9489 NEAR BAY    696 -122.10 37.69 41.0 746.0 387.0 161.0 3.9063 NEAR BAY     Ok, nh∆∞ v·∫≠y c·ªôt \u0026ldquo;total_bedrooms\u0026rdquo; c·ªßa ch√∫ng ta ƒë√£ b·ªã x√≥a.\nL·ª±a ch·ªçn 3 th√¨ nh∆∞ m√¨nh ƒë√£ n√≥i ·ªü tr√™n v√† b√¢y gi·ªù ch√∫ng ta s·∫Ω xem k·∫øt qu·∫£ n√≥ nh∆∞ th·∫ø n√†o nh√©.\n1 2 3  median = housing[\u0026#34;total_bedrooms\u0026#34;].median() sample_incomplete_rows[\u0026#34;total_bedrooms\u0026#34;].fillna(median, inplace=True) # option 3 sample_incomplete_rows.head()   H√†m fillna l√† h√†m m√† l·∫•y gi√° tr·ªã b·∫•t k·ª≥ m√† ch√∫ng ta mu·ªën ƒë·ªÉ thay th·∫ø cho to√†n b·ªô gi√° tr·ªã NaN c·ªßa c·ªôt ƒë√≥, ·ªü ƒë√¢y l√† median ( ch√∫ng c√≥ th·ªÉ l√† gi√° tr·ªã 0, ho·∫∑c mean)\n    longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     290 -122.16 37.77 47.0 1256.0 435.0 570.0 218.0 4.3750 NEAR BAY   341 -122.17 37.75 38.0 992.0 435.0 732.0 259.0 1.6196 NEAR BAY   538 -122.28 37.78 29.0 5154.0 435.0 3741.0 1273.0 2.5762 NEAR BAY   563 -122.24 37.75 45.0 891.0 435.0 384.0 146.0 4.9489 NEAR BAY   696 -122.10 37.69 41.0 746.0 435.0 387.0 161.0 3.9063 NEAR BAY    Ch√∫ng ta th·∫•y gi√° tr·ªã median l√† 435.0 v√† ƒë√£ ƒë∆∞·ª£c thay th·∫ø cho gi√° tr·ªã NaN r·ªìi nh√©.\nVi·ªác l·ª±a ch·ªçn 3 ph∆∞∆°ng √°n tr√™n l√† t√πy v√†o b√†i to√°n, v√† l∆∞·ª£ng gi√° tr·ªã NaN. Nh∆∞ng h·∫ßu h·∫øt ng∆∞·ªùi ta khuy·∫øn kh√≠ch s·ª≠ d·ª•ng l·ª±a ch·ªçn 3 n·∫øu trong tr∆∞·ªùng h·ª£p l∆∞·ª£ng gi√° tr·ªã NaN qu√° nh·ªè so v·ªõi to√†n b·ªô d·ªØ li·ªáu.\nOK, v·∫≠y ch√∫ng ta ƒë√£ xong vi·ªác x·ª≠ l√Ω \u0026ldquo;missing value\u0026rdquo;, th·∫ø l√† t·∫°m ·ªïn üòÑ. M·ªõi ch·ªâ t·∫°m ·ªïn th√¥i nh√© ch·ª© ch∆∞a ·ªïn ƒë√¢u, c√≤n ti·∫øp t·ª•c.\nNh∆∞ ch√∫ng ta th·∫•y h·∫ßu h·∫øt to√†n b·ªô c√°c attributes ƒë·ªÅu l√† gi√° tr·ªã s·ªë, nh∆∞ng l·∫°i c√≥ m·ªôt attribute l√† text, t·ªõi ƒë√¢y c√≥ nhi·ªÅu b·∫°n th·∫Øc m·∫Øc r·∫±ng n·∫øu l√† text th√¨ sao t√≠nh ƒë∆∞·ª£c median, th·∫≠t may l√† ·ªü tr√™n m√¨nh ch·ªâ s·ª≠ d·ª•ng c·ªôt total_bedrooms. M√¨nh tr·∫£ l·ªùi cho c√°c b·∫°n l√† text th√¨ kh√¥ng th·ªÉ t√≠nh ƒë∆∞·ª£c gi√° tr·ªã median nh√©, c√≤n n·∫øu c√°c b·∫°n mu·ªën t√≠nh gi√° tr·ªã median tr√™n to√†n b·ªô attributes th√¨ c√°c b·∫°n s·∫Ω ph·∫£i lo·∫°i b·ªè attribute n√†o m√† gi√° tr·ªã c·ªßa n√≥ thu·ªôc d·∫°ng text. Nh∆∞ng b√¢y gi·ªù ch√∫ng ta s·∫Ω x·ª≠ l√Ω n√≥.\n1 2  housing_text = housing[[\u0026#39;ocean_proximity\u0026#39;]] housing_text.head()       ocean_proximity     0 NEAR BAY   1 NEAR BAY   2 NEAR BAY   3 NEAR BAY   4 NEAR BAY    H·∫ßu h·∫øt c√°c thu·∫≠t to√°n machine learning l√†m vi·ªác v·ªõi s·ªë h∆°n l√† text ch√≠nh v√¨ v·∫≠y vi·ªác c·ªßa ch√∫ng ta b√¢y gi·ªù l√† chuy·ªÉn text th√†nh s·ªë th√¥i.\nTrong th∆∞ vi·ªán Scikit-Learn c√≥ hai h√†m ƒë·ªÉ l√†m vi·ªác n√†y cho ch√∫ng ta ƒë√≥ l√† LabelEncoder v√† OrdinalEncoder, b·∫£n ch·∫•t c·ªßa hai h√†m n√†y l√† chuy·ªÉn d·∫°ng text sang d·∫°ng s·ªë, ·ªü ƒë√¢y m√¨nh d√πng OrdinalEncoder.\n1 2 3 4  from sklearn.preprocessing import OrdinalEncoder ordinal_encoder = OrdinalEncoder() housing_text_encoded = ordinal_encoder.fit_transform(housing_text) housing_text_encoded[:10]    array([[0.],\n[0.],\n[4.],\n[1.],\n[0.],\n[1.],\n[0.],\n[1.],\n[0.],\n[0.]])\n OK, b√¢y gi·ªù ch√∫ng ta xem d·ªØ li·ªáu c·ªßa ch√∫ng ta tr√¥ng nh∆∞ th·∫ø n√†o sau khi l√†m s·∫°ch nh√©:\n1 2  housing[\u0026#34;ocean_proximity\u0026#34;] = housing_text_encoded housing.head()       longitude latitude housing_median_age total_rooms total_bedrooms population households median_income ocean_proximity     0 -122.23 37.88 41.0 880.0 129.0 322.0 126.0 8.3252 3.0   1 -122.22 37.86 21.0 7099.0 1106.0 2401.0 1138.0 8.3014 3.0   2 -122.24 37.85 52.0 1467.0 190.0 496.0 177.0 7.2574 3.0   3 -122.25 37.85 52.0 1274.0 235.0 558.0 219.0 5.6431 3.0   4 -122.25 37.85 52.0 1627.0 280.0 565.0 259.0 3.8462 3.0    1  housing[\u0026#34;ocean_proximity\u0026#34;].dtypes    dtype(\u0026lsquo;float64\u0026rsquo;)\n K·∫øt qu·∫£ ch√∫ng ta th·∫•y \u0026ldquo;ocean_proximity\u0026rdquo; attribute ƒë√£ chuy·ªÉn t·ª´ text ban ƒë·∫ßu th√†nh ki·ªÉu d·ªØ li·ªáu s·ªë (float).\nNh∆∞ v·∫≠y ch√∫ng ta ƒë√£ ho√†n th√†nh ƒë∆∞·ª£c m·ª•c ti√™u c·ªßa ch√∫ng ta trong b√†i vi·∫øt n√†y. K·∫øt qu·∫£ l√† m·ªôt b·ªô d·ªØ li·ªáu m√† ch√∫ng ta mong mu·ªën.\n3. T·ªïng k·∫øt K·∫øt qu·∫£ nghi√™n c·ª©u cho th·∫•y ƒë·ªÉ l√†m m·ªôt b√†i to√°n Machine Learning th√†nh c√¥ng th√¨ vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu chi·∫øm kho·∫£ng 70% th·ªùi gian c·ªßa ch√∫ng ta, ƒë·ªÉ c√≥ m·ªôt k·∫øt qu·∫£ v·ªõi ƒë·ªô ch√≠nh x√°c nh∆∞ mong mu·ªën th√¨ vi·ªác l√†m s·∫°ch d·ªØ li·ªáu l√† b∆∞·ªõc c·ª±c k·ª≥ quan tr·ªçng v√† kh√¥ng th·ªÉ b·ªè qua khi ch√∫ng ta l√†m m·ªôt ·ª©ng d·ª•ng machine learning.\nL√†m s·∫°ch d·ªØ li·ªáu l√† vi·ªác ƒë√≤i h·ªèi ch√∫ng ta ph·∫£i c√≥ kinh nghi·ªám k·∫øt h·ª£p v·ªõi nhi·ªÅu ki·∫øn th·ª©c. Qua b√†i vi·∫øt n√†y m√¨nh ƒë√£ h∆∞·ªõng d·∫´n t·ªõi c√°c b·∫°n c√°ch th·ª±c hi·ªán ƒë·ªÉ ch√∫ng ta c√≥ m·ªôt b·ªô d·ªØ li·ªáu s·∫°ch l√† nh∆∞ th·∫ø n√†o. Hy v·ªçng r·∫±ng c√°c b·∫°n s·∫Ω √°p d·ª•ng hi·ªáu qu·∫£ trong qu√° tr√¨nh x√¢y d·ª±ng v√† hu·∫•n luy·ªán c√°c thu·∫≠t to√°n machine learning.\nB√†i vi·∫øt ti·∫øp theo m√¨nh s·∫Ω h∆∞·ªõng d·∫´n c√°ch ƒë·ªÉ chu·∫©n b·ªã m·ªôt b·ªô d·ªØ li·ªáu ho√†n ch·ªânh ƒë·ªÉ √°p d·ª•ng v√†o c√°c thu·∫≠t to√°n machine learning m·ªôt c√°ch hi·ªáu qu·∫£ nh·∫•t. C√°c b·∫°n ƒë√≥n ƒë·ªçc trong b√†i vi·∫øt ti·∫øp theo nh√©\n4. T√†i li·ªáu tham kh·∫£o   https://towardsdatascience.com/data-cleaning-with-python-and-pandas-detecting-missing-values-3e9c6ebcf78b\n  https://www.tutorialspoint.com/python_data_science/python_data_cleansing.htm\n  ","description":"K·ªπ thu·∫≠t x·ª≠ l√Ω d·ªØ li·ªáu cho b√†i to√°n machine learning","id":3,"section":"posts","tags":["data cleaning"],"title":"Data Cleaning trong Machine Learning ","uri":"https://tranvanly107.github.io/posts/data-cleaning/"},{"content":"1. Ki·∫øn th·ª©c c·∫ßn c√≥ Tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu v·ªõi b√†i vi·∫øt n√†y, vui l√≤ng ch·∫Øc ch·∫Øn r·∫±ng b·∫°n c√≥ ki·∫øn th·ª©c v·ªÅ Python. C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt nh∆∞:\n Pandas Numpy Sklearn\nC√°c b·∫°n c√≥ th·ªÉ d√πng google colab ho·∫∑c jupyter ƒë·ªÉ code nh√©.  2. C√°c b∆∞·ªõc ƒë·ªÉ l√†m m·ªôt b√†i to√°n ML H·∫ßu h·∫øt to√†n b·ªô b√†i to√°n v·ªÅ Machine Learning ƒë·ªÅu tr·∫£i qua c√°c b∆∞·ªõc c∆° b·∫£n sau: ( M√¨nh s·∫Ω kh√¥ng n√≥i s√¢u v·ªÅ t·ª´ng b∆∞·ªõc, c√°c b·∫°n c·∫ßn t√¨m hi·ªÉu th√™m nh√©)\n X√°c ƒë·ªãnh v·∫•n ƒë·ªÅ. Chu·∫©n b·ªã d·ªØ li·ªáu ƒê√°nh gi√° thu·∫≠t to√°n C·∫£i thi·ªán k·∫øt qu·∫£ Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n  3. B√†i to√°n Machine Learning C√°c b∆∞·ªõc ch√≠nh ƒë·ªÉ ch√∫ng ta x√¢y d·ª±ng b√†i to√°n nh∆∞ sau:\n C√†i ƒë·∫∑t Python v√† SciPy platform. T·∫£i d·ªØ li·ªáu v·ªÅ Hi·ªÉu d·ªØ li·ªáu Tr·ª±c quan h√≥a d·ªØ li·ªáu ƒê√°nh gi√° m·ªôt s·ªë thu·∫≠t to√°n D·ª± ƒëo√°n k·∫øt qu·∫£\nB√¢y gi·ªù ch√∫ng ta b·∫Øt ƒë·∫ßu nh√©.  3.1. C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt C√°c b·∫°n t·ª± c√†i ƒë·∫∑t Python ( 3.x) v√† c√°c th∆∞ vi·ªán sau:\n scipy numpy matplotlib pandas sklearn\nVi·ªác c√†i ƒë·∫∑t c√°c th·ª± vi·ªán n√†y r·∫•t ƒë∆°n gi·∫£n n√™n c√°c b·∫°n t·ª± t√¨m hi·ªÉu.\nC√†i ƒë·∫∑t scipy t·∫°i ƒë√¢y\nSau khi c√°c b·∫°n c√†i xong th√¨ c√°c b·∫°n test xem ƒë√£ th√†nh c√¥ng hay ch∆∞a:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19  import sys\r# Python version\r print(\u0026#39;Python: {}\u0026#39;.format(sys.version))\r# scipy\r import scipy\rprint(\u0026#39;scipy: {}\u0026#39;.format(scipy.__version__))\r# numpy\r import numpy\rprint(\u0026#39;numpy: {}\u0026#39;.format(numpy.__version__))\r# matplotlib\r import matplotlib\rprint(\u0026#39;matplotlib: {}\u0026#39;.format(matplotlib.__version__))\r# pandas\r import pandas\rprint(\u0026#39;pandas: {}\u0026#39;.format(pandas.__version__))\r# scikit-learn\r import sklearn\rprint(\u0026#39;sklearn: {}\u0026#39;.format(sklearn.__version__))\r  N·∫øu c√°c b·∫°n ƒë√£ c√†i th√†nh c√¥ng th√¨ k·∫øt qu·∫£ c√≥ d·∫°ng th·∫ø n√†y:\n Python: 3.7.4 (default, Oct 19 2019, 05:21:45) [GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.2)]\nscipy: 1.4.1\nnumpy: 1.17.3\nmatplotlib: 3.2.0\npandas: 0.25.1\nsklearn: 0.22.2\n C√≤n n·∫øu c√°c b·∫°n c√†i ƒë·∫∑t b·ªã l·ªói th√¨ c√≥ th·ªÉ fix l·ªói t·∫°i ƒë√¢y.\n3.2. T·∫£i d·ªØ li·ªáu Trong b√†i vi·∫øt n√†y ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng d·ªØ li·ªáu c√≥ s·∫µn tr√™n google.\nC√°c b·∫°n c√≥ th·ªÉ t√¨m hi·ªÉu d·ªØ li·ªáu t·∫°i ƒë√¢y.\n3.2.1. import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt Ti·∫øp theo ch√∫ng ta s·∫Ω import c√°c th∆∞ vi·ªán d∆∞·ªõi ƒë√¢y ƒë·ªÉ s·ª± d·ª•ng trong b√†i to√°n.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  # Load libraries\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.metrics import classification_report\rfrom sklearn.metrics import confusion_matrix\rfrom sklearn.metrics import accuracy_score\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r  3.2.2. T·∫£i d·ªØ li·ªáu v·ªÅ Ch√∫ng ta s·∫Ω s·ª≠ d·ª•ng pandas ƒë·ªÉ t·∫£i d·ªØ li·ªáu v·ªÅ. V√† c≈©ng s·∫Ω s·ª≠ d·ª•ng pandas ƒë·ªÉ khai ph√° v√† tr·ª±c quan h√≥a d·ªØ li·ªáu.\n1 2 3 4  # Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r  Ch√∫ng ta s·ª≠ d·ª•ng h√†m read_csv v√† ƒë∆∞·ª£c tr·∫£ v·ªÅ m·ªôt dataframe, ch√∫ng ta s·∫Ω xem k·∫øt qu·∫£ sau.\nN·∫øu c√°c b·∫°n g·∫∑p l·ªói khi load th√¨ b·∫°n c√≥ th·ªÉ download iris.csv.\n3.3. Khai ph√° d·ªØ li·ªáu B√¢y gi·ªù ch√∫ng ta xem d·ªØ li·ªáu n√≥ c√≥ g√¨ nh√©.\nCh√∫ng ta s·∫Ω th·ª±c hi·ªán m·ªôt s·ªë c√°ch ti·∫øp c·∫≠n d∆∞·ªõi ƒë√¢y:\n Chi·ªÅu c·ªßa d·ªØ li·ªáu T·ªïng quan v·ªÅ d·ªØ li·ªáu Th·ªëng k√™ t·∫•t c·∫£ c√°c thu·ªôc t√≠nh c·ªßa d·ªØ li·ªáu Ph√¢n b·ªë c·ªßa d·ªØ li·ªáu\nB√¢y gi·ªù ch√∫ng ti·∫øn h√†nh ƒëi th·ª±c hi·ªán t·ª´ng m·ª•c m·ªôt nh√©:  3.3.1. Chi·ªÅu c·ªßa d·ªØ li·ªáu Chi·ªÅu c·ªßa d·ªØ li·ªáu cho ch√∫ng ta bi·∫øt l√† c√≥ bao nhi√™u h√†ng v√† bao nhi√™u c·ªôt c·ªßa to√†n b·ªô d·ªØ li·ªáu. ƒê·ªÉ xem chi·ªÅu d·ªØ li·ªáu ch√∫ng ta s·ª≠ d·ª•ng h√†m shape trong pandas.\nprint(dataset.shape)\rOutput: (150,5)\nC√≥ nghƒ©a l√† d·ªØ li·ªáu c·ªßa ch√∫ng ta c√≥ 150 h√†ng v√† 5 c·ªôt. N√≥i c√°ch kh√°c l√† c√≥ 150 ƒëi·ªÉm d·ªØ li·ªáu v√† 4 features, 1 class ch√≠nh l√† nh√£n m√† model c·∫ßn d·ª± ƒëo√°n.\n3.3.2. T·ªïng quan d·ªØ li·ªáu B√¢y gi·ªù ch√∫ng ta s·∫Ω xem r√µ h∆°n v·ªÅ d·ªØ li·ªáu nh√©. ƒê·ªÉ xem ch√∫ng ta ch·ªâ c·∫ßn d√πng h√†m head trong pandas nh∆∞ sau:\nprint(dataset.head(20))\rH√†m head gi√∫p ch√∫ng ta hi·ªÉn th·ªã n b·∫£n ghi ƒë·∫ßu ti√™n, ·ªü ƒë√¢y ch√∫ng s·∫Ω hi·ªÉn th·ªã 20 h√†ng ƒë·∫ßu ti√™n c·ªßa to√†n b·ªô d·ªØ li·ªáu.\n    sepal-length sepal-width petal-length petal-width class     0 5.1 3.5 1.4 0.2 Iris-setosa   1 4.9 3.0 1.4 0.2 Iris-setosa   2 4.7 3.2 1.3 0.2 Iris-setosa   3 4.6 3.1 1.5 0.2 Iris-setosa   4 5.0 3.6 1.4 0.2 Iris-setosa   5 5.4 3.9 1.7 0.4 Iris-setosa   6 4.6 3.4 1.4 0.3 Iris-setosa   7 5.0 3.4 1.5 0.2 Iris-setosa   8 4.4 2.9 1.4 0.2 Iris-setosa   9 4.9 3.1 1.5 0.1 Iris-setosa   10 5.4 3.7 1.5 0.2 Iris-setosa   11 4.8 3.4 1.6 0.2 Iris-setosa   12 4.8 3.0 1.4 0.1 Iris-setosa   13 4.3 3.0 1.1 0.1 Iris-setosa   14 5.8 4.0 1.2 0.2 Iris-setosa   15 5.7 4.4 1.5 0.4 Iris-setosa   16 5.4 3.9 1.3 0.4 Iris-setosa   17 5.1 3.5 1.4 0.3 Iris-setosa   18 5.7 3.8 1.7 0.3 Iris-setosa   19 5.1 3.8 1.5 0.3 Iris-setosa    ƒê√¢y l√† h√¨nh d·∫°ng c·ªßa m·ªôt dataframe. C√°c features c·ªßa d·ªØ li·ªáu bao g·ªìm: chi·ªÅu d√†i ƒë√†i hoa (sepal-length), chi·ªÅu r·ªông ƒë√†i hoa(sepal-width ), chi·ªÅu d√†i c√°nh hoa (petal-length), chi·ªÅu r·ªông c√°nh hoa(petal-width ).V√† c√≥ 3 classes m√† ch√∫ng ta c·∫ßn d·ª± ƒëo√°n ƒë√≥ l√†: Iris-setosa, Iris-versicolor, Iris-virginica. T·∫•t nhi√™n m·ªói lo√†i hoa s·∫Ω c√≥ c√°c features kh√°c nhau ƒë·ªÉ m√¥ h√¨nh n√≥ ph√¢n bi·ªát v√† ƒë∆∞a ra d·ª± ƒëo√°n ch√≠nh x√°c.\n3.3.3. Th·ªëng k√™ c√°c thu·ªôc t√≠nh c·ªßa d·ªØ li·ªáu B√¢y gi·ªù ch√∫ng ta s·∫Ω xem chi ti·∫øt h∆°n v·ªÅ c√°c features: Bao g·ªìm count(s·ªë l∆∞·ª£ng), mean(chi·ªÅu d√†i, chi·ªÅu r·ªông trung b√¨nh), min v√† max. C·ª• th·ªÉ nh∆∞ sau:\nprint(dataset.describe())\rV√† k·∫øt qu·∫£ l√†:\n    sepal-length sepal-width petal-length petal-width     count 150.000000 150.000000 150.000000 150.000000   mean 5.843333 3.054000 3.758667 1.198667   std 0.828066 0.433594 1.764420 0.763161   min 4.300000 2.000000 1.000000 0.100000   25% 5.100000 2.800000 1.600000 0.300000   50% 5.800000 3.000000 4.350000 1.300000   75% 6.400000 3.300000 5.100000 1.800000   max 7.900000 4.400000 6.900000 2.500000    Nh∆∞ chung ta th·∫•y ·ªü tr√™n ƒë·ªô d√†i v√† r·ªông c·ªßa t·ª´ng feature thu·ªôc c√πng ƒë∆°n v·ªã (cm) v√† c√πng n·∫±m trong kho·∫£ng t·ª´ 0-8 cm.\n3.3.4. Ph√¢n b·ªë d·ªØ li·ªáu B√¢y gi·ªù ch√∫ng ta xem s·ªë l∆∞·ª£ng c·ªßa t·ª´ng lo√†i hoa ( t·ª´ng class) b·∫±ng c√°ch:\nprint(dataset.groupby('class').size()\rV√† k·∫øt qu·∫£ s·∫Ω l√†:\n class\nIris-setosa 50\nIris-versicolor 50\nIris-virginica 50\n Ch√∫ng ta th·∫•y s·ªë l∆∞·ª£ng hoa ( class) ƒë·ªìng ƒë·ªÅu nhau v√† c√πng l√† 50.\nCode ƒë·∫ßy ƒë·ªß cho c√°c b∆∞·ªõc tr√™n nh∆∞ sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  # summarize the data\r from pandas import read_csv\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# shape\r print(dataset.shape)\r# head\r print(dataset.head(20))\r# descriptions\r print(dataset.describe())\r# class distribution\r print(dataset.groupby(\u0026#39;class\u0026#39;).size())\r  3.4. Tr·ª±c quan d·ªØ li·ªáu Ch√∫ng ta ƒë√£ bi·∫øt ƒë∆∞·ª£c c∆° b·∫£n v·ªÅ d·ªØ li·ªáu, b√¢y gi·ªù l√† l√∫c ch√∫ng ta tr·ª±c quan h√≥a b·∫±ng bi·ªÉu ƒë·ªì.\nB∆∞·ªõc n√†y gi√∫p ch√∫ng ta hi·ªÉu r√µ h∆°n ph√¢n b·ªë chi ti·∫øt t·ª´ng lo√†i hoa(class) b·∫±ng c√°ch nh√¨n v√†o c√°c bi·ªÉu ƒë·ªì sau:\n3.4.1. Univariate Plots Bi·ªÉu ƒë·ªì n√†y gi√∫p ch√∫ng ta hi·ªÉu ƒë∆∞·ª£c ph√¢n b·ªë c·ªßa t·ª´ng feature.\ndataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\rK·∫øt qu·∫£ hi·ªÉn th·ªã nh∆∞ sau:\nCh√∫ng ta c≈©ng c√≥ th·ªÉ t·∫°o ra c√°c bi·ªÉu ƒë·ªì histogram v·ªõi d·ªØ li·ªáu ƒë·∫ßu v√†o:\n# histograms\rdataset.hist()\rpyplot.show()\rK·∫øt qu·∫£ hi·ªÉn th·ªã nh∆∞ sau:\n3.4.2. Multivariate Plots B√¢y gi·ªù ch√∫ng ta c√≥ th·ªÉ th·∫•y s·ª± li√™n quan gi·ªØa c√°c features. Ch√∫ng ta s·ª≠ d·ª•ng scatter_matrix trong pandas ƒë·ªÉ th·∫•y ƒë∆∞·ª£c quan h·ªá c·ªßa c√°c features tr√™n to√†n b·ªô d·ªØ li·ªáu:\n# scatter plot matrix\rscatter_matrix(dataset)\rpyplot.show()\rCh√∫ng ta th·∫•y ƒë∆∞·ª£c s·ª± t∆∞∆°ng quan gi·ªØa ch√∫ng th√¥ng qua bi·ªÉu ƒë·ªì sau:\nCode ƒë·∫ßy ƒë·ªß cho c√°c b∆∞·ªõc tr√™n nh∆∞ sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # visualize the data\r from pandas import read_csv\rfrom pandas.plotting import scatter_matrix\rfrom matplotlib import pyplot\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# box and whisker plots\r dataset.plot(kind=\u0026#39;box\u0026#39;, subplots=True, layout=(2,2), sharex=False, sharey=False)\rpyplot.show()\r# histograms\r dataset.hist()\rpyplot.show()\r# scatter plot matrix\r scatter_matrix(dataset)\rpyplot.show()\r  3.5. ƒê√°nh gi√° thu·∫≠t to√°n B√¢y gi·ªù l√† l√∫c ch√∫ng ta s·∫Ω th·ª≠ ch·ªçn m·ªôt v√†i model ƒë·ªÉ xem ƒë·ªô ch√≠nh x√°c c·ªßa t·ª´ng model tr√™n t·∫≠p test c·ªßa ch√∫ng ta nh√©.\nC√°c b∆∞·ªõc th·ª±c hi·ªán nh∆∞ sau:\n T√°ch m·ªôt ph·∫ßn d·ªØ li·ªáu ƒë·ªÉ ƒë√°nh gi√° model ( validation dataset) Thi·∫øt l·∫≠p test harness ƒë·ªÉ s·ª≠ d·ª•ng 10-fold cross validation Ch·∫°y v√† d·ª± ƒëo√°n k·∫øt qu·∫£ tr√™n t·ª´ng model Ch·ªçn ra model c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t ƒë·ªÉ s·ª≠ d·ª•ng cu·ªëi c√πng\nok, b√¢y gi·ªù s·∫Ω v√†o chi ti·∫øt nh√©:  3.5.1. T√°ch m·ªôt ph·∫ßn d·ªØ li·ªáu (validation Dataset) Ch√∫ng ta s·∫Ω chia d·ªØ li·ªáu c·ªßa ch√∫ng ta ra th√†nh 2 ph·∫ßn: 80% d·ªØ li·ªáu d√πng ƒë·ªÉ hu·∫•n luy·ªán model ( training data), 20% c√≤n l·∫°i l√† d√πng ƒë·ªÉ ƒë√°nh gi√° model(validation data)\n1 2 3 4 5  # Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1)\r  Ch√∫ng ta l·∫•y b·ªën c·ªôt ƒë·∫ßu ti√™n (X) l√† c√°c features hay c√≤n g·ªçi l√† ƒë·∫∑c t√≠nh c·ªßa d·ªØ li·ªáu(attributes), c·ªôt cu·ªëi c√πng l√† (y) nh√£n c·ªßa d·ªØ li·ªáu.\nB√¢y gi·ªù ch√∫ng ta ƒë√£ chu·∫©n b·ªã ƒë∆∞·ª£c d·ªØ li·ªáu ƒë·∫ßy ƒë·ªß cho vi·ªác training v√† testing r·ªìi.\n3.5.2. Cross validation Ch√∫ng ta s·∫Ω t·∫°o ra 10 fold cross validation ƒë·ªÉ estimate model.\nC·ª• th·ªÉ l√† ch√∫ng ta chia d·ªØ li·ªáu train ra l√†m 10 ph·∫ßn b·∫±ng nhau, l·∫•y 9 ph·∫ßn cho vi·ªác train v√† ph·∫ßn c√≤n l·∫°i l√† ƒë·ªÉ test trong l√∫c train. ƒê·ªÉ hi·ªÉu r√µ h∆°n v·ªÅ cross validation th√¨ c√°c b·∫°n c√≥ th·ªÉ t√¨m hi·ªÉu t·∫°i ƒë√¢y nh√©.\n3.5.3. Build models Ch√∫ng ta b√¢y gi·ªù s·∫Ω kh√¥ng th·ªÉ bi·∫øt ƒë∆∞·ª£c model n√†o s·∫Ω t·ªët nh·∫•t.\nSau ƒë√¢y l√† nh·ªØng model m√¨nh d√πng ƒë·ªÉ ch·∫°y v√† d·ª± ƒëo√°n ƒë·ªô ch√≠nh x√°c.\n Logistic Regression (LR) Linear Discriminant Analysis (LDA) K-Nearest Neighbors (KNN). Classification and Regression Trees (CART). Gaussian Naive Bayes (NB). Support Vector Machines (SVM).\nC·ª• th·ªÉ t·ª´ng model th√¨ m√¨nh s·∫Ω c·∫≠p nh·∫≠t sau n·∫øu c√≥ th·ªùi gian nh√©.\nN√†o b√¢y gi·ªù ch√∫ng ta s·∫Ω code v√† ch·∫°y nh√©:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  # Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r  3.5.4. Ch·ªçn model c√≥ k·∫øt qu·∫£ t·ªët nh·∫•t B√¢y gi·ªù ch√∫ng ta ƒë√£ ch·∫°y ƒë∆∞·ª£c 6 model v√† m·ªói model s·∫Ω cho ra ƒë·ªô ch√≠nh x√°c kh√°c nhau. Gi·ªù l√† l√∫c ch√∫ng ta so s√°nh t·ª´ng k·∫øt qu·∫£ v√† ch·ªçn ra model c√≥ ƒë·ªô ch√≠nh x√°c t·ªët nh·∫•t.\nCh·∫°y xong ƒëo·∫°n code tr√™n th√¨ ch√∫ng ta nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£ sau:\n LR: 0.960897 (0.052113)\nLDA: 0.973974 (0.040110)\nKNN: 0.957191 (0.043263)\nCART: 0.957191 (0.043263)\nNB: 0.948858 (0.056322)\nSVM: 0.983974 (0.032083)\n Nh∆∞ ch√∫ng ta th·∫•y ƒë·ªô ch√≠nh x√°c c·ªßa model Support Vector Machine(SVM)\nƒë·∫°t k·∫øt qu·∫£ v·ªõi ƒë·ªô ch√≠nh x√°c l√™n ƒë·∫øn h∆°n 98% v√† l√† k·∫øt qu·∫£ t·ªët nh·∫•t.\nTo√†n b·ªô code cho b√†i vi·∫øt nh∆∞ sau:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47  # compare algorithms\r from pandas import read_csv\rfrom matplotlib import pyplot\rfrom sklearn.model_selection import train_test_split\rfrom sklearn.model_selection import cross_val_score\rfrom sklearn.model_selection import StratifiedKFold\rfrom sklearn.linear_model import LogisticRegression\rfrom sklearn.tree import DecisionTreeClassifier\rfrom sklearn.neighbors import KNeighborsClassifier\rfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\rfrom sklearn.naive_bayes import GaussianNB\rfrom sklearn.svm import SVC\r# Load dataset\r url = \u0026#34;https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv\u0026#34;\rnames = [\u0026#39;sepal-length\u0026#39;, \u0026#39;sepal-width\u0026#39;, \u0026#39;petal-length\u0026#39;, \u0026#39;petal-width\u0026#39;, \u0026#39;class\u0026#39;]\rdataset = read_csv(url, names=names)\r# Split-out validation dataset\r array = dataset.values\rX = array[:,0:4]\ry = array[:,4]\rX_train, X_validation, Y_train, Y_validation = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\r# Spot Check Algorithms\r models = []\rmodels.append((\u0026#39;LR\u0026#39;, LogisticRegression(solver=\u0026#39;liblinear\u0026#39;, multi_class=\u0026#39;ovr\u0026#39;)))\rmodels.append((\u0026#39;LDA\u0026#39;, LinearDiscriminantAnalysis()))\rmodels.append((\u0026#39;KNN\u0026#39;, KNeighborsClassifier()))\rmodels.append((\u0026#39;CART\u0026#39;, DecisionTreeClassifier()))\rmodels.append((\u0026#39;NB\u0026#39;, GaussianNB()))\rmodels.append((\u0026#39;SVM\u0026#39;, SVC(gamma=\u0026#39;auto\u0026#39;)))\r# evaluate each model in turn\r results = []\rnames = []\rfor name, model in models:\rkfold = StratifiedKFold(n_splits=10, random_state=1)\rcv_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring=\u0026#39;accuracy\u0026#39;)\rresults.append(cv_results)\rnames.append(name)\rprint(\u0026#39;%s: %f(%f)\u0026#39; % (name, cv_results.mean(), cv_results.std()))\r# Compare Algorithms\r pyplot.boxplot(results, labels=names)\rpyplot.title(\u0026#39;Algorithm Comparison\u0026#39;)\rpyplot.show()\r  3.5.5. D·ª± ƒëo√°n k·∫øt qu·∫£ B√¢y gi·ªù ch√∫ng ta ch·ªçn m·ªôt model t·ªët nh·∫•t ƒë·ªÉ d·ª± ƒëo√°n. Th√¨ theo nh∆∞ k·∫øt qu·∫£ ·ªü tr√™n th√¨ ch√∫ng ta s·∫Ω ch·ªçn SVM l√† model cu·ªëi c√πng ƒë·ªÉ d·ª± ƒëo√°n.\nCh√∫ng ta s·∫Ω d·ª± ƒëo√°n d·ª±a tr√™n t·∫≠p validation set m√† ch√∫ng ta ƒë√£ t√°ch ·ªü m·ª•c 3.5.1.\nƒê√¢y l√† d·ªØ li·ªáu kh√¥ng h·ªÅ li√™n quan t·ªõi model trong qu√° tr√¨nh training. M·ª•c ƒë√≠ch c·ªßa vi·ªác d·ª± ƒëo√°n tr√™n t·∫≠p d·ªØ li·ªáu n√†y ƒë·ªÉ ƒë√°nh gi√° xem model ch√∫ng ta ch·ªçn c√≥ th·ª±c s·ª± d·ª± ƒëo√°n t·ªët hay kh√¥ng, ƒë·ªÉ t·ª´ ƒë√≥ bi·∫øt ƒë∆∞·ª£c n√≥ c√≥ b·ªã overfitting hay kh√¥ng.\nƒê·∫ßu ti√™n ch√∫ng ta fit model tr√™n to√†n b·ªô t·∫≠p train v√† d·ª± ƒëo√°n tr√™n to√†n b·ªô t·∫≠p validation.\n1 2 3 4 5 6  # Make predictions on validation dataset\r model = SVC(gamma=\u0026#39;auto\u0026#39;)\rmodel.fit(X_train, Y_train)\rpredictions = model.predict(X_validation)\r# Evaluate predictions\r print(accuracy_score(Y_validation, predictions))\r  K·∫øt qu·∫£ c·ªßa ch√∫ng ta l√†:\n 0.9666666666666667\n[[11 0 0]\n[ 0 12 1]\n[ 0 0 6]]\n Ho·∫∑c ch√∫ng ta c√≥ th·ªÉ l√™n google download ·∫£nh c·ªßa m·ªôt lo√†i hoa thu·ªôc m·ªôt trong 3 lo·∫°i hoa ·ªü tr√™n v√† cho v√†o model ƒë·ªÉ xem model d·ª± ƒëo√°n xem l√† lo√†i hoa g√¨ nh√©.\nCh√∫ng ta c√≥ th·ªÉ s·ª≠ d·ª•ng precision, recall\u0026hellip; ƒë·ªÉ show k·∫øt qu·∫£.\nprint(confusion_matrix(Y_validation, predictions))\rprint(classification_report(Y_validation, predictions))\rV√† k·∫øt qu·∫£:\n    precision recall f1-score support     Iris-setosa 1.00 1.00 1.00 11   Iris-versicolor 1.00 0.92 0.96 13   Iris-virginica 0.86 1.00 0.92 6       accuracy  0.97 30     macro avg 0.95 0.97 0.96 30   weighted avg 0.97 0.97 0.97 30    V·∫≠y l√† ch√∫ng ta ƒë√£ c∆° b·∫£n ho√†n th√†nh b√†i to√°n ƒë·∫ßu ti√™n c·ªßa Machine Learning.\n4. T·ªïng K·∫øt  B√†i vi·∫øt n√†y nh·∫±m m·ª•c ƒë√≠ch gi√∫p c√°c b·∫°n hi·ªÉu ƒë∆∞·ª£c lu·ªìng ho·∫°t ƒë·ªông hay n√≥i c√°ch kh√°c l√† c√°ch ƒë·ªÉ l√†m m·ªôt b√†i to√°n machine learning n√™n m√¨nh s·∫Ω kh√¥ng ƒëi chi ti·∫øt v√†o t·ª´ng d√≤ng code. M·∫∑c ƒë·ªãnh ban ƒë·∫ßu l√† c√°c b·∫°n ƒë√£ c√≥ ki·∫øn th·ª©c c∆° b·∫£n m√† b√†i vi·∫øt ƒë√£ y√™u c·∫ßu n√™n m√¨nh c≈©ng kh√¥ng n√≥i r√µ t·ª´ng th∆∞ vi·ªán, t·ª´ng kh√°i ni·ªám trong b√†i vi·∫øt n·ªØa. ƒê√¢y l√† b√†i vi·∫øt ƒë·∫ßu ti√™n trong lo·∫°t b√†i ML c∆° b·∫£n n√™n c√≥ r·∫•t nhi·ªÅu h·∫°n ch·∫ø v√† thi·∫øu s√≥t, r·∫•t mong ƒë∆∞·ª£c s·ª± g√≥p √Ω c·ªßa c√°c b·∫°n( m√¨nh s·∫Ω c·∫£i thi·ªán d·∫ßn trong nh·ªØng b√†i vi·∫øt sau). Nh∆∞ng hy v·ªçng qua b√†i vi·∫øt n√†y c√°c b·∫°n ƒë√£ bi·∫øt c√°ch x√¢y d·ª±ng m·ªôt b√†i to√°n machine learning.  C·∫£m ∆°n t·∫•t c·∫£ c√°c b·∫°n ƒë√£ ƒë·ªçc b√†i vi·∫øt. B√†i vi·∫øt ti·∫øp theo s·∫Ω l√† c√°ch x·ª≠ l√Ω d·ªØ li·ªáu, cleaning data nh∆∞ th·∫ø n√†o. Mong c√°c b·∫°n ƒë√≥n ƒë·ªçc!!!\n","description":"ƒê√¢y l√† b√†i to√°n ƒë·∫ßu ti√™n trong lo·∫°t b√†i ML c∆° b·∫£n","id":4,"section":"posts","tags":["tutorial","machine learning"],"title":"Gi·∫£i b√†i to√°n Machine Learning ƒë·∫ßu ti√™n ","uri":"https://tranvanly107.github.io/posts/first-machine-learning/"}]